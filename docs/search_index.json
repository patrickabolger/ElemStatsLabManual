[["index.html", "A Lab Manual for PSYC 301: Elementary Statistics for Psychology Preface 0.1 Goals 0.2 Resources 0.3 Rationale for jamovi 0.4 Organization of this manual 0.5 Main outcome of class", " A Lab Manual for PSYC 301: Elementary Statistics for Psychology Patrick Bolger, PhD Department of Psychological &amp; Brain Sciences at Texas A&amp;M University14 March 2021 Preface Howdy! This is a lab manual for PSYC 301: Elementary Statistics for Psychology at Texas A&amp;M University. It is an Open Educational Resource, and is free to read, licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. It complements the main class, which also uses an OER textbook, Learning Statistics with jamovi by Navarro &amp; Foxcroft (2019), linked here. In short, the class is more about the theory behind statistics, and the lab is more about practical application, especially writing up research that employs statistics. Among other things, your TA will guide you through this manual by directing you which chapters to read, what to pay particular attention to, and which exercises to practice with. More specific goals are listed below. 0.1 Goals The purpose of this manual and the other resources listed above is to give you the resources necessary to do the following: Learn jamovi. Using the free statistical software jamovi, leverage the knowledge of statistical theory that you learn in the main class in order to carry out small-scale, in-class exercises. jamovi employs an easy-to-use graphical user interface (GUI, pronounced “gooey”), but is built on top of the programming language R (R Core Team, 2021), which has become extremely popular among data scientists during the last two decades. Chapters 1-9 of this lab manual cover learning jamovi. Design and conduct your own study in small groups. This will take the form of a survey that you will design and distribute using Google Forms (see above). You already have a template for this survey in your shared group folder in Google Drive (under the main instructor’s account). Please save this Google Form (i.e., GoogleFormsTemplate) as a new form with a name that appropriately reflects the topic of your survey (e.g., SleepAndAcademicAchievement). There is more on Google Drive and Google Forms down below. Chapter 10 of this lab manual provides a general overview of this portion of the research project. Analyze the data from your study. Using the same software (jamovi), perform the front-end work on the main project assignment of the lab: a small-scale research paper. Specifically, you will import raw data from your survey into jamovi, modify the data however necessary (both covered in Chapter 11 of this manual), and carry out the appropriate statistical analyses and visualizations quickly and efficiently These statistical procedures are covered in Chapters 3-9 of this manual, but specifically the following chapters in the following order: 6 and 8, followed by 7). The output of these analyses will underlie what you write in the Results and Discussion sections of your paper (see below). Write a paper (in four parts). Based on both (1) through (3) directly above, you will write a small-scale research paper. This effort will be staged through four, major writing assignments based broadly speaking on each of the four typical sections of a research paper in psychology: Introduction, Methods, Results, and Discussion.1 However, due to the timing of statistical topics in the main class, the paper will take the following, slightly modified, though still very common form:2 Introduction, Methods, STUDY 1 (Results, Discussion), STUDY 2 (Results, Discussion), General Discussion. Each assignment will be broken into two parts: (1) a rough draft that will be peer-reviewed through Peerceptiv (via eCampus); and (2) a final draft that will be submitted through Turnitin (also via eCampus). This results in a total of eight writing assignments to turn in. Perceptiv and Turnitin are covered in more detail below. Chapters 12-17, as well as all the Appendixes (A-J), are dedicated to showing you how to write just such a paper. 0.2 Resources As noted above, you will need certain resources in order to achieve these goals. What follows are two lists of all the resources you will need to do so, all of which are available to you at no extra cost (beyond the tuition you have already paid). The first list consists of fixed resources, meaning that they are either textbooks, handouts, videos, or data sources that are given to you as-is. The second list is active, meaning generally that it’s software that you can use freely, but you must do the work for it to be meaningful. 0.2.1 Fixed resources This lab manual (Bolger, 2019) was created also using the programming language R within RStudio Desktop, along with the R package bookdown (Xie, 2020), which uses rmarkdown (Allaire et al., 2021) to create books. R Markdown is a markdown language for R created by the popular integrated development environment (IDE) RStudio (RStudio Team, 2015), which makes programming in R easier. Several other R packages were also used throughout. These are represented by the hexagon stickers near the top of this page Figure 0.1: The jamovi user guide. Figure 0.2: The jamovi blog. Figure 0.3: Learning statistics with jamovi. Figure 0.4: The jamovi quickstart guide. Figure 0.5: datalab.cc. Figure 0.6: Statistics for social science - A sourcebook for basic statistical methods. 0.2.2 Software Figure 0.7: jamovi. Figure 0.8: eCampus. Figure 0.9: Peerceptiv. Figure 0.10: Turnitin. Figure 0.11: Google apps at TAMU. 0.3 Rationale for jamovi You might be wondering why this lab manual focuses on jamovi and not, say, the more popular statistical-analysis software in the social sciences: SPSS (IBM Corp., 2019). Indeed, jamovi is new and not as popular (at least not yet) in the social sciences, including Psychology. To be honest, SPSS is still the most popular among professors in Psychology. You are likely to encounter SPSS if you, say, join a lab in the Psychology Department. However, jamovi offers some features that exceed in quality those of SPSS (at least the current version):3 Cost. jamovi is free for you to download onto your laptop (Windows, Mac, Linux, and ChromeOS). Note that you can also download SPSS on to your computer, but it will cost you. It is marketed through various third-party retailers. See the SPSS website for details.4 In the end, however, jamovi turns out to be easier to use than SPSS, though the underlying statistical analyses are the same.5 Navigability. jamovi’s GUI is much easier to navigate than that of SPSS. jamovi is really, really new (established in 2017), and SPSS has been around since the 1960s. As a result of its age and, presumably, a disinclination to field user complaints about broken code, SPSS carries a lot of menu “baggage” that retains backwards-compatibility with earlier versions of the software. That is, you can run statistical analyses in SPSS using code that your wrote decades ago. The cost of that failure to get rid of older functions is that SPSS has accumulated a lot of menu clutter. This can be frustrating for beginners. jamovi has no such clutter. Compactness. jamovi keeps its data, code, and output all in one file (a file with the .omv suffix). This makes analyses really easy to share with classmates, TAs, and (eventually) colleagues. You just send them the file that you worked on, and it opens up for them exactly the same way you left it when you last saved it. SPSS is not like this as the data, the syntax (to run the statistical procedures), and the output are all stored as three separate files (.sav, .sps, and .spv files, respectively). This can be frustrating. In jamovi, there is only one file (.omv). Convenience. If you send someone a .omv file from jamovi to work on, but they don’t have jamovi installed, naturally they can just download and install it for free,6 and then get to work (as long as they have the space on their hard drive). For SPSS in contrast, if the person you are sharing your SPSS output with does not have SPSS available (on the computer they’re currently working on), then they will either have to find a computer with SPSS on it or purchase and SPSS license to open your analysis. Alternatively, you could export all output to .pdf or .htm format. But then they would only be able to view your output, and not be able to change anything. Moreover, if you just want to share data, you need to export it to a text-delimited file so that your colleague can open it up on their end. All of this is solved by jamovi, where everything is all in one file. 0.4 Organization of this manual The lab manual is organized as follows. Unit I is Basics. Chapter 1 is an introduction to jamovi, namely: how to install it on your own computer, how to add modules, navigating the interface, sharing files, etc. Chapter 2 shows you how to, enter and import data into jamovi, as well as manipulate it, transform it, etc. Chapter 3 shows you how to generate the basic statistics (descriptive statistics) that must always be reported in research. These descriptive statistics also lay the foundation for the inferential statistics that follow in subsequent chapters. Chapter 4 tutors you on how to use the graphical capabilities of jamovi. Unit II is Inferential statistics. Chapter 5 begins the first such test in this class: categorical data analysis, and in particular, chi-square, which you can use when the outcome variable is a discrete count of something (like number of people in the class), and your predictor is a categorical variable with (hopefully) very few levels (e.g., Class Level: Freshman, Sophomore, Junior, Senior). Chapter 6 covers t-tests, which you use when your outcome variable is continuous (e.g., body weight), and you have a single, dichotomous predictor variable (e.g., Dietary Preference: vegetarian vs. omnivore). Chapter 7, Correlation and linear regression, shows you the analysis to use when you have a single, continuous outcome variable, and one or more continuous predictor variables, like IQ and GPA.7 Chapter 8 is an extension of chapter 6, but instead of having just 2 levels for the single, categorical predictor variable, it can have 3 or more levels. This is called a one-way ANOVA. For example, you might have salary as predicted by whether you’re faculty, staff, or a student. Chapter 9 extends the ANOVA model from Chapter 8 even further by allowing you to have 2 or more predictors, each of which is categorical with 2 or more levels (e.g., Degree of Happiness as predicted by Gender and Class Level (freshman, sophomore, junior, senior)). This is called a factorial ANOVA Unit III is Research Projects. Chapter 10 is about getting your study designed and your data collected. Chapter 11 is concerned with getting the data you have collected in your project into a format that you can use for the analyses, and then carrying out those analyses. Unit IV is Writing. Chapter 12 is an overview of the process and product of a research paper in Psychology. Subsequent chapters get more specific. Namely, Chapter 13 covers how to write the Introduction section. Chapter 14 covers how to write the Method section. Chapter 15, introduces you to how to report the Results section. Chapter 16, introduces you to how to report the Discussion section. And Chapter 17 shows you how to write the General Discussion, which ties everything together. 0.5 Main outcome of class When you finish this class, as well as this lab, you should not only understand the theoretical underpinnings of data science (a more general label for statistics), but also know how to communicate about it. This kind of knowledge should serve you well for the rest of your life, not only professionally, but also personally. You are bombarded every day with statistics, especially in the news. There is a reason that it is required by most departments at the university: Knowledge of statistics makes you into a more informed citizen in general. So let’s take the first steps on the way to becoming a data analyst!! This is sometimes referred to as IMRaD structure, an acronym composed of each of the section’s first letters.↩︎ In fact, it is more common.↩︎ The authors of your main textbook, Navarro &amp; Foxcroft (2019), also have a list of reasons to prefer jamovi over SPSS. This list can be found at the very beginning of Chapter 3 of that textbook↩︎ You can find SPSS on designated computers throughout campus. This includes the computers in your lab because the department has purchased these licenses↩︎ We should mention that SAS is another commercial statistical-analysis software company that actually has made their extremely powerful software freely available for educational purposes. You can download it for free as SAS University Edition, and it will run on your computer through your browser as a virtualized application. The downside is that few psychologists use SAS, and it is notoriously difficult to learn. You can also find SAS on the VOAL↩︎ … unless your collaborator refuses to download jamovi onto their own computer (a possibility, presumably)↩︎ This is another lie; there is no limit to the number or types of variables in linear regression, but we keep it mostly simple to start with in this chapter.↩︎ "],["GettingUsedtoJamovi.html", "Chapter 1 Getting used to jamovi 1.1 Downloading and installing 1.2 Navigation 1.3 Outside help on basics 1.4 Conclusion", " Chapter 1 Getting used to jamovi This short chapter will introduce you to jamovi (The jamovi Project, 2019). As noted in the last chapter, jamovi (first letter is lowercase) is free statistical-analysis software that is built on top of a wildly popular, free, statistically-centered, programming language called R (R Core Team, 2021). The difference is that jamovi consists of a GUI, whereas R is command-line software. That is, jamovi is point-and-click with easy-to-use pull-down menus, whereas R involves typing code on a blank screen. R is rapidly changing the face of data science, but it can be very intimidating at first because of its command-line nature and, what many (even seasoned programmers) feel to be, unusual programming syntax. jamovi is a much easier program to use, and it is explicitly designed to be manageable for beginners. 1.1 Downloading and installing This is easy. Just go to the main jamovi website, find the download tab, choose your operating system, then download and install. 1.2 Navigation jamovi is laid out such that your data, your input, is always on the left in a spreadsheet format (like Excel), and whatever analyses you apply to those data, your output, is always on the right. There are four tabs in the uppermost menu bar. They are as follows: The Data tab in Figure 1.1 below shows you a set of options on how to manipulate data within the spreadsheet. Figure 1.1: The data tab. The Analyses tab in Figure 1.2 below shows you options for different statistical analyses you can run on the data that you have in the spreadsheet. Figure 1.2: The analyses tab. The window that occupies most of the right side of the interface is where output goes. It will be blank until you do something. I’m going to open a data set that will be used in sections below for various pedagogical reasons. I’m just going to calculate the mean for the variable called mood.gain. In Figure 1.3 below, we have not yet done anything to the data, so the output window on the right is blank. Figure 1.3: Data with no analyses performed (blank window on left). But Figure 1.4 below shows what you see in that same window after asking for descriptive statistics and a histogram (commands you haven’t learned how to execute yet) of the mood.gain variable. Figure 1.4: Basic descriptive statistics. There is a Basic operations8 menu on the far left that is triggered by clicking the three horizontal lines, or \\((\\equiv)\\). See Figure 1.5 below. Clicking this will make some file operations slide in from the left. These are things like saving the .omv file with a name, importing data, exporting data, etc. Note that to get out of this menu option, you just need to click the big arrow. This is true for any sub-menu in jamovi. Figure 1.5: The basic operations menu. There is a Preferences9 menu on the far right. See Figure 1.6 below. To activate it click the three vertical dots \\((\\vdots)\\). There are settings in here. You may want to change the color scheme, or round numbers to a certain number of decimals, etc. You can set these preferences here. Figure 1.6: The preferences menu. Finally, there is a tab labeled Modules on the far right when you are under Analyses (there is a plus sign “+” to identify it as well). To understand what is going on with this tab, you first have to understand that when you downloaded jamovi, you downloaded it in its base form. The base form is what the authors of the software believe will carry you through basic analyses without overloading the software and making it unnecessarily large to download and install. However, many users will want or need to carry out specific analyses that the software authors consider either unusual or extra. They provide this extra functionality in the form of modules. They are written by either the jamovi team, or in many cases, avid users of jamovi (you can do so as well, if you’re so inclined). If you click this tab, and then select jamovi library, you will see a pop-up menu consisting of the available modules that are either already Installed (the upper-left tab) or Available for download (the middle tab). Do not worry about tab on the upper right, Sideload, unless you want to write and test your own module. See Figure 1.7 below. Figure 1.7: Modules in jamovi. You might notice that one module is installed by default. This is the base jamovi module that comes with the software. However, there are some other modules you will need installed for this class. They are as follows: learning statistics with jamovi. This module contains the data sets for your main textbook by Navarro &amp; Foxcroft (2019). Theses data sets will subsequently become available under the File Operations icon (the three horizontal lines in the uppermost menu bar), where you navigate to Open &gt; Data Library &gt; learning statistics with jamovi, where you will then see a list of the data sets. scatr. This module lets you make nicer scatterplots than are offered in the base module. Scatterplots are relevant to Chapters 4, 7, and 15. Once installed, you will find the scatr menu under the Analyses view, under Exploration &gt; scatr. R data Sets. This module loads the sample data sets that come with R. You will be using one or more of these data sets in this lab manual. 1.3 Outside help on basics 1.3.1 Overall introduction datalab.cc’s general introductory tutorial on jamovi is a nice overview of jamovi. Choose video #1, called Welcome. If you are at Texas A&amp;M University, you can also go to the Howdy! portal and search for jamovi in the LinkedIn Learning tutorials. In fact, all of datalab.cc’s tutorials on jamovi are available there. 1.3.2 Downloading and installing Here are some tutorials on how to download and install jamovi: - the jamovi User Guide - the datalab.cc video tutorial on how to do this. Scroll to video #2, Installing jamovi. - Navarro &amp; Foxcroft (2019), Chapter 3, Section 1 (3.1) 1.3.3 Navigating datalab.cc’s video tutorial on navigating jamovi. Scroll to video #3, Navigating jamovi the jamovi user guide 1.3.4 Modules datalab.cc’s video tutorial on jamovi modules. Scroll to video #7, jamovi modules. 1.4 Conclusion Now that you are somewhat familiar with the layout of jamovi, it’s time to learn how to work with data. This is the topic of the next chapter. We don’t know the official name of this.↩︎ Same here. There seems to be no official name.↩︎ "],["BasicDataOperations.html", "Chapter 2 Basic data operations 2.1 Entering data manually 2.2 Opening data files 2.3 Importing data 2.4 Exporting data 2.5 Manipulating data 2.6 General note: Understanding tidy-ness 2.7 Outside help on basic data preparation 2.8 Practice Exercises", " Chapter 2 Basic data operations This chapter helps you through a number of issues that have to do with data entry, opening it, exploring it, importing it. exporting it, modifying it, etc. The best way to learn in this chapter is to follow along in each section using jamovi on your own computer. There are also some extra practice exercises at the end of the chapter. 2.1 Entering data manually Okay, about the only people who enter data manually anymore are students who are learning about statistical analysis software.10 Nonetheless, entering data manually is a great exercise in getting to know any statistical-analysis software, like jamovi. In fact, there is one case where entering data manually serves a unique purpose in jamovi. We’ll get to that at the end of this chapter since it is not critical. When you start jamovi, or open a new document [i.e., \\((\\equiv)\\) &gt; New], you will see three default variable names, A, B, and C, in the the three leftmost columns of the spreadsheet (jamovi expects you to change these variable names). There is also a tiny, grayed-out column on the far left that gives you the row number of the spreadsheet. The default scale of measurement for variables A, B, and C is nominal (categorical). To see this, just double-click A, B, or C. See Figure 2.1 Figure 2.1: Default view of variables in jamovi. As long as you’re doing this, you might as well save this file as YOUR.LASTNAME_YOUR.FIRSTNAME_jamoviPractice.omv,11 or something like that. The point of saving the file this way is to emphasize a few things: jamovi is a desktop application (not a cloud-based program), so you actually do need to save files on your hard drive You should always use meaningful file names so that you can idenfity what’s in them later on You should have your name in the title of almost any document that you share with your instructor or TA (receiving 22 documents that are all titled “MyJamoviPractice.omv” can be pretty frustrating) Back to the task at hand… When you double-click on the variable name (or single-click, followed by Data &gt; Setup), you’ll see the details of that variable. The topmost box will be the official variable name. This variable name should be short enough to keep you from having to type out really long names repeatedly (e.g., use Eighth Grade Aggression instead of the aggression measurements of kids in the eighth grade), but not so short that the meaning of the variable becomes opaque (e.g., use Eighth Grade Aggression instead of 8GA).12 The next box down, Description, is an alternative description for the variable. This is basically for the researcher’s notes. Be as descriptive as you like. It won’t be output to any graphics or analyses. Below that on the left, you’ll see a vertical series of 4 buttons. These define the scale of measurement that the variable represents. We only use nominal, continuous, and ID in this class. Implicit in continuous are two levels: interval and ratio. Using the drop-down box to the right of the button selections, interval data should technically be coded as integer, and ratio data should be coded as decimal. See Figure 2.2 for an example. Figure 2.2: Re-labeling and changing the scale of measurement. The ID variable is just an anonymous substitute for a participant’s name. It’s either a pseudonym (a fake name, like AAB or Los Angeles or frogma) or a number, but for analysis purposes, it’s just a unique label, not a number. If one or more of your variables is nominal, then you will need to define the levels, which appear in the box to the right, labeled Levels. But the levels won’t appear there magically, if you have a variable Sex with two levels (with 0 for female and 1 for male), then first you’ll need to type in the numbers you need in that column directly in the spreadsheet. These numbers will then appear automatically as the levels in the Setup window for that variable. This is where you’ll need to double-click the box for 0, and type over it with female, and then do the same for 1, and replace it with male (don’t do this in the spreadsheet, but rather here, in the DATA VARIABLE box). What you do here matters for several functions that you will carry out later, particularly graphics. So it’s a good idea to get this right, and to to be consistent for any nominal variable you have. In Figure 2.3 below, we’ve created the variable Sex, made it Nominal/Integer and coded 0s as male and 1s as female.13 Figure 2.3: Assigning values to factors. Now that that’s done, if you start entering the real data manually in the spreadsheet, one of three things will happen. If your data is continuous, and you enter a number, it will stay as that number. Note however, that the maximum number of decimal places that jamovi will display (irrespective of how many you enter) is defined under preferences, or \\((\\vdots)\\) &gt; Results &gt; Number format. If your data is nominal, with numbers representing levels (e.g., 0 = female), then as you enter a number in the spreadsheet (say 1, under Sex), that cell will turn into male as soon as you enter a 1 in there. Underlyingly, it is a 1, but jamovi is displaying it as male to you. If you have your nominal variable coded as text, then you can just type in the level in here (e.g., female). jamovi can deal with both (2) and (3) above for analysis, but method (3) is not really recommended as it’s much more likely that you’ll accidentally introduce a new, unwanted level through misspelling (e.g., famale). See Figure 2.4 below for an example of this, which you might not notice browsing through the spreadsheet. That said, when using method (2), it’s a good idea to make sure that in the drop-down menu under Data type you choose text. Otherwise, you will get some confusing (albeit harmless) statistics when carry out descriptives (the topic of Chapter 3). Figure 2.4: Relative difficulty in seeing a bad level when the data is entered as text. In contrast, if you make a mistake and enter a 2 instead of a 0 or a 1 for Sex, you’ll see a 2 there. It will stand out like a sore thumb. See Figure 2.5 for an example of this. Figure 2.5: Relative ease in seeing a bad level when the data is entered as a number. You can find a practice exercise for this section below: Practice Exercise 2.8.1. Now on to something much simpler, but also much more reflective of what you will actually be doing with data: Opening existing data files. 2.2 Opening data files This really isn’t so much about how to open files (something we assume you all learned back in elementary school), as it is about what files are available to open in jamovi in order to get used to how data looks and works in the software. jamovi comes with a few sample data sets pre-installed. They can be found by clicking \\((\\equiv)\\) &gt; Open and choose one of the four default sample data sets. If you haven’t already done so, you should also install the module for the main textbook for the class (Navarro &amp; Foxcroft, 2019). To install it go to Analyses &gt; Modules (+) &gt; jamovi library &gt; Available &gt; learning statistics with jamovi &gt; Install. As noted above, you’ll find the sample data sets (all from Psychology) under \\((\\equiv)\\) &gt; Open &gt; learning statistics with jamovi. The main textbook will refer to these data sets throughout the course. You can see this step in Figure 2.6 below. Also, in contrast to the four default data sets in jamovi, not all the data sets from Navarro &amp; Foxcroft (2019) are native-jamovi .omv files; some of them have different file extensions, like .csv. We’ll get to that, especially under 2.3. Finally, you should also install the module called R data sets, as you will work with one or more of these in this lab manual. Figure 2.6: Finding sample spreadsheet data in jamovi. 2.3 Importing data jamovi is not data-collection software.14 This means that when the data file is not given to you from within jamovi (e.g., sample data sets), you will need to get the data from wherever you collected it into jamovi. This is known as importing data. There are a variety of ways that this might take place, but two of them are far more common than others: - importing data from a proprietary data format - importing data from a non-proprietary delimited text file A proprietary data format is primarily one generated by commercial software. For instance, SPSS stores its data in .sav files. SAS stores its data in .sas7bdat files15 and Stata in .dta files. Usually, jamovi will open these directly as it uses powerful R packages, like haven, that know how to open these files. Just click \\((\\equiv)\\) &gt; Open &gt; This PC &gt; Browse and find the file that you need to open (it needs to be downloaded on your computer already). But the most convenient and reliable way to transfer data from computer to computer and software to software is through an intermediate data-file structure known as a delimited text file. This is a plain-text file that can be opened by pretty much any software (free or commercial) that reads text files in any way. A plain-text file is really the simplest of file formats. But a delimited text file is a special case: It takes data that is in table format, and converts it into plain text, where rows from your data frame usually correspond to new lines, and column boundaries from within rows in the data frame are converted into specific characters, like commas, semicolons, or tabs. The top-most row often, but not always, contains the variable names (the header). An example of such a file can be seen in the jamovi Quickstart Guide. We converted the data set from Navarro &amp; Foxcroft (2019) called Clinical Trial into both comma-delimited and tab-delimited files. When you open them up with a simple text editor like Notepad or Text Edit, you’ll see something like Figure 2.7 below. It is in raw, plain-text, comma-separated (.csv) format on the left, and in its tab-delimited format (.txt or .tsv) on the right.16 Figure 2.7: Examples of comma-delimited (left) and tab-delimited (right) files jamovi opens delimited text files just as easily as proprietary data files. In Excel, there’s generally a wizard that you have to get through to import delimited files. In contrast, jamovi imports them straight up, no wizards. Just click \\((\\equiv)\\) &gt; Open &gt; This PC &gt; Browse and find the delimited file. jamovi will do its best to import the data. It pretty much always works unless there is something wrong with the data file itself. You can try this yourself by opening the data set booksales, located at the top of the data sets available from Navarro &amp; Foxcroft (2019). Just go to \\((\\equiv)\\) &gt; Open &gt; Data library, and look for booksales at the top. It’s a comma-delimited text file. It opens up quite easily. However, you will need to go in and change the data types for a few variables. It seems to treat numbers as nominal data. Beware however, that jamovi will sometimes import defective delimited files and assume that they’re not defective. This results in a bad data set. You often need to check your visually. If you data set is large, then you can use descriptive statistics (covered in Chapter @ref(Descriptive Stats)) to find evidence of this kind of problem. To illustrate this, we deliberately distorted a file containing reaction times, and then imported it through jamovi. In this case, we removed the commas from the very last line of data. The results can be seen below in Figure 2.8 below. Figure 2.8: jamovi importing a bad comma-delimited file, where the final line of data had its commas removed. As you can see, there were supposed to be 3 columns of data: an ID variable (participant), a condition variable, and the dependent (outcome) variable rt (reaction time). But since the commas were removed from the last line, jamovi thought that all three variables were really just one, and assigned that value to participant. You should always be on the lookout for such data-import errors in your own work. For practice on importing data, go to Practice Exercise 2.8.3, but you might want to wait until you read the next section, as well as do the practice exercise for exporting. The reason is that you will be importing data that you export (from Google Sheets, or manually) for those exercises. 2.4 Exporting data All statistical software (e.g., SPSS, SAS), all spreadsheet programs (e.g., Excel, Google Sheets), and all database software (e.g., MS Access, SQL, Filemaker) will export their native tables/data frames into delimited text file formats. Crucially for those of you collecting your own data in groups, Google Forms also does this (as long as you have already collected some data). You should take advantage of exporting your data into delimited text files in all cases except when you are absolutely positive that your colleague has the exact same software as you do (e.g., you both have jamovi, or SPSS, or SAS, etc.). The comma-delimited file usually has a specific .csv extension (meaning “comma-separated values”). Tab-delimited files use a .tsv or just .txt extension. NOTE: There is something you should understand about these delimited text files however. This is ultimately not that important, but may help to eliminate some confusion down the road. Delimited text files are non-proprietary (not owned by any particular company). So like we said, just about any software can open them and read them as they are supposed to be read. Moreover, the file extensions that go with them (e.g., .csv, .tsv) are simply informal user conventions that serve as “clues” for your computer and/or software as to how to open them. So a .csv extension tells your software that it is likely (but not assured) that this is a delimited file with commas acting as delimiters. The less-universal .tsv extension tells your software to guess similarly, but to expect tabs instead. All this helps explain a common phenomenon that many novice data analysts find confusing. Specifically, many data-collection software programs say that they will export to Microsoft Excel, and do so with an .xls file extension. Then when you open it with Excel (which your computer will probably do automatically if Excel is installed), Excel tells you that it is not actually an Excel file after all, and that you have to import it as a delimited text file. This is confusing until you realize that what these software programs are doing is creating a valid, text-delimited file, but arbitrarily pasting an .xls extension to the end of of the file name. The extension tells your computer to use Excel specifically to open it (and Excel can do so), but since it is not actually a proprietary Excel file, Excel will instead sent it through a delimited-file import procedures. You can actually try this trick yourself by clicking any delimited text file that you have created, and changing the extension to .xls. Then double-click it. If you have Excel installed, your computer will probably call up Excel to open it, but then Excel will have to import it. Presumably, these data-collection software programs do this so that people do not have to figure out which program to use to open these files. This is useful for novices, but a little annoying for experts. One thing you want to make sure of is that you don’t have any of these special delimiters inside the values of your delimited file. This will split up your data in places you don’t want it to split. For this reason, linguists, for instance, often avoid comma-delimited files since a lot of language data has commas in it (e.g., passages scraped from the Web). Linguists would prefer something like tabs, or something never used in natural text, like pipes.17 You also have to be careful receiving data from countries that use commas to indicate decimals instead of periods. Finally, you can also create your own delimiters (e.g., the pipe, or semicolons), but you will need to inform your colleagues what your non-standard delimiter is before you share the file with them. Data-analysis software programs always have options do deal with these sorts of anomalies. For practice on this, go to Practice Exercise 2.8.2. And don’t forget to follow that exercise with the practice exercise on importing data (Practice Exercise 2.8.3). 2.5 Manipulating data The three main types of data manipulation that you can carry out in jamovi are transforming, computing, and filtering. We will only cover the latter two here. Data transforming in jamovi is for creating formulas that can be applied to multiple variables. It is a bit beyond the scope of this class, but a nice tutorial from datalab.cc can be found here as video #15. 2.5.1 Computing new variables We are going to show you three different ways to compute new variables in this section: Recoding new variables based on the values in existing variables Generating random variables Creating data transformations 2.5.1.1 Recoding new variables You can add a new variable to a data set, based on variables that you already have. One of the most common reasons to do this is if you have one or more other variables already in your data set that you want to derive alternative information from in order to change your analysis or make it simpler.18 For instance, you may have a variable called ClassLevel that has the following four levels: Freshman, Sophomore, Junior, and Senior. But it is possible that you’d want to create a new variable called ClassLevelStatus (for lack of a better term), with the following two levels: Upperclassman and Underclassman.19 Doing something like this is pretty simple. As illustrated in Section 2.1, you can create an artificial data set with, say, 4 observations each of Freshman through Senior under a variable called ClassLevel. But now we want to create a new variable called ClassLevelStatus. We started by going to the Data tab in jamovi, double-clicking the header of a blank variable, and clicking the \\(f_{x}\\) NEW COMPUTED VARIABLE icon in the middle of the list. We gave the new variable the name ClassLevelStatus (the default was a letter, like D). You should now have something that looks similar to Figure 2.9 below. Figure 2.9: Initial window when computing a new variable based on an old variable To create the new variable, we needed a nested if-then-else formula.20 We could have clicked the \\(f_{x}\\) drop-down box and find the IF() function under the Logical: heading. But we needed something more complex. The syntax for what we needed is below. We just typed this in to the box next to \\(f_{x}\\). IF(ClassLevel=='Freshman','Underclassman',IF(ClassLevel=='Sophomore','Underclassman',IF(ClassLevel=='Junior','Upperclassman',IF(ClassLevel=='Senior','Upperclassman','error')))) You can interpret this formula in plain English as follows: If the value of ClassLevel is Freshman, Then place the value Underclassman here, under the current variable (ClassLevelStatus). (Else) If that’s not the case, then check to see if the level is Sophomore; If it is, then also put Underclassman here. (Else) If that’s not the case, then check to see if the level is Junior; If it is, then put Upperclassman here. (Else) If that’s not the case, then check to see if the level is Senior; If it is, then also put Upperclassman here. (Else) If none of those are true, then put error here. After we did this, jamovi looked like the Figure 2.10 below. Figure 2.10: Computed-variable window after the nested if-then-else formula was added. Note the new values that populated the ClassLevelStatus variable in the spreadsheet. What’s going on is that the basic formula is if-then-else, but the else part of the formula can be replaced with another if statement. This is nesting, and it is very, very useful for recoding variables. NOTE: You might be wondering why we need two equals signs (==) in the computed variable, and not just a single equals sign (=). Recall that jamovi is based on R. In R, two equals signs are used for evaluating particular values. In this case, it can be interpreted as “where it is equal to.” In contrast, a single equals sign (=) is used for various other, common operations, including the creation of variables, the assignment of arguments to functions, etc. In addition, the exclamation mark (!) means NOT in R and jamovi. So != means “where it is not equal to.” Thus, the equivalent for our purposes would be =ClassLevel != ‘Freshman’ (i.e., “if the value is NOT ‘freshman,’ then…”). For numerical variables you can also use the symbols &lt; for “less than,” &gt; for “greater than,” &lt;= for “less than or equal to” etc. Your main textbook (Navarro &amp; Foxcroft, 2019) covers this in Section 6.2. You might also have noticed the weird sequence of closing parentheses at the end of the formula. This is due to the fact that each IF() statement must have both opening and closing parentheses. The open throughout the formula, but only close at the end. They are actually embedded, like If(IF(IF(IF))), similar to those Russian dolls. 2.5.1.2 Generating random data As noted above, there is a special case of manual data entry that you might find yourself doing for this lab. Many software programs can generate data randomly [e.g., the RAND() and RANDBETWEEN() functions in Microsoft Excel]. In jamovi, this can be done quite easily. Go to the Data tab. Then create a new variable by double-clicking at the top of a blank column. You will see a small menu appear with NEW DATA VARIABLE, NEW COMPUTED VARIABLE, and NEW TRANSFORMED VARIABLE. See Figure 2.11, below, for a depiction of what you should see. Figure 2.11: Options for creating new variables. Click the middle one: NEW COMPUTED VARIABLE. Give the new variable an appropriate name. The example we have chosen Random Normal, which means that it’s going to result in numbers generated randomly from a normal distribution that we define with a user-specified mean and standard deviation. There is a small box labeled \\((f~x~)\\). Click it and scroll down to the bottom of the Functions window where it finally says Simulation. See Figure 2.12, below, for a depiction of what you should see. Figure 2.12: Finding the formula for a random normal distribution Double-click NORM, and then look in the box to the right of \\(f_{x}\\), and you’ll see the following formula: =NORM(). Between the parentheses, type in two numbers separated by a comma. The number to the left of the comma will be the mean of your randomly generated data, and the number to the right of the comma will the standard deviation of that data. For illustration purposes (see below), we have chosen the mean to be 65, and the standard deviation to be 16. You can choose whatever values you want. Press the ENTER/RETURN key. Once you do that, start typing in numbers (or names, or whatever) in one of the other default columns (you can rename it “ID” later, if you were so interested). You can also skip rows (as we did to illustrate). Then look what happens under the variable Random Normal. You should see random numbers being put into the cells between the topmost row with a value, and the bottom-most row with a value. These are randomly generated by jamovi. The results of this little simulation here can be seen below in Figure 2.13 below. Figure 2.13: Inserting the formula and observations for a random, normal distribution with a mean of 65, and a standard deviation of 16. You will get some actual practice doing this in a later exercise (Section 6.2.2). 2.5.1.3 Data transformations Sometimes variables have inherently non-normal distributions. This makes them difficult (though not impossible) to analyze because there’s an important assumption that applies across all the types of statistics that we do in this class (with the notable exception of chi-square in Chapter 5). This assumption is that the residuals (or deviations) or normally distributed. These are the distances of each observed value from the model (in the case, the model is the mean).21 If you measure all these distances, and put them in, say, a histogram, you should see a normal distribution (i.e., a bell curve). If you don’t see a bell curve, then you often apply a transformation. For instance, reaction-time data is almost always skewed positively. The reason for this is that although you can take almost as long as you want to press a button in response to a stimulus, it is very difficult for you to respond faster than, say, 300 milliseconds (in the easiest of tasks). This means that really fast responders will have many reaction times near the low end, clustering around 300 ms, but a few at the high end (for a confusing stimulus; or perhaps they got distracted; there are many reasons, actually). A solution that is typically applied in reaction-time analyses is the log transformation (or a square-root transformation, or a reciprocal transformation; there are several transformations available). 2.5.1.3.1 Log transformations If you’ve forgotten what logarithms are, they are (in base 10) the value you need to exponentiate 10 by in order to reach the number in question. Thus, the logarithm of 1000 is 3 since \\(10^3=1,000\\). The log of 100 is 2 \\((10^2=100)\\), and the log of 10,000 is 4 \\((10^4=10,000)\\). You may have noticed that by doing this, extreme values get “pulled in” to the left. So a reaction time of 532 milliseconds (ms) is 2.726 \\((10^{2.726}=532)\\), and a reaction time of 2,345 ms is 3.37 \\((10^{3.37}=2,345)\\). So 2.726 and 3.37 are much closer to each other than 532 and 2,345. Below is some raw reaction-time data.22 Our only purpose here is to show you the positive skew in a histogram, and how applying a logarithm to the data brings in the skew. ## ## DESCRIPTIVES Clearly, the reaction-time data is extremely skewed, with some reaction times as high as 8,000 ms (8 seconds!). Now we will apply a log transformation to the data. For those at Texas A&amp;M, this data set is located in eCampus as BasicDataPrep_RTdata.csv. When the data is opened, we changed the participant variable to the variable time ID. The variable to the right of that was the raw reaction times (rt). We double-clicked the column to the right of that, and then clicked the icon above labeled \\(f_{x}\\) NEW COMPUTED VARIABLE (not TRANSFORMED, ironically). It automatically gets a letter name (e.g., C), but we changed that to log10_rt (for rt with a base-10 log transformation). Just to the right \\(f_{x}\\) symbol, you can see a small down triangle \\((\\blacktriangledown)\\). We clicked that and searched down for LOG1023 and double-clicked it. The formula for the transformation appears in the box to the right. Inside the parentheses we typed rt (since that was the variable we were applying the log transformation to). We then pressed enter, and watched as the new, logged values appeared in the new column. Most of what we have described above appears in Figure 2.14 below. Figure 2.14: Parameter settings to apply a log transformation (base 10) to positively skewed data. Now, we can create of histogram of the new, logged reaction-time variable. It is still slightly skewed to the right, but it is much better than before. ## ## DESCRIPTIVES 2.5.1.3.2 z-scores That said, it is probably warranted to remove a few outliers. The main textbook Navarro &amp; Foxcroft (2019) covers outliers in Chapter 5, and in Chapter 12, but they deserve some more attention here. Using the same procedure as above, we will generate z-scores for the data. What are z-scores? Z-scores are used to re-cast raw scores of any interval or continuous variable into standard-deviation units. One advantage of this is that you standardize the scores. We’re not being tautological here.24 Rather, think of how standardized parts revolutionized manufacturing. Instead of each, say, wheelwright making wheels to their own dimensions, societies could invent standards that all manufacturers must follow. This made the world much easier to live in because you could get a wheel in one town, move to a different county or country, and replace the wheel there. You couldn’t do that before. Standardizes scores (z-scores) are kind of the same thing. Whereas before you could not compare, say, raw IQ scores to raw cumulative school grades, standardizes scores let you do just that, as all continuous or interval scores can be converted into a scale where 0 means the mean, whole integers refer to the number of standard deviations, and the polarity refer to whether that particular is above (+) or below (-) the mean. For example, a z-score of 0 means average (the mean, \\(\\overline x\\)); a z-score of 1 means “located at one standard deviation above the mean;” and a z-score of -1.75 means “located at 1.75 standard deviations below the mean.” In jamovi, we calculate z-scores by clicking on a new, blank column, selecting \\(f_{x}\\) NEW COMPUTED VARIABLE, naming the new variable something like log_rt_zscores clicking the \\(f_{x}\\) box and scrolling down to the function Z, double-clicking it, followed by double-clicking log10_rt (under the right-hand column Variables) to place it in between the parentheses (or just typing the variable name there). This can be seen in Figure 2.15 below. Figure 2.15: Parameter settings to calculate z-scores on the log transformation of the reaction-time data. After pressing the z-scores are calculated for that column. It’s that simple. Now let’s go back to the problem of extreme observations (i.e., outliers). A conservative approach to filtering extreme observations is to remove observations with z-scores more extreme than 2 or 2.5 (positive or negative). These correspond to values that are beyond 2 or 2.5 standard deviations away from the mean, so they comprise about 5% or 1% of the observations, respectively.25 You can see why in the histogram below. NOTE: Often, researchers do not remove extreme values, but rather trim them (or windsorize them). These approaches change the raw values to scores lying at the extremes, but they do not remove the observations altogether. The decision about which approach to use depends on the study at hand and the tradition of the particular field the study is in. Below, we will show you how to remove these outliers, after we show you how to remove observations from reluctant participants. If you wanted to trim outliers instead of deleting them, you would need to compute a new variable as similar to what was demonstrated in Section 2.5.1.3 above, not use the filtering procedure demonstrated below. 2.5.2 Filtering data Just like R, unless you tell jamovi otherwise, it will analyze all the observations (rows) in your data. All good statistical software does this.26 In order to restrict the analysis to certain rows, you need what is called a filter. For instance, it is common to have a question at the end of a survey that asks the participant whether they are still comfortable with the researcher(s) using the data they provided for general analysis. The questions could be as simple as “Is it still okay to use your data for analysis? Remember, your data will be completely anonymized, and so in no way will it be traceable back to your identity.” Some people are not comfortable with having their data analyzed, and so they answer, “No.” Researchers need to filter these observations out. It is easy enough to create such a data set, so we will do so using the technique learned above in Section 2.5.1.2. We opened a new file in jamovi [\\((\\equiv)\\) &gt; File &gt; New]. Under column A, we typed in a 1 in the first cell, and changed the variable type to ID, and renamed the variable ID. For variable B, we renamed it Consent (with variable type set to Nominal and text). Then we typed in Yes in the first cell. Finally, we deleted variable C, and double-clicked on a blank variable and chose NEW COMPUTED VARIABLE. We renamed that to rt, and then applied the NORM() function to the first cell (see Section 2.5.1.2 above). We arbitrarily set the mean to 615 ms, and the standard deviation to 112 [hence, NORM(615,112)]. We then typed in “20” in row 20 under ID. Under Consent we copied-and-pasted Yes from row 1 all the way down to row 18. But for the cells in rows 19 and 20, we typed No. This would mean that fake participants 19 and 20 did not consent to have their data used. So then we needed to filter them out. This was easy to do. The filter function is located under the Data tab. Once there, we just clicked the icon labeled Filters (at the upper-right). The icon looks like a funnel, half full of liquid. There’s a box there labeled ROW FILTERS. In the box under Filter 1, labeled \\(f_{x}\\), we typed in the following (the first “=” sign is provided by jamovi): Consent=='Yes' This filter equation could be read as follows, like a command to jamovi: “Create a new variable called”Filter 1. For this variable, look under the variable Consent and return only the rows that have Yes as a value in the cell; filter out the others. You can see this below in Figure 2.16 Figure 2.16: Results of applying filter (Consent==‘Yes’) to fabricated lack-of-consent data. In the figure (or your own jamovi file, if you are following along), note the green check marks \\((\\color{green}\\checkmark)\\) versus red times or X symbols \\((\\color{red}\\times)\\) in the new filter variable that appears on the left under the new variable called Filter 1. This means that any subsequent analysis of our variables would be restricted to the observations with green check marks, and those with red Xs will be excluded. That is, the analysis would only apply to people who had consented to have their data analyzed.27 ## ## DESCRIPTIVES ## ## Descriptives ## ─────────────────────── ## rt ## ─────────────────────── ## N 18 ## Missing 0 ## Mean 575.8419 ## ─────────────────────── If you do a simple Descriptives procedure (see Section 3.2 in the next chapter), you can see that N=18 (as above) since the procedure did not analyze the observations that were filtered out (i.e., #19 and #20). To remove the logged reaction-time outliers described in Section 2.5.1.3 above, you would simple repeat the filter above, but the formula in the \\(f_{x}\\) box would be as follows (for deleting observations greater than 2.5 standard deviations from the mean). In this case, we are not using the z-score column we created; rather, we’re calculating the z-score within the filter formula. -2.5 &lt; Z(log10_rt) &lt; 2.5 This can be read as follows: “Filter in any values of log10_rt where the z-score of that variable is not only greater than -2.5, but also less than +2.5” (i.e., filter out the more extreme values that are either less than or equal to -2.5 or greater than or equal to +2.5). You can see this jamovi filter in Figure 2.17 below. Figure 2.17: Results of applying filter to extreme (logged) reaction times. 2.6 General note: Understanding tidy-ness This final section (before covering outside tutorials) is about understanding how data ought to be organized for easy analysis. As such, it’s less about how you prepare data in jamovi, and more about how you should structure data in general, irrespective of the software you are using. Often, when working with data, researchers find themselves dealing with data that was entered in such a way that it can’t be analyzed statistically. This is usually data entered by people who are not used to doing statistical analyses. You might fear that each statistical software program has its own way of organizing data for analysis, but we’re happy to tell you that your fears would be unjustified. As luck would have it (skill, not luck, actually), these programs are written by some very smart people, and they have converged on one way of organizing data: tidy format.28. (Text-delimited files are covered in more detail in the next section) This universal data format has come to be known as tidy data format, a term we can probably trace back to Hadley Wickham, head programmer at RStudio (Wickham, 2014). The briefest description is that in a tidy data set, all of the following are true: all columns are variables (outcome, predictor, or ID variables; but sometimes things like filters or even comments); all rows are observations (or cases): people, rats, etc.; and each cell is one datum29 for that observation on that variable, with no data summaries. Anything other than the tidy data structure above will usually cause problems with data analysis, something that is true across statistical-analysis software. You will not have to worry too much about having non-tidy data sets in this class because, at least for your class projects, Google Forms automatically exports data in tidy format. The sample data sets from jamovi and the class textbook are all tidy. But if you find yourself handling data as a professional some day, even if you are not doing statistics yourself, please remember tidy data format. This is especially true if you are preparing data for other data scientists (e.g., statisticians) to use. They will appreciate your efforts to make their lives easier. 2.7 Outside help on basic data preparation datalab.cc has a general introduction to this part of data analysis. Go to video #9 (Wrangling Data) to see this. As noted previously, at Texas A&amp;M, you can find it under the LinkedIn Learning tutorials through the Howdy! portal. 2.7.1 Entering and adjusting data You should know that for a good overview of what is covered above, you can go to datalab.cc’s videos #10 and #12 (Entering data and Variable types &amp; labels, respectively). 2.7.2 Opening data For further orientation as to how to do what we just explained above, watch the video from the Sample data tutorial from datalab.cc. Scroll to video #4. 2.7.3 Importing data Here is the datalab.cc video tutorial on importing data. Go to video #11. Or here for Texas A&amp;M students. Your main textbook (Navarro &amp; Foxcroft, 2019) also covers this in Sections 3.4 and 3.5. 2.7.4 Recoding variables There is no tutorial at datalab.cc, per se, for re-coding variables in the way described above in Section 2.5.1. However, it does have a nice tutorial on how to average two or more variables into a third. This is extremely useful sometimes. This video is #13. Texas A&amp;M students can find it here. Your main textbook (Navarro &amp; Foxcroft, 2019) covers this in a way not covered here. Specifically, it outlines how to convert a numeric variable to a discrete, nominal variable using formulas.30 This is Section 6.3.2 of the textbook. There is also a tutorial on how to create a transform. Transforms in jamovi are saved functions that allow you to create new variables quickly according to the parameters of the function. This is usually most useful in large data sets with many variables. Go to datalab.cc and scroll to videos #15. At Texas A&amp;M, you can find it here. The jamovi blog (Love, Dropmann, &amp; Selker, 2019) also has a tutorial on this here. The jamovi blog also covers data transforms here Finally, your textbooks also covers this in Section 6.3.3. 2.7.5 Generating random data There is nothing on datalab.cc. However, the jamovi blog has a short blog post on it (a mention, mostly) here. 2.7.6 Transforming variables Go to datalab.cc and scroll to video #14 to learn how to transform your data into z-scores. This is really the same kind of procedure (using \\(f_{x}\\) NEW COMPUTED VARIABLE) as in Section 2.5.1.3 above, and video #13, mentioned above in Section 2.7.4. This can be found at Texas A&amp;M here. The jamovi blog also has a post on computed variables here Finally, your main textbook (Navarro &amp; Foxcroft, 2019) covers this in section 3.3.2. And it covers logarithms and exponentials in Section 6.4. 2.7.7 Filtering data Go to datalab.cc and scroll down to video #16. At Texas A&amp;M, go here. The jamovi blog (Love, Dropmann, &amp; Selker, 2019) also has an introduction to filters here. Your main textbook (Navarro &amp; Foxcroft, 2019) covers this in Section 6.5. 2.8 Practice Exercises 2.8.1 Practice entering data Here is some practice, Exercise 2A, Manually Entering data. 2.8.1.1 The data The textbook by Navarro &amp; Foxcroft (2019) includes a data set about a clinical trial involving 18 participants divided into two treatment groups (Cognitive Behavioral Therapy [CBT] vs. no therapy [no.therapy]). Within each of these groups of nine, three had been taking a drug called anxifree, three had been taking a drug called joyzepam, and three a placebo. The outcome variable is mood.gain, which is some measure of improvement of mood over some time period. The complete data set is displayed below Table 2.1. You will have to use the scroll bar to see the entire data set. Table 2.1: A clinical-trial data set. ID drug therapy mood.gain 1 placebo no.therapy 0.5 2 placebo no.therapy 0.3 3 placebo no.therapy 0.1 4 anxifree no.therapy 0.6 5 anxifree no.therapy 0.4 6 anxifree no.therapy 0.2 7 joyzepam no.therapy 1.4 8 joyzepam no.therapy 1.7 9 joyzepam no.therapy 1.3 10 placebo CBT 0.6 11 placebo CBT 0.9 12 placebo CBT 0.3 13 anxifree CBT 1.1 14 anxifree CBT 0.8 15 anxifree CBT 1.2 16 joyzepam CBT 1.8 17 joyzepam CBT 1.3 18 joyzepam CBT 1.4 Your task is to enter this data manually into a new spreadsheet in jamovi. You’ll need to assign levels to the numbers you use for the two nominal variables. Do not enter them as text. First, open a new spreadsheet in jamovi by clicking \\((\\equiv)\\) &gt; New. Then save it with a reasonable name, like P301.Exercise_2.8.1.omv. 2.8.1.2 The ID variable Second, double-click on the variable named A, and change the name to ID. You can provide a description if you like (this is good for remembering what your variables mean). Click ID as the variable type, and leave the box below that as Integer. Finally, type in 1-18 in the first 18 rows, respectively. You should have something like the results depicted in Figure 2.18, below. Figure 2.18: Entering an ID variable. 2.8.1.3 The drug variable Third, prepare the variable drug with its three levels: anxifree, joyzepam, and placebo. Do this by double-clicking on the default variable B, and renaming it drug. The data type should remain nominal. Associate each of the levels of the drug variable with an integer, associations that will be arbitrary. We have chosen 0 for placebo, 1 for anxifree, and 2 for joyzepam (but you can use whatever integers to refer to whatever levels you like, as long as you don’t repeat them within the same variable). Then, for the first three rows (IDs 1-3) under drug, type in 0; for IDs 4-6 type in 1; and for IDs 7-9, type in 2. You should see something like Figure 2.19 below. Figure 2.19: Entering the drug variable. Once you have done this, you can replace the numbers in the Levels box with the names of the levels. Start with selecting 0, and typing over it with placebo. After this first step, you should see something like Figure 2.20 below. Figure 2.20: Start assigning labels to integer levels in a nominal variable. Then, change overwrite 1 with anxifree and 2 with joyzepam. Click anywhere in the spreadsheet and you should see all the numbers disappear, replaced by the appropriate levels. Your results should look like what’s depicted in Figure 2.21. Figure 2.21: Complete assigning labels to integer levels in a nominal variable. The last thing you need to do for the variable drug is type in the last nine integer values under drug (do not type in labels at this point). In this case, the same sequence of integers is repeated as above: that is, three 0s, followed by three 1s, and then three 2s. As you do this, you should see the labels fill in automatically, as in Figure 2.22 below. Figure 2.22: Fill in the rest of the values under the variable. 2.8.1.4 The therapy variable Fourth, prepare the variable therapy. This is another nominal variable. There are two levels for the variable: no.therapy and CBT (which stands for Cognitive Behavioral Therapy). We will leave doing this to you since it is straightforwardly the same procedure as for drug, above. But pay close attention to Table 2.1. 2.8.1.5 The mood.gain variable Fifth, you need to enter the data for mood.gain. When you click this fourth, blank column, a little menu with three options will appear (jamovi only provides you with 3 default variables). Choose NEW DATA VARIABLE. Rename D (or whatever letter you get there) with mood.gain. Then click the Continuous button, because this variable is going to be on an interval scale. Under the Data type pull-down menu, switch the variable from Integer to Decimal. At this point, you can just enter the data. Do so carefully, however, as the data are different for each row. And double-check your work. When you’re finished, you should see something like what’s depicted in Figure 2.23 below. Figure 2.23: Entering the mood.gain variable manually. You’re done!! with Practice Exercise @ref(#PracManualing) Next you will work on importing and exporting data, but in reverse order. That is, we are first going to export data from a spreadsheet, which corresponds above to section 2.4 Then we will work on importing the data we just exported, which is covered above in Section 2.3. 2.8.2 Practice exporting Here is some practice on Exporting, Practice Exercise 2B, starting with entering data into a spreadsheet. 2.8.2.1 Spreadsheet data A great many researchers end up with their data stored somehow in a spreadsheet. This is not ideal, but it is very common. However, spreadsheets are not very powerful at statistics. For this reason, it’s usually best to export whatever data is in the spreadsheet into a text-delimited file for subsequent import into statistics software like jamovi.31 So to mimic this all-too-common experience, you need to open up a spreadsheet in Google Sheets. The link takes you to Google Docs which includes Google Sheets. Note, you will need to create a Google account if you do not have one already. If you are a student at Texas A&amp;M, you already have this, and we require you to use your university Google account. You can find it here, where you log in to Google Drive. WARNING: When you log in to your university Google account, it is wise to log OUT of any other non-university Google accounts you may have. You will lose functionality if you are connected to a private Google account simultaneously. This apparently has something to do with the fact that Google Apps for Texas A&amp;M is compliant with the Family Educational Rights and Privacy Act (FERPA), whereas the private Google accounts are not. Under Drive in the upper left, click the + New icon to create a new document in your drive, and select Google Sheets. You should see a new Google Sheet open up.32 You can change the name in the upper left from Untitled spreadsheet to something like PracticeExportingDelimitedFiles (or something like that). Just click where it says Untitled Spreadsheet and type over it with the new name (e.g., Practice_Exercise_2.8.2. It’s automatically saved in your Google Drive. In cell A1 (the upper-right-most cell in the grid), type ID. Move over one cell to the right (B1). Type in the name of some two-level nominal variable, like Sex (we’ll get to entering values below). Finally, in the third cell over (C1), type in the name of a continuous variable, like Aggression. Let’s make 6 rows of data. For ID, type in 1-6 in cells A2 to A7, respectively. Under Sex type in a “1” in cells B2 to B4, and a “2” in cells B5 to B7. Under Aggression type in some random values on a scale from 1 to 7. See Figure 2.24 below for an example of how we did this.33 Figure 2.24: Entering data manually into Google Sheets. 2.8.2.2 Exporting to a delimited file The next step is to export the data from the Google Sheet. This is extremely easy. Simply click File &gt; Download as &gt; Comma-separated values (.csv, current sheet). You can see an illustration of this below in Figure 2.25 Figure 2.25: Exporting data into .csv format from Google Sheets. Do this, and, when prompted, save it to somewhere on the same computer where jamovi is located. And remember where you saved it! You could also bypass all this exporting from a spreadsheet program by just entering the data manually into a plain text file, and appending the appropriate file extension. No one does this, but it is useful for illustration. In fact, you can try this yourself by copying and pasting the text below into a program like TextEdit (on a Mac) or Notepad (on a PC), and saving it as a plain-text file with the appropriate file extension. Here is how you would do that (using TextEdit’s menu as the example [Notepad should be similar]): Select the four lines of text below, and then copy it to the clipboard: \"ID\",\"Age\",\"MaritalStatus\" \"Bilbo\",24,2 \"Gandalf\",32,1 \"Boromir\",44,1 Open up TextEdit (or any other word processor) Paste the clipboard material in (you could also try typing it in directly) Go to File &gt; Save, give the file a name, and a .csv extension, and then click Save. Open up jamovi Go to \\((\\equiv)\\) &gt; Import and find the file you just created. This should work as well, though you can see why you’d have to be crazy to do it for large data sets. 2.8.3 Practice importing Here is some practice on Importing, Exercise 2C: importing. Open jamovi. Go to \\((\\equiv)\\) &gt; Import. Once there, optionally select CSV at the bottom where it gives you a drop-down menu called Data files (this will narrow the file search). Click the Browse button at the upper right, and then search for your .csv file. You should see something like what’s in Figure 2.26. Figure 2.26: Starting to import a delimited file in jamovi. Double-click it when you see it. You should see something like you see in Figure 2.27 below. Figure 2.27: jamovi spreadsheet after importing delimited file. Notice that jamovi just added the variables to the right of its default variables. Just delete the default variables by going to the Data tab at the top, selecting the column or any cell for variable A, and clicking the Delete icon in the menu at the top (the one with the vertical red stripe). Repeat this for variables B and C. Alternatively, you could select variable A, then hold down the SHIFT key, and select variable C. Then delete them all at once. This latter method was used in the screenshot depicted in Figure 2.28 below.34 Figure 2.28: Deleting variables in jamovi. In addition to adding unnecessary, default variables, jamovi also doesn’t recognize certain things that are critical to your analysis. It actually has no way of knowing the following, so it’s understandable: ID should be an ID variable, where the numbers should not be math-ed upon (it is NOT the end of the world if this doesn’t get changed, as long as you know to ignore any statistics that get calculated from it); the numbers (1 and 2) for for Sex should represent levels of a nominal variable (it couldn’t have since the levels were never assigned); and Aggression should be continuous. You just change these manually. Double-click ID and then click the ID option. Use the same procedure for Sex as you did in Sections 2.8.1.3 and 2.8.1.4, but assign, say “1” to Female and “2” to Male (or vice-versa; your choice). For Aggression use the same procedure you used in Section 2.8.1.5. That is, just change the variable type to Continuous. Keep the Data type as Integer (or make it so). Make sure to save your results. Congratulations! You’re done In some cases, presumably, data might be collected on paper, and have to be entered manually. And there might be people digitizing certain, important data from analog to digital for archiving purposes. But overall, these sorts of tasks are rare nowadays in psychology.↩︎ .omv being the native file extension for jamovi files↩︎ You might have noticed (or eventually notice) that some researchers (like me) use underscores (_) instead of blank spaces in variable names (periods are also common). This is because in many statistical programming languages like R, spaces in variable names can be problematic and cause errors. You can use spaces in jamovi though, and it works out better for graphics. Just don’t use too many words.↩︎ Yes, we reversed them from how they were described above, just to show you that the number-label associations are wholly arbitrary↩︎ Nor are SPSS, SAS, Stata, R, or any data-analysis software, with the two notable exceptions of MATLAB and Python, which can be used to write programs that collect data.↩︎ Yes, that’s the weirdest file extension in the history of computers.↩︎ You can’t see the tabs because tabs aren’t normally meant to be seen by computer users, but rest assured: they’re there.↩︎ The pipe is the \\((\\mid)\\) character, which is on the same key as the backslash \\((\\backslash)\\). This key is located above your computer’s enter/return key and below the delete/backspace key at the upper-right of the QWERTY keyboard layout (the default in the US)↩︎ Another common reason is if you want to create a new variable that is the sum or average of two or more other variables.↩︎ Pardon the apparent sexism. There seems to be no way to say this without making it sound like socio-economic status (e.g., “Upperclassperson”)↩︎ This is very common. Microsoft Excel also allows you to do this. In fact, the syntax is identical in Excel as the form is nearly universal.↩︎ This might be slightly confusing, but the normality assumption, the assumption of a normal distribution, is not technically about the outcome variable itself, but rather about the residuals/deviations that are left over after a statistical model has been applied.↩︎ courtesy of the R package trimr.↩︎ You will also see LN there, which is a natural-log transformation, where the base isn’t 10, but rather the irrational base e, or 2.718281… To be sure, the natural log has special mathematical properties, which you can read about here, but we have never seen it make a difference in the log transformations of reaction-time data.↩︎ Well, maybe a little.↩︎ Technically, the area below a normal curve at and beyond a z-score of +2 is 2.275%. We often use a z-score of 2 as a cutoff since it is easier than remembering 1.96, which is the z-score beyond which 2.5% of the observations should occur under a normal distribution. Multiplied by 2 for the two sides of the distribution, this comes to 5% of the data. The same is true of using a z-score of 2.5. It is easier than remembering 2.57, which, if used as a cutoff, will cut off 1% of the data (only 0.5% of the scores lie beyond 2.57 standard deviations above the mean; multiplied by two equals 1%). Ultimately, cutting off 5% or 1% of the extreme values using z-scores of 1.96 and 2.57, respectively, is really just as arbitrary as using z-scores of 2 and 2.5 to cut off 4.55% or 1.242% of the data, respectively. The results are really just about the same anyway. It’s just easier to remember 2 and 2.5 than 1.96 and 2.57.↩︎ Spreadsheets like Microsoft Excel do not do this necessarily, which is one reason why they can be so dangerous for data analysis. This deficiency is exactly what caused the “Excel error heard around the world.” (you can Google that phrase)↩︎ If you click the “eye” symbol, it will hide the rows with \\(\\color{red}\\times\\)’s in them. Click it back, however, before you proceed below, assuming you are following along.↩︎ Note that this is not true for spreadsheets like Microsoft Excel and Google Sheets. Spreadsheet programs like Lotus 1-2-3 were originally designed for accountants, and because of that, they offer so much flexibility and power that proper data storage and statistical analysis can quickly become problematic. Spreadsheets also astoundingly opaque when you share it with someone else (it’s notoriously difficult to unpack a developed spreadsheet). Nonetheless, because of their apparent simplicity and rapid learning curve, many people use spreadsheets to store data, and even analyze it there. One humorous/not-so-humorous example of a major mistake using spreadsheets was the “Excel Error Heard around the World,” (Google this) caught in 2013 by the (at-the-time) 28-year-old Thomas Herndon. That said, spreadsheets can be useful for some things in data analysis, but one must be extremely careful. A better alternative is usually a delimited-file editor like CommaChameleon↩︎ It’s not usually recommended nowadays to use the singular form for data (lest you sound pedantic or ostentatious, since it really has fallen out of normal usage), but in this case it’s justified.↩︎ This practice is possible, but generally frowned upon since it necessarily involves the loss of information.↩︎ jamovi itself will also export data into different formats (as long as it has some data). To do this go to \\((\\equiv)\\) &gt; Export. There are both proprietary (e.g., SPSS’ .sav format) and text-delimited formats (e.g., .csv). We won’t be doing this in this class, but it may be useful when sharing your data with others who don’t use jamovi.↩︎ It is essentially the same as Microsoft Excel (commercial) or Libre Office Calc (free), among others.↩︎ To do this in a non-thinking way, a function in Google Sheets called RANDBETWEEN was used. You can see this in the formula bar just above the column references. The exact function is =RANDBETWEEN(1,7), which means: “Produce a random number (integer) between the values of 1 and 7.” It was then copy-and-pasted from C2 to C3 through C7.↩︎ You can also do all this before importing the new data.↩︎ "],["DescriptiveStats.html", "Chapter 3 Descriptive statistics 3.1 Overview 3.2 Descriptives in jamovi 3.3 Outside help on descriptives 3.4 Practice summarizing and reporting 3.5 Glossary of descriptives", " Chapter 3 Descriptive statistics 3.1 Overview This chapter shows you how to generate the basic descriptive statistics (or data summaries35) that subsequent chapters rely on. It is tightly aligned with Chapter 4 in your classroom textbook by Navarro &amp; Foxcroft (2019). As in the last chapter, the best way to learn in this chapter is to follow along in each section using jamovi on your own computer. There are also some extra practice exercises at the end of the chapter (section 3.4). Following that is a quick glossary of important terms in this chapter. There are really three types of descriptive statistics, which are covered more in depth in Navarro &amp; Foxcroft (2019), Chapters 4 and 5. These three types are central tendency, dispersion (or variability), and diagnostics. Central tendency tells us what the best prediction would be for a person (or a rat, or a plant) on any given measure, assuming you had no other information available. Dispersion gives us convenient numbers that indicate how accurate (non-noisy) our measure of central tendency is. Finally, diagnostics tell us various things, but most importantly in this class, whether we can be confident that the dispersion (see above) has a normal distribution. It would also be a good idea to include sample size and missing values here, along with frequency-of-category data, since diagnostics the closest statistical description. There is one more category here, but it covers all three types of descriptive statistics. This is data visualization. That is, all descriptive statistics can be represented as numbers, or depicted visually. We will cover data visualization in Chapter 4. But for now, you can also consult the main textbook (Navarro &amp; Foxcroft, 2019), Chapter 5 (Drawing graphs), which is all about data visualization in jamovi. 3.2 Descriptives in jamovi To briefly illustrate descriptive statistics in jamovi in this lab manual, we will use main textbook’s (Navarro &amp; Foxcroft, 2019) data set Clinical Trial again (we used this data set previously in sections 2.3 and 2.8.1). You can get to this data by clicking \\((\\equiv)\\) &gt; Open (Data Library) &gt; learning statistics with jamovi &gt; Clinical Trial. Once open, if you click the Analyses tab on top, then click Exploration on the left, a drop-down menu will appear where you select Descriptives. You’ll see a window with the variables from the data set on the right in a box. If you want to analyze them, you need to select them and click the arrow to get them into the right box (either Variables or Split by). You can see how this all begins in Figure 3.1 below. Figure 3.1: The opening Descriptives window in jamovi (with the data set ‘Clinical Trial’ from Navarro &amp; Foxcroft, 2019). The green box (not seen in jamovi) encloses measures of central tendency. The violet/purple box at the lower left encloses measures of dispersion/error. The blue boxes enclose either simple measures or measures that can be used as diagnostics. The data are summarized in some, but not all ways. We chose descriptives that are either most often reported, or most often covered in introductory courses. These are as follows: Central Tendency Mean Median Mode Dispersion Standard deviation (“Std. deviation”) Range Minimum Maximum Diagnostics Frequencies (of nominal variables by level) Number of observations (“N”) Missing values (“Missing”) 3.2.1 Summarizing continuous variables For reasons of simplicity, we also chose only two variables to analyze: drug, which is one of the predictors, and mood.gain, the outcome variable for this data set. These choices can be seen below in Figure 3.2. The results are below that. Figure 3.2: Parameters for certain descriptive statistics in jamovi (with the data set ‘Clinical Trial’ from Navarro &amp; Foxcroft, 2019). ## ## DESCRIPTIVES ## ## Descriptives ## ─────────────────────────────────────────── ## drug mood.gain ## ─────────────────────────────────────────── ## N 18 18 ## Missing 0 0 ## Mean 0.8833333 ## Median 0.8500000 ## Mode 0.3000000 ## Standard deviation 0.5338539 ## Range 1.700000 ## Minimum 0.1000000 ## Maximum 1.800000 ## ─────────────────────────────────────────── The first thing you might notice is that the variable drug only shows the results for N and Missing. This is because it is a nominal variable, and you can’t do very much with nominal data except describe frequencies, detect missing values, and split data up into subgroups (the function of the Split by box below the Variables box). Under the mood.gain variable is where you see most of the work done. In addition to N and Missing, you see all the other descriptive statistics listed above that can be carried out on continuous data (interval or ratio). TIP: One useful function to highlight at this point is how to remove analyses (or visualizations) that you no longer wish to include. Just CTRL-click (or right-click) anywhere on the output, then select Analysis &gt; Remove. It will disappear. See the figure below. Figure 3.3: Removing output from jamovi. TIP: Another useful function, using that same CTRL-click (or right-click) function, is that you can also save or copy a table or image for use in another document. So if you wanted to take the table output, save it as a .pdf, then insert it into this document, you would CTRL-click the table, go to Table &gt; Save and save it as a .pdf file. This image could then be displayed in many different types of presentations. See Figure 3.4 below. datalab.cc has a video tutorial on this, which is video number 25, also available in the LinkedIn Learning tutorials in the Howdy! portal. Figure 3.4: How to save a table in jamovi as a .pdf file for further use. 3.2.2 Grouped summaries Now we will cover how to view descriptive statistics by group in jamovi. Recall that we can’t get very many descriptive statistics from nominal variables. However, where they are very useful is in splitting up your outcome variable into various groups. Using the same data, Clinical Trial, we will split mood.gain into six groups, namely, those getting: anxifree while getting CBT therapy anxifree while getting no therapy joyzepam while getting CBT therapy joyzepam while getting no therapy a placebo while getting CBT therapy a placebo while getting no therapy To do this, you drag mood.gain into the Variables box, like before. But instead of also dragging the nominal variables drug and therapy into the Variables box, you instead drag them into the Split by box. The parameter choices are depicted in Figure 3.5 below. Figure 3.5: Setting parameters for descriptives statistics split by nominal variables in jamovi (with the data set ‘Clinical Trial’ from Navarro &amp; Foxcroft, 2019). This outputs descriptive statistics for each of the 6 groups (though only the mean and standard deviation were included in order to simplify the output). You can see the results below. ## ## DESCRIPTIVES ## ## Descriptives ## ───────────────────────────────────────────────────────────── ## drug therapy mood.gain ## ───────────────────────────────────────────────────────────── ## Mean anxifree CBT 1.033333 ## no.therapy 0.4000000 ## joyzepam CBT 1.500000 ## no.therapy 1.466667 ## placebo CBT 0.6000000 ## no.therapy 0.3000000 ## Standard deviation anxifree CBT 0.2081666 ## no.therapy 0.2000000 ## joyzepam CBT 0.2645751 ## no.therapy 0.2081666 ## placebo CBT 0.3000000 ## no.therapy 0.2000000 ## ───────────────────────────────────────────────────────────── 3.2.3 Frequencies Lastly, when describing our data (say, in a Methods section in a manuscript), we usually need to provide an indication of the relative numbers of responses in each category. For instance, the reader/listener will usually want to know how many responses there were in our data set across the levels of the drug condition as well as across the levels of the therapy condition.. To do this, you need to slide the two nominal variables, drug and therapy into the Variables box. Then simply check the Frequency tables check box just above the Statistics sub-menu label. Figure 3.6 shows you how to do this. Figure 3.6: Setting the parameters for the frequencies of different nominal variables independently. The results of this analysis are below. As you can see, this creates two new tables that give us the counts and proportions of responses in each level of each variable in the Variables box. Here, the data are symmetric and balanced across conditions. But this is not always the case. These descriptive statistics need to be provided to the reader/listener as well. ## ## DESCRIPTIVES ## ## FREQUENCIES ## ## Frequencies of drug ## ──────────────────────────────────────────────────── ## Levels Counts % of Total Cumulative % ## ──────────────────────────────────────────────────── ## anxifree 6 33.33333 33.33333 ## joyzepam 6 33.33333 66.66667 ## placebo 6 33.33333 100.00000 ## ──────────────────────────────────────────────────── ## ## ## Frequencies of therapy ## ────────────────────────────────────────────────────── ## Levels Counts % of Total Cumulative % ## ────────────────────────────────────────────────────── ## CBT 9 50.00000 50.00000 ## no.therapy 9 50.00000 100.00000 ## ────────────────────────────────────────────────────── There is another way to do this, where you split one nominal variable by another. This will put the two variables into the same table. To do this, just slide one of the nominal variables into the Variables box, and the other into the Split by box. This was done for these data, as can be seen in Figure 3.7 below. Figure 3.7: Setting the parameters to examin the frequencies of different nominal variables jointly ## ## DESCRIPTIVES ## ## FREQUENCIES ## ## Frequencies of drug ## ───────────────────────────────── ## drug CBT no.therapy ## ───────────────────────────────── ## anxifree 3 3 ## joyzepam 3 3 ## placebo 3 3 ## ───────────────────────────────── Here there is one table, with the Variables variable (drug) represented vertically on the left, and the Split by variable (therapy) represented horizontally across the top. Note that there are a total of 18 observations, where 3 belong to each combination of levels across the two variables. This is a more succinct summary than the one in the first table produced above. 3.2.4 Reporting data summaries A great deal of this lab is devoted to writing. The first step will be writing quick summaries of descriptive statistics, something that usually takes place near the beginning of the Results section of a scientific research paper. However, some of these statistics will be reported in the Method section. CAUTION: You might be tempted to put all possible descriptive statistics in the Results section of the paper (the one that comes after Method), but this is not true of all variables. The Results section is about reporting on inferential statistics, or your main analysis (whatever that may be). The descriptive statistics you provide there are the ones relevant to your inferential statistics (i.e., the means and standard deviations that correspond to the levels reported in your analyses). Some of your descriptive statistics (e.g., the total number of participants [N], the number of men vs. women, the average age, etc.) often go in the Methods section, especially if they are not critical to your main analyses. So below we have inserted, in table format, the results of a simple Descriptives procedure on the Clinical Data provided by Navarro &amp; Foxcroft (2019). ## ## DESCRIPTIVES ## ## Descriptives ## ─────────────────────────────────── ## mood.gain ## ─────────────────────────────────── ## N 18 ## Mean 0.8833333 ## Median 0.8500000 ## Standard deviation 0.5338539 ## Range 1.700000 ## Minimum 0.1000000 ## Maximum 1.800000 ## ─────────────────────────────────── Here is how it could be summarized at the beginning of the Results section: Method … There were three participants in each combination of drug (joyzepam, anxifree, and placebo) and therapy (therapy vs. no therapy), for a total of 18 participants. Results …There was an increase on average for all 18 participants on mood gain (M=0.883), but there was also a significant amount of variability (SD=0.534), with the gain ranging from 0.1 to 1.8 across participants. NOTE: Keep in mind that the summary above for the Method section is extremely minimal. For instance, frequencies of Gender and average Age are also usually reported, among any other variables that the researcher deems relevant. Eventually, you will see that whatever was reported in the Method section is material that would not be reported in the Results section, at least not directly. That said, Sex and average Age are almost always reported in the Methods section, irrespective of their use in the main analysis. More information on how to report on different elements of the study in the Method section will be provided later in Chapter 14. 3.3 Outside help on descriptives To reinforce your learning here, you can go to datalab.cc (Poulson, 2019), which provides a short video on why we summarize data in the first place, along with the options available in jamovi (generally). You can find it as video #17. After that in video #18, datalab.cc provides a more detailed tutorial on how to get descriptive statistics in jamovi. It is video #18. The jamovi quickstart guide by Rafi (2019) (Chapter 2) also covers Descriptives, but only very briefly. Finally, the online textbook Statistics for Psychologists by Wendorf (2018) covers descriptive statistics in jamovi in two ways. First, it shows you how to do it. Just click JAMOVI &gt; JAMOVI: Using the Software. Then scroll down to the Table of Contents and click the third entry down DESCRIPTIVES…. Critically, he also provides a guide on how to interpret the output. In this case, you click JAMOVI &gt; JAMOVI: Annotated Output. Then scroll down to the Table of Contents and click the first entry DESCRIPTIVES. Here you can see extensive annotations for each kind of output. 3.4 Practice summarizing and reporting Practice 3A. And now for some practice. To do this, we will access a data set that has been made available online. The Open-Source Psychometrics Project has made many data sets about personality available, along with references to the publications that used them. This particular data set comes from a survey called the Nature Relatedness Scale (or NR-6), which attempts to measure how connected people are to nature. The researchers (Nisbet &amp; Zelenski, 2013) were trying to shorten an earlier survey of 21 questions down to 6 in order to make it easier to administer (people find long surveys annoying). The data set consists of those six key questions about attitudes towards nature, along with more than 30 other variables. The data set is available online as a .zip file here. (You might need a utility like 7-zip to open .zip files if you are not using a Mac or Linux). However, Texas A&amp;M students in PSYC 301 can find the data set (see below), unzipped, on eCampus under Lab: Lab manual data sets36 &gt; DescStats_NR6.csv. Since it ends with .csv, it should37 be in comma-delimited format. see Sections 2.3 and 2.8.3) to refresh your memory on how to do this. CAUTION: Do not import the data yet. Make sure you read the next two paragraphs below before importing the data. Something that is very, very, very useful when you are importing a data set is a codebook, if any. A codebook is either a separate file or an additional sheet in a spreadsheet program. The codebook tells you what the (often-opaque) variable names really stand for (e.g., Q3A is My connection to nature and the environment is a part of my spirituality), and what the numbers under the nominal variables represent. On this note, recall from Sections 2.1 and 2.8.1 that responses like male vs female (as levels for Sex) are often initially recorded as numbers. It is then your job to translate them back into categories. This data set comes with a codebook called codebook.txt in the original zip folder, but at Texas A&amp;M, it resides in the same eCampus folder as the data set. It is called DescStats_NR6_codebook.txt. One thing that is really important that we can infer38 from this particular codebook is that the number 0 is used to indicate missing values on nominal variables.39 Sometimes when we collect data, especially on surveys, people (either deliberately or by mistake) don’t respond to certain questions.40 The code for such a non-response in this data set (data-final.csv; or DescStats_NR6.csv on eCampus at Texas A&amp;M) is 0. So before you import this data set in jamovi, you should change the Default missings option to 0. To do this, go to \\((\\vdots)\\) &gt; (Import) Default missings and replace NA with 0. See the very bottom of Figure 3.8 below. Figure 3.8: Setting the (Import) Default missings from ‘NA’ to ‘0’ under Preferences. So now you are ready to import the data as you did in Sections 2.3 and 2.8.3). But we have done one of descriptive analyses for you, just for good measure. In Figure 3.9 we changed Q1A to the name in the codebook: My ideal vacation spot would be a remote, wilderness area. We also changed the variable type to Continuous. Figure 3.9: Changing Q1A to name in codebook, along with changing variable type to Continuous. We also chose a variable that is bound to have no real effect: Handedness (for writing). We changed the imported variable name (hand) to the one in the codebook: What hand do you use to write with?, where 0=NA,41 1=Right, 2=Left, and 3=Both. You already know how to associate levels with labels because you did this in Sections 2.1 and 2.8.1. See Figure 3.10 below for a screenshot of this change in jamovi. CAUTION: Make sure the Data type is text. Otherwise, jamovi will give you meaningless statistics about means, medians, etc. Figure 3.10: Changing hand to name in codebook, along with indicating what each level means. Now all we need to do is generate some descriptive statistics and write about them. See the two tables below, which are identical to the output that jamovi would render. ## ## DESCRIPTIVES ## ## Descriptives ## ─────────────────────────────────────────────────────────────────────── ## Ideal.Vacation.Is.Wildnerness Writing.Hand ## ─────────────────────────────────────────────────────────────────────── ## N 1522 1506 ## Missing 0 16 ## Mean 3.910644 ## Standard deviation 1.268608 ## Range 4 ## Minimum 1 ## Maximum 5 ## ─────────────────────────────────────────────────────────────────────── ## ## DESCRIPTIVES ## ## Descriptives ## ─────────────────────────────────────────────────────────────────────── ## Writing.Hand Ideal.Vacation.Is.Wildnerness ## ─────────────────────────────────────────────────────────────────────── ## N Right 1302 ## Left 155 ## Both 49 ## Mean Right 3.900154 ## Left 3.941935 ## Both 4.061224 ## Standard deviation Right 1.264127 ## Left 1.310491 ## Both 1.297564 ## ─────────────────────────────────────────────────────────────────────── Here is an example of how one might summarize this data: Methods …There were a total of 1,522 participants who responded to this study… The range of responses (4) covered the entire spectrum of possible answers, from 1 (“disagree”) to 5 (“agree”). Results Overall, participants responded positively to the notion their ideal vacation would be in the remote wilderness (M=3.91) with a bit of variability (SD=1.27). With respect to their dominant writing hand, there were slightly fewer responses (1,506), as 16 participants failed to indicate their handedness for whatever reason. Among those who did respond however, 1,302 reported being right-handed, 155 left-handed, and 49 ambidextrous. Respectively, each group responded roughly in the same way to the question (see above) about their ideal vacation (right-handers: M=3.90, SD=1.26; left-handers: M=3.94, SD=1.31; and ambidextrous: M=4.06, SD=1.30). Now it is your job to do the same for another pair of variables. Choose one of the variables between Q2A and Q5A (i.e., excluding Q1A), as well as one of the following nominal variables: education, urban, gender, engnat, orientation, voted, and married. Then do carry out exactly the same statistics in jamovi as are reported above, and provide a short, written summary of those descriptives. 3.5 Glossary of descriptives 3.5.1 Central tendency There are three basic measures of central tendency: the mean, the median, and the mode (sections 4.1.1, 4.1.3, and 4.1.6, respectively, in Navarro &amp; Foxcroft (2019)). jamovi includes another, Sum, but this is rarely reported, though it can be useful in certain circumstances (as you will see in Chapter 5 (section 5.1.2). 3.5.1.1 The mean To review, the mean is the balanced “fulcrum” of the data (with data points further away from the center being given progressively more weight in the calculation). The formula is simply \\(\\frac{\\sum X_{i}}{N}\\). 3.5.1.2 The median The median is the value located where “half”42 the ordered values are to the left and “half” to the right (without any regard to the magnitude of the values). 3.5.1.3 The mode Finally, the mode is any subgroup of values in a data set that are both highly similar to each other in value, and relatively high in number (a peak in a histogram) compared to other clustered values.43 3.5.2 Dispersion There are a seven measures of dispersion available in jamovi, though one of them is reported under an alternative heading. These are as follows: Quartiles (under the heading Percentile values), Std. deviation (standard deviation), Variance, Range, Minimum, Maximum, and S. E. Mean (standard error of the mean). 3.5.2.1 The range, minimum, and maximum The range is the simply the value you get when you subtract the maximum data value collected from the minimum data value collected (Importantly, these maximum and minimum values may (and probably will) be different from the lowest possible score and the highest possible score on whatever variable you are collecting data on). These values are usually reported, but they are not ultimately that useful. 3.5.2.2 The standard deviation The standard deviation is the typical distance from the mean. One way to imagine this is to imagine doing the following: Draw a histogram of the data you have up on a whiteboard about the size of, say, someone’s head; Carefully mark where the mean is and draw a long, vertical line at that location; Plot each individual data point on the histogram; Get a bag of dried spaghetti; For any one data point, measure the horizontal distance from that data point to the mean; Break a “stick” (or whatever it’s called) of spaghetti to the exact length between that data point and the mean (discard the unused portion of that spaghetti stick); Do this for all the data points; Gather up all the spaghetti sticks and measure each one of them; Calculate the average length of the spaghetti sticks; That’s the standard deviation! Obviously, the formula for the standard deviation (below) is more complicated than this. So the thought experiment above isn’t quite right. But it is very close. \\[SD=\\sqrt{\\frac{1}{N-1}{\\sum_{i=1}^{n} (x_{i}-\\bar{x})^2}}\\] 3.5.2.3 Mean absolute deviation Now, if you wanted a statistic that directly reflected this thought experiment, then you’d probably use the mean absolute deviation, which is rarely used. Navarro &amp; Foxcroft (2019) covers this topic in section 4.2.3 of Chapter 4. \\[MAD=\\frac{1}{N}{\\sum_{i=1}^{n} |x_{i}-\\bar{x}|}\\] The standard deviation and the mean absolute deviation are close in value, but statisticians prefer standard deviation for reasons beyond the scope of this manual 3.5.2.4 The variance The variance, as you probably know from Navarro &amp; Foxcroft (2019), is simply the standard deviation squared. So you just remove the radical symbol, \\(\\sqrt{}\\), from the formula for the standard deviation. Thus: \\[Variance=\\frac{1}{N-1}{\\sum_{i=1}^{n} (x_{i}-\\bar{x})^2}\\] The variance is extremely important in inferential statistics, but it is rarely reported as a summary statistic since it’s not on a scale that makes sense to anyone. It becomes especially important in the Analysis of Variance (ANOVA), which comes up later. 3.5.2.5 Quartiles, quantiles, and percentiles When you ask for Quartiles, you’re asking jamovi to find data values that cut the range of the data into fourths, where each fourth contains 25% of the data. The second quartile is always the median. First quartile is the data value indicating the point in the data where half of the values below the median are below it, and half are above it. The same is true for the third quartile, but above the mean instead of below it. The span between the first and third quartiles contain 50% of the data, and has a special name: the Interquartile range or IQR. Although quartiles are arguably the most common way to bin data, you can set other cut points for quintiles (5 equal bins), sextiles (6 bins), and so on If you ask for 100 of them, you will get percentiles. If you recall from your ACT/SAT scores, if you scored in, say, the 86th percentile, that means that 86% of the other people your age who took that test scored below you. This is exactly the same concept. 3.5.2.6 Standard error of the mean All of these are covered in your main textbook (Navarro &amp; Foxcroft, 2019), in Chapter 4. But one deserves special mention here: “S. E. Mean” (among jamovi’s Descriptives options), also known as the standard error of the mean, or more generally, the standard error. This isn’t mentioned until Chapter 8 of your main textbook (Navarro &amp; Foxcroft, 2019) (specifically, Chapter 8, section 8.3.3). This is essentially another version of standard deviation, but it is the standard deviation, not of the raw data, but rather of something known as the sampling distribution. To understand this concept, you need to use your imagination a little. Imagine that instead of collecting the data from the 18 patients on mood.gain only one time, you collected the data 100 separate times, using 100 independent samples from the same population. And instead of inputting the raw data (which would amount to 18 x 100, or 1,800 data points), you instead just calculated the mean for each of the 100 samples, and inputted those into your data frame instead. This would give you 100 means (and 100 rows of data in your data frame). Notice now that it would be trivial calculate the mean and standard deviation of these 100 means. If you did, the mean of those means would be the same as the the mean of the 1,800 observations you could have (but didn’t) analyze. However, the standard deviation of these 100 means (the standard error) would be much smaller. This is because, critically, those 100 means would be much closer, on average, to the overall mean than the 1,800 raw data values. Strangely, for such an ostensibly unusual concept, the formula is quite simple: \\[SE_{mean}=\\frac{SD_{mean}} {\\sqrt{N}}\\] The standard error is a very important concept in statistics, but we’ll have to leave how it’s used for later, or Chapter 8 in your main textbook. What’s also important from this little discussion is that we just led you through a mental exercise in what it feels like to be a frequentist. All the statistics we do in this introductory course fall under the umbrella of frequentism (the main other school of thought is Bayesian, which we don’t do). Our thought experiment is an example of how frequentists conceptualize inferential statistics. That is, each time they run a statistic, they are (at least supposed to be) thinking to themselves, “Now, what if I ran this study 100 or 1,000 times? What would the distribution of results look like?” Believe it or not, these two questions form the heart and soul of the frequentist statistics you are learning in this class. This will become clearer in Section IV (Statistical theory) of your main textbook by Navarro &amp; Foxcroft (2019). 3.5.3 Diagnostics There are lots of diagnostic statistics out there that help us determine whether we are using the right statistics or not, given the nature of our data. And almost all of them related to the dispersion of the data (not the central tendency). However, only a few, key ones are offered among jamovi’s Descriptives options: namely, Skewness, Kurtosis, and the Shapiro-Wilk test of normality. But we won’t cover these options here. The reason is that very few practitioners use actual numbers to evaluate these particular diagnostics. It turns out that it is instead generally more useful simply to visualize the data in order to identify such issues with data. The term data summaries is intentional. In the traditional approach that we use in this class, descriptive statistics are merely data summaries, which are a completely different class of “statistics” than inferential statistics. Some students get confused by this, but descriptive statistics are not anything like the inferential statistics we cover in Chapters 5 to 9. Specifically, descriptive data summaries tell you only about your particular sample of observations. In contrast, inferential statistics use special signal-to-noise ratios (generated from specific formulas unique to each data type) that let you make educated guesses about the larger population (hence, inferring population parameters from sample statistics). Nonetheless, the authors of jamovi decided to use the term descriptive statistics, so we will honor that in this manual.↩︎ Open Data is the official term for data that is freely available to the public↩︎ If you look closely, it’s in tab-delimited format because (as noted before) there are some commas in the responses (see the very last variable). But nicely, jamovi doesn’t care, and finds the right delimiter for you automatically.↩︎ The researchers did not directly write this in the codebook, but they did indicate the possible responses, which never included 0. Thus, we can infer that the 0 represents a non-response, a missing value.↩︎ Blanks are used under free responses, like the last variable in the data set: major, but “messy” free-response data is a topic for another class.↩︎ You can force them to in online surveys like Google Forms, but respondents often find this annoyingly coercive, especially if you do this for lots of questions.↩︎ NA (no quotes) is the native missing-value used in jamovi and R. When we imported ‘0’ as missing, jamovi turned all the 0s into NAs under the hood.↩︎ or the average of the middle two values if there are an even number of observations↩︎ This is a somewhat alternative definition from most that you will encounter (e.g., “the most common value in a data set”), but one that accounts for the fact that a data set can have more than one mode.↩︎ "],["DataVisualization.html", "Chapter 4 Data visualizations 4.1 Histograms 4.2 Boxplots 4.3 Violin plots 4.4 Dot plots 4.5 Bar plots 4.6 Q-Q plots 4.7 Outside help on visualization", " Chapter 4 Data visualizations There are a few visualizations possible from among the jamovi Descriptives options. Again, your main textbook by Navarro &amp; Foxcroft (2019) dedicates a chapter to data visualization in jamovi. This is Chapter 5 (Drawing graphs). And also again, the best way to get through this chapter is to open up jamovi and follow along at each step. There are no practice exercises for this chapter. NOTE: There is one important thing you should understand, however, about the visualizations in this chapter (e.g., histograms, boxplots, Q-Q plots). Namely, these visualizations of descriptive statistics are used mostly by researchers to help them make decisions about subsequent analyses. These are NOT typically visualizations that are reported in the Results sections of journal articles. In jamovi, the plots that are used in Results sections are included within the menu for the statistical analysis itself. For example, there is a box called Descriptives plots that you can check under the T-Tests tab. This will give you a dot plot with confidence intervals (to be explained later in Navarro &amp; Foxcroft, 2019). The exception below is the bar plot for frequency data (section 4.5). Such plots are, in fact, often used in Results sections of papers. To illustrate these various visualizations, we will use the Chico2.omv sample data set from Navarro &amp; Foxcroft (2019). To get to this, just go to \\((\\equiv)\\) &gt; Open &gt; Data Library &gt; Chico 2. The data consist of grades on some test (grade_test), which is a continuous variable, along with a time-of-test variable, time, which is a 2-level factor, where “1” means at time 1 and “2” means at time 2. NOTE: It is important to note at this point that none of the Plots from the the Descriptives menu will work if the variable you are trying to visualize is not Continuous. Go back to the Data tab to check this if you are having trouble getting jamovi to depict your data visually. 4.1 Histograms The first visualization is the well-known histogram, where the frequency of particular responses are either “binned” if they are non-discrete (e.g., values with decimals), or possibly “stacked” if they are discrete integers (though such a variable may be binned as well). What does this mean? For certain variables, like reaction times (RTs), values almost never repeat. It would be very strange for any one or two people to get the same exact reaction time of 632.44 milliseconds (ms). Therefore, you can’t really “stack” these observations in a histogram. What you can do, however, is bin them. You can create various bins of, say, 50 milliseconds, and place relevant values in there. For instance, there could be a bin for RTs between 350-400 ms, another one adjacent, to the right of that one for RTs between 400-450 ms, then for 450-500 ms, and so on. You can change the width of the bin. For other variables, it’s entirely possible to stack them, sometimes. For instance, if age (in years) is recorded as a simple integer, it would be very easy to stack the 18s, the 19s, the 20s, the 21s, and so on, as long as your age range is relatively limited (e.g., typical college students). Note that if you had a really wide range of ages, you might bin them (1-5, 6-10, 11-15, 16-20,… 61-65… etc.). Often paired with the histogram is the density plot, which uses math to estimate what a population distribution with many, many observations would look like based on the sample data you have. Ideally, we would like to see a perfect bell curve as the density plot, but that is rare. Go get these for the Chico2.omv data, we moved grade_test over to the Variables box, and time over to the Split by box. Then we checked Histogram and Density under Histograms. You can see the parameters that need to be set in Figure 4.1 below. Figure 4.1: Parameters for histograms and density plots for test grade split by time of test (with the data set ‘Chico 2’ from Navarro &amp; Foxcroft, 2019). The results of this are below. ## ## DESCRIPTIVES ## ## Descriptives ## ──────────────────────────────────────────── ## time grade_test ## ──────────────────────────────────────────── ## N 1 20 ## 2 20 ## Mean 1 56.98000 ## 2 58.38500 ## Standard deviation 1 6.616137 ## 2 6.405612 ## ──────────────────────────────────────────── 4.2 Boxplots One of the drawbacks of histograms and density plots is that, although they are handy with respect to depicting the informal shape of a distribution, they are not so good at showing particular statistics about any given variable. This is where the boxplot comes in. Together with violin plots and dotplots (which can all be superimposed), you can offer your reader/listener a great deal of information about your variables. We will use the Harpo data set provided by Navarro &amp; Foxcroft (2019) (we see this data again in Chapter 11 of Navarro &amp; Foxcroft (2019)). This data has to do with the grades of 33 fictional students in “Dr. Harpo’s” statistics class. Dr. Harpo also has two tutors: Anastasia and Bernadette. Go to \\((\\equiv)\\) &gt; Open &gt; Data Library &gt; Harpo. NOTE: When you open the Harpo data set, you will need to change the grade variable to continuous, which you learned how to do in Section 2.1. You can leave the tutor variable alone. TIP: Save this file as a .omv file. Not only are there a lot of analyses here that you will be carrying out, but we also re-visit this data in Chapter 6, when we work on t-tests. Once you have addressed the note above, go to Exploration &gt; Descriptives and simply slide the grade variable into the Variables box, and then (since you already know how to do this) slide the tutor variable into the Split by box. Finally, click the Box plot box under the Box Plots heading. Also check the following boxes, which will help you understand the boxplot: Quartiles, Median, Minimum, and Maximum. You can see these settings in Figure 4.2 below. Figure 4.2: Parameter settings for boxplots along with related descriptive statistics. NOTE: If we had chosen two separate variables to slide into the Variables box, and then clicked Box plot, you would have seen two separate plots, each with one boxplot. That is, to get the side-by-side plots, you need to have a Split by variable, which in turn must be nominal (e.g., time, Sex, tutor), not continuous. Along with the statistics, Two side-by-side plots should appear in the output window. You can see all this below. But because of the length of the explanation (spanning several paragraphs), we are going to start explaining the boxplot above the descriptives and figure below, so that it is easier for you to scroll back and forth. Looking at the boxplots below, you are probably asking yourself: “What do the different elements of the box mean?” You have probably also noticed that there isn’t just a box in the plot. Rather, the box is divided into two by a horizontal line, and the box has vertical lines extending from the top and the bottom. Finally, there’s a strange dot at the bottom of the boxplot for the tutor named “Bernadette.” First, the box itself. The box simply encloses the data from the 1st to the 3rd quartiles. As explained in glossary below (Section 3.5.2.5), the first quartile is the same as the 25th percentile, or the grade that lies just above the bottom 25% of the grade scores (ordered from lowest to highest). It’s the bottom of the box. The 3rd quartile is identical to the 75th percentile, or the grade that lies just above the bottom 75% of grades. This is the top of the box. So if your grade was at the third quartile, then 75% of the class would have scores below yours. To verify this, you can compare the boxplot to the descriptives above them. The 25th percentile for Anastasia is 69. This corresponds to the bottom of “her” box. Likewise, the 75th percentile for her data is 79. This is the top of her box. You can double check this for Bernadette’s box. All this also suggests that half of all the scores lie between the 1st and 3rd quartile. So, in effect, the box shows you where the middle half of your data is (which also entails that half of the data is outside the confines of the box). This box is also known as the Interquartile Range or IQR (see Navarro &amp; Foxcroft (2019), Section 4.2.2, for more information). So now, what does the horizontal line in the middle of the boxes mean? That line represents the median (also explained below in the glossary below, Section 3.5.1.2)). Working from the discussion directly above, the median is simply the 2nd quartile, or the 50th percentile. This can be verified by looking at the values for the median and the 50th percentile; they are identical values (for Anastasia and Bernadette, respectively). ## ## DESCRIPTIVES ## ## Descriptives ## ───────────────────────────────────────────── ## tutor grade ## ───────────────────────────────────────────── ## Median Anastasia 76 ## Bernadette 69.00000 ## Minimum Anastasia 55 ## Bernadette 56 ## Maximum Anastasia 90 ## Bernadette 79 ## 25th percentile Anastasia 69.00000 ## Bernadette 66.25000 ## 50th percentile Anastasia 76.00000 ## Bernadette 69.00000 ## 75th percentile Anastasia 79.00000 ## Bernadette 73.00000 ## ───────────────────────────────────────────── In other words, the median is the middle value of your data, if you ordered your data from smallest to largest. That is, irrespective of their own values, half of the data points should lie below the median, and half above (and if there is an even number of data points, the median is the average of the middle two values). The next issue is the vertical lines extending from the top and bottom of the box, respectively. These are often called whiskers.44 Traditionally, these lines extend out to 1.5 times the interquartile range (the value at the 3rd quartile minus the value at the 1st quartile), a calculation we can attribute to Tukey (1977). The idea here is to capture all the data that is not considered potentially extreme within the ends of the whiskers. Anything outside the whiskers, then, is considered possibly extreme, and could have an unexpectedly high influence on the calculation of the mean. This brings us to the final element of the boxplot: the dots (sometimes small circles). These appear beyond the whiskers and represent individual observations whose values are probably a bit extreme, given what we’d expect from a normal distribution. Another name for such observations is outliers, or potential outliers, to be precise. Determining what is vs. isn’t an outlier is a little bit involved and goes well beyond boxplots. Your main textbook (Navarro &amp; Foxcroft, 2019) covers this issue with respect to boxplots in Section 5.2.3., though the general concept is introduced in Section 4.2.1. Note that in the boxplot figure there is one dot on the plot, which is located at the bottom of Bernadette’s group of students. This is a potential outlier. If you see any dots at all, then the topmost and/or bottomost of these will be equivalent to the Maximum and/or Minimum values in your data. For the person with the low grade who has Bernadette as a tutor, you can see that the minimum in her group is 56, which is exactly where the dot is at the bottom of Bernadette’s box. Boxplots provide no representations of data beyond these dots. Overall, the boxplot is a quick way of visually determining critical characteristics of your data. Here are some guidelines: The bigger the box and the longer the whiskers, the more spread out your data is (and vice-versa) The closer the median is to the center of the box, the more symmetrical (non-skewed) your data is (and vice-versa) A higher number of dots in your boxplot indicate a higher number of extreme values in the tails of your distribution. However: This matters much more if there are considerably more dots above the top whisker than below the bottom whisker (or vice-versa) The larger the data set, the more dots in the extremes you can expect (what matters is their relative distribution above or below the whiskers), so dots appearing in the boxplot is much more of a concern for smaller data sets 4.3 Violin plots Violin plots are really an extension of the boxplot. You can think of it this way: A violin plot is to a boxplot what a density plot is to a histogram (see Section 4.1). In fact, you could just call the violin plot a sideways, self-reflected density plot (with the ends cut off), because that’s all it is, really. There are really only two main differences between a density plot and a violin plot: A violin plot is vertical, whereas a density plot is horizontal A violin plot is usually reflected against itself,45 whereas a density plot is not.46 To get a violin plot, simply click of the Violin check box under Box plots. Also click Box plot, as that will give you an idea of how the violin plot circumscribes the boxplot, which you already understand. Finally, also check Density under Histograms. We have seen these before, but it is a good idea to do so now to see how the density plots and violin plots relate. ## ## DESCRIPTIVES Nicely, the boxplots get embedded within the violin plots when you click both in jamovi. This is another reason why these kinds of plots can stuff a lot of information into a small space. Anyway, the first thing to notice (if you hadn’t already with the boxplots before), the axes are switched between histograms/density plots, on the one hand, and boxplots/violin plots, on the other. In fact, it’s the histograms and density plots that are unusual in the world of statistical visualization. In almost all other cases, the outcome variable is plotted on the y-axis (as you see with the boxplots and violin plots), not the x-axis (as you see with the histograms and density plots). We encourage you to always think of your outcome variable as being plotted on the y-axis; this “mind trick” actually helps you understand other statistics better, especially linear regression (coming up in Chapter 7). Note the “violin-like” quality of the plots.47 This comes from turning a density plot on its side and reflecting it against itself. Obviously, the density plot by itself is more detailed, whereas the violin plot is a more restricted visualization. 4.4 Dot plots One final way of illustrating the nature of your data is to use a dot plot. This simply plots out each observation along the y-axis of the graph. There are two options here: You can stack the dots, or jitter them. Stacking works better for small data sets (like the Harpo data set), whereas jittering works better for large data sets where you are likely to have many dots with close or identical values. Generating this graph is just as easy as the others. Just check the Data box (under Box plots), and choose Stacked or Jittered from the drop-down menu below it. An example of this is shown in Figure 4.3 below. Figure 4.3: Parameters for generating a stacked dot plot. The results of the stacked dot plot can be seen below. We also checked Violin in order to illustrate the relationship between the two. Side-by-side dots represent identical values (i.e., students who got the same grade). ## ## DESCRIPTIVES Note that these all fall under Box plots for a reason. Namely, you can combine these plots in any way you see fit. We have combined all three into one plot below. Note that the dots are jittered this time, strictly for illustration purposes (i.e., it was not helpful in any way for visualization since the number of observations is low). ## ## DESCRIPTIVES 4.5 Bar plots The next visualization covered here is a bar plot. NOTE: This is not the clumsy bar graph that has been used so often throughout history. A bar graph (for lack of a better term that distinguishes it from bar plot) represents continuous data (e.g., reaction times) with a bar (or bars). This type of chart is often discouraged by many in favor of line charts and dot charts with error bars. One reason is that bar graphs are often wasteful in terms of space. Another reason is that if we restrict bar plots to counts, then we “unite” bar plots with histograms, since histograms represent counts as well. So universally, bars = counts. You cannot rely on that last sentence yet, as it’s only an ideal. But regardless, bars are used in jamovi only to count up frequencies. jamovi replaces bar graphs representing continuous data with dot or line plots These are available under the appropriate inferential tests. The bar plot is a good way of visualizing frequency data.48 This type of visualization is not usually necessary for any kind of write-up summarizing your data set. However, it can be very useful, ultimately, for chi-square analyses and the display of individual Likert items. A discussion of chi-square analyses comes up later in this Guide (Chapter 5), and in your main textbook [Navarro &amp; Foxcroft (2019); Chapter 10]. The display of Likert items does not really come up in this class. Despite the fact that you will probably not need this kind of plot for this class, you can still practice. We will use the Harpo data again. We will compare the number of students in Anastasia’s vs. Bernadette’s tutoring groups. First, you need to drag the tutor variable over to the Variables box. Then check the Frequency tables check box located under the left-most (but label-less) white box. Finally, check the Bar plot box under Bar plots to get the frequency plot. You can see the parameter settings for this in Figure 4.4 Figure 4.4: Parameters for generating a bar plot. The result is a simple bar plot of the number of students in each tutoring group, along with relevant descriptive statistics above the plot. See below. ## ## DESCRIPTIVES ## ## FREQUENCIES ## ## Frequencies of tutor ## ────────────────────────────────────────────────────── ## Levels Counts % of Total Cumulative % ## ────────────────────────────────────────────────────── ## Anastasia 15 45.45455 45.45455 ## Bernadette 18 54.54545 100.00000 ## ────────────────────────────────────────────────────── The meaning of the bar plot is almost self-explanatory in this case. 4.6 Q-Q plots Finally, we get to the last type of visualization available under Descriptives: the Q-Q plot (which is short for quantile-quantile plot). This plot is strictly diagnostic. It’s a quick way to detect whether your data is normally distributed or not. In general, it plots the quantiles of two distributions against each other. To assess normality (the default option in jamovi), it plots the quantiles of a theoretically normal distribution on the x-axis against the data that you have on the y-axis in the form of standardized residuals (another term for individual deviations from central tendency; z-scores, essentially). That may have made little sense to you at this point, and that’s okay. What’s important about this plot is that the dots should line up near the line without any systematic or extreme deviations.49 We’ve provided the Q-Q plot for the Harpo data, with grade divided into the two tutor groups: Anastasia and Bernadette. The parameters you need to set to generate this is in Figure 4.5 below. Figure 4.5: Parameters for generating Q-Q plots. ## ## DESCRIPTIVES As you can see, the dots line up quite closely to the line, suggesting that the data is normally distributed. To show what a positively skewed data set looks like in a Q-Q plot, we have opened the AFL Margins data set from Navarro &amp; Foxcroft (2019), a data set that has something to do with Australian Rules Football. You can see the results of this below. ## ## DESCRIPTIVES This time we included the histogram to show you the positive skew. In the Q-Q plot, look at the y-axis, and how the scale only goes down to about -1.5 standardized residuals (z-scores), whereas the x-axis goes down to -3. The dots splay out to the left away from the line, suggesting that they’re clumping up at the bottom end and not producing a nice, leftward, asymptotic (small) tail. At the upper end, you can see that several dots have higher values than the line would “suggest.” This means that the z-scores are too high for the distribution for it to be considered a normal. In other words, there is positive skew. If you didn’t understand very much of that, that’s okay. The beauty of the Q-Q plot is that dots-on-or-near-the-line means that your data fit that particular theoretical distribution well (in this case, the default: a normally distribution). Dots-not-on-or-near-the-line means that you might need to do something to your data (a transformation) in order to do statistics, or choose a different kind of analysis altogether. 4.7 Outside help on visualization 4.7.1 Histograms datalab.cc also has a tutorial on histograms and density plots. They are videos #19 and #20, respectively (these are also available for Texas A&amp;M as tutorials in the set of LinkedIn Learning tutorials at Texas A&amp;M). Your textbook (Navarro &amp; Foxcroft, 2019) also covers histograms in the first section (5.1) of Chapter 5. 4.7.2 Boxplots For some outside help on boxplots, you can go to video #21, datalab.cc’s tutorial on boxplots. 4.7.3 Violin plots For more on violin plots, you can go datalab.cc’s tutorial on them, which is video #22. Your main textbook also covers violin plots in Section 5.2.1. 4.7.4 Dot plots For outside help on dot plots, you can go to datalab.cc’s tutorial on dot plots, which is video #23. 4.7.5 Bar plots (#OutsideHelpOnBarPlots) For outside help on bar plots in jamovi, you can go to datalab.cc’s tutorial on bar charts, which is video #24. Moreover, a more descriptive name for the boxplot is the box-and-whiskers plot. Boxplot is really just a shortened version of that.↩︎ Sometimes, the reflecting is dispensed with since it is, actually, redundant.↩︎ The reflecting is what ultimately gives the violin plot its shape, and hence, its name.↩︎ This is sort of a lie. Honestly, normally distributed data look more like deep-sea skates or rays, but the labels stingray plot or skate plot are going to take off any time soon; the misnomer has stuck. Nonetheless, when the data is really bimodal, you do get a violin-like quality to the plots.↩︎ Traditionally, pie charts have been used to do this, but pie charts can be problematic for several reasons. Here is a link that explains pretty clearly why.↩︎ A really nice feature of the QQ plot is that it isn’t restricted to comparing your data to normal distributions (though that’s all we do in this class). Rather, you can use it to determine whether your data conform to some other well-known distribution, which might lead to another family of statistics (this is way beyond the scope of this class, but you should know about it).↩︎ "],["CategoricalDataAnalysis.html", "Chapter 5 Categorical data analysis 5.1 Chi-square goodness-of-fit test 5.2 Chi-square test of independence 5.3 Outside help on chi-square", " Chapter 5 Categorical data analysis Up to this point, we have been following, roughly, the order of information set out in your main textbook by Navarro &amp; Foxcroft (2019). We depart a little at this point as we are not going to cover the material in Chapter 6 of that book. We will cover that material in Appendix A. Nor will we cover chapter 7-9 at all. Instead, this lab manual resumes below as parallel to Chapter 10 of Navarro &amp; Foxcroft (2019). From the current chapter (Chapters 5) to Chapter 9, you’ll see that the point of inferential statistics is to calculate specific signal-to-noise ratios in order to see if the signal that emerges from our data is so strong over the noise (that emerges from our data) that we can confidently claim that we indeed have a signal, and not just a bunch of noise. We will start with the first of these tests: the chi-square test. Thus, this chapter shows you how to use jamovi to conduct analyses when your outcome variable consists of counts of nominal (categorical) data (e.g., the number of men vs. women who voted in a particular district, where the proportions are known to be about 50/50). The topic is covered in much more detail in your main textbook Navarro &amp; Foxcroft (2019Chapter 10). There are several versions of the chi-square test, but we will only cover two here: the \\(\\chi^2\\) goodness-of-fit test, and the \\(\\chi^2\\) test of association. The former is used is when you want to compare your sample data to known population proportions (like the voting example above), and the latter is when you want to compare the counts of two groups against one another. For example, perhaps you wanted to know whether the morning-beverage preferences (coffee, tea, or juice) were the same across men and women. A \\(\\chi^2\\) test of association could answer this (assuming you collected the data, of course). The first we will cover here is the goodness-of-fit test. 5.1 Chi-square goodness-of-fit test In each chapter in this manual, there will be one basic analysis, and often, one, more advanced analysis. The basic analysis will always be a quick way to do the analysis presented in the relevant chapter in Navarro &amp; Foxcroft (2019). The more advanced analysis will be the UC Berkeley graduate-admissions data from 1973 (a famous data-analysis case). 5.1.1 Basic: The Cards data This is from Section 10.1.1 of Navarro &amp; Foxcroft (2019), and the data set is labeled Randomness.csv. Thus: \\((\\equiv)\\) &gt; Open &gt; Data Library &gt; learning statistics with jamovi &gt; randomness.csv. In this imaginary experiment (with imaginary data), participants were asked to mentally choose a single card from an imagined deck of cards, and then a second card. The idea was to figure out if people really could do things at random, or if they had preferences for different suits (clubs, diamonds, hearts, or spades). 5.1.1.1 Inspecting To start, you just want to get a bird’s-eye view of the relative proportions across the two choices. So you to go Exploration &gt; Descriptives, drag the first choice over to the Variables box and check Frequency tables. We also unchecked all the other options for descriptive statistics. You should see what is below in Figure (5.1). Figure 5.1: Setting the parameters in jamovi for getting the counts on the first choice of the cards data from Navarro &amp; Foxcroft (2019). Your output should look like the following: ## ## DESCRIPTIVES ## ## FREQUENCIES ## ## Frequencies of choice_1 ## ──────────────────────────────────────────────────── ## Levels Counts % of Total Cumulative % ## ──────────────────────────────────────────────────── ## clubs 35 17.50000 17.50000 ## diamonds 51 25.50000 43.00000 ## hearts 64 32.00000 75.00000 ## spades 50 25.00000 100.00000 ## ──────────────────────────────────────────────────── It looks like people have a preference for hearts over clubs, but this could be due to chance. We need to test this. 5.1.1.2 Hypothesizing So referring to Chapter 9 in Navarro &amp; Foxcroft (2019), first we need to create null and research hypotheses. Stated in English, it would be as follows: \\(H_0:\\) There is no suit preference among participants Mathematically, it would come out as below, where we state each outcome as a probability, P: \\(H_0:P_{clubs}=P_{diamonds}=P_{hearts}=P_{spades}\\) Or even better for this particular inferential test: \\(H_0:P=(.25, .25, .25, .25)\\) This last one means that you are predicting that the probability for each suit is equal. IMPORTANT: In this chi-square goodness-of-fit test, all the probabilities were presumably equal at 25% each. But this isn’t always the case. For instance, if we had asked participants to imagine any card ignoring the suit (clubs, diamonds, hearts, and spades) but including jokers, then the probabilities under the null hypothesis would be 4/54 (or 7.41%) for each of the values 2-10, Jack, Queen, King, and Ace, but only 2/54 (or 3.7%) for a Joker. The chi-square goodness-of-fit test can account for these differences. You will see this in the advanced analysis in section 5.1.2, if you choose to do so. This is also covered in Navarro &amp; Foxcroft (2019) in section 10.1.8. Conversely, the research hypothesis would be as follows: \\(H_1\\): Participants have a preference for one or more of the suits over one or more of the others \\(H_1:P \\neq (.25, .25, .25, .25)\\) IMPORTANT: The research hypothesis above is not very specific. It can also be stated in English as any outcome that is not the null-hypothesis outcome. This formulation is appropriate when there more than two things to compare. This is because it could be true that \\(P_{clubs} \\neq P_{spades}\\), or \\(P_{clubs} \\neq (P_{diamonds}=P_{hearts}=P_{spades})\\), or even \\((P_{clubs} =P_{diamonds}) \\neq (P_{hearts}=P_{spades})\\). Clearly, there are several possible outcomes for the research hypothesis. You simply need to infer these underlying possibilities when reading the research hypothesis since it would be onerous to write all of them out. That said, there are cases where you do write them out (in which case they then become specific predictions). This issue will come up again and again, especially when we get to ANOVAs. 5.1.1.3 Testing Carrying out the chi-square goodness-of-fit test is ultimately quite easy in jamovi. But we will spend a little extra time explaining the results since this is the first inferential test in the lab manual. To carry out the test, Go to the Analyses tab across the top, click Frequencies, and then choose N Outcomes \\(\\chi^2\\) Goodness of fit. This is shown in the screenshot below, Figure 5.2. Figure 5.2: Choosing the chi-square goodness-of-fit test from the Frequencies menu in jamovi. Then slide the variable choice_1 over to the Variable box. Also click the Expected counts box. The parameters should be as you see below in Figure 5.3. Figure 5.3: Setting the parameters in jamovi for doing the chi-square goodness-of-fit test on first choice of the cards data from Navarro &amp; Foxcroft (2019). The output should look like the following: ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - choice_1 ## ────────────────────────────────────────────────── ## Level Count Proportion ## ────────────────────────────────────────────────── ## clubs Observed 35 0.1750000 ## Expected 50.00000 0.2500000 ## ## diamonds Observed 51 0.2550000 ## Expected 50.00000 0.2500000 ## ## hearts Observed 64 0.3200000 ## Expected 50.00000 0.2500000 ## ## spades Observed 50 0.2500000 ## Expected 50.00000 0.2500000 ## ────────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ─────────────────────────────── ## χ² df p ## ─────────────────────────────── ## 8.440000 3 0.0377419 ## ─────────────────────────────── You might also want to plot this in jamovi. The way to do this is by checking the Barplot box in Descriptives. See Figure 5.4 below for the right parameter settings. Figure 5.4: Setting the parameters in jamovi for creating a barplot for the first choice of the cards data from Navarro &amp; Foxcroft (2019). The results will look like the following: ## ## DESCRIPTIVES 5.1.1.4 Interpreting First, note that since we asked for Expected counts, jamovi told use the exact numbers that would have been expected under the null hypothesis. This was 50 for each group since there were a total of 200 participants, and we assumed that there was an equal chance of any card suit being preferred (25% for each suit). The observed values are the counts that we actually got from the participants. This is all easy enough to read from the first table. The second table gives us the results of the inferential test. The degrees of freedom is for the chi-square goodness-of-fit test is the total number of categories minus 1. There were 4 card suits, minus 1, therefore, df = 3. The test statistic gave us an obtained value of 8.44. The test statistic is the following formula: \\[\\chi^2=\\sum_{i=1}^{k}\\frac{(O_{i}-E_{i})^2}{E_i}\\] The obtained value is 8.44, which is what you get after you refer to the table above, and substitute the Os and the Es with the Observed and Expected values (respectively) for each card suit (i) found in the table above, . What is important about the degrees of freedom (3 in this case) is that this figure sets up the particular chi-squared distribution that the test statistics of 8.44 will be evaluated against. This distribution is the solid line in Figure 10.1 in Navarro &amp; Foxcroft (2019, sec. 10.1.5, p. 217). But we reproduce it below. What does this distribution represent? Think of it this way: If you carried out an infinite number of these experiments with the cards and the four suits, this \\(\\chi_2\\) distribution (with degrees of freedom equal to 3) represents the relative probabilities (areas under the curve) of getting different ranges of observed values (mapped across the x-axis) under the assumption that the null hypothesis is underlyingly true. It turns out that for this inferential test (\\(\\chi^2\\) goodness of fit) with three degrees of freedom, obtained values of 7.815 or greater would be less than 5% likely to occur if the null hypothesis were true. The value 7.815 is the critical value, where that 5% area extending out to the right starts. Our obtained value of 8.44 is greater than that (7.815) of course. In fact, if we were to calculate the area under the curve to the right of 8.44, we would get 3.8%, which is our p-value up above (an exact p-value). Our p-value is less than our pre-set \\(\\alpha\\) (alpha) level of .05, so we are given the “green light” to reject the null hypothesis. What does that actually mean? Normally, one doesn’t talk about rejecting the null hypothesis, per se. Rather, the expression was statistically significant suffices because it is shorthand for a more complex process that all researchers are assumed to understand. What is this process. By way of illustration, here is an overwrought, but accurate way of rejecting the null hypothesis: The obtained value for our chi-square goodness-of-fit test was 8.44. If we replicated this exact experiment an infinite number of times with new samples (thereby giving us 3 degrees of freedom each time) and the null hypothesis were underlyingly true, then obtained values of 8.44 or greater would occur only about 3.8% of the time. Since it is the industry standard in Psychology not to accept a risk of being wrong greater than 5%, we are comfortable accepting the 3.8% risk here, thus concluding (at least provisionally) that it’s not our data that are strange, but rather that the null hypothesis itself. That is, it seems to be an inappropriate assumption in this case, so we provisionally reject it. Something other than the null hypothesis is more likely to be true, namely (assuming we have designed things appropriately), the research hypothesis. Since there are more than 2 categories (df&gt;1), the exact form of the research hypothesis still needs to be determined. This may be determined either through post-hoc or planned comparisons. WARNING: We noted above that this passsage is overwrought. Do not ever write such a thing (though we think maybe researchers should practice it to remind themselves what they are doing). It is also pedantic and condescending since any researcher reading such a passage should already understand that this is the actual meaning that undergirds statistical significance. 5.1.1.5 Reporting Actually writing out results takes a far different form. Here, we will briefly discuss how to write out the results of statistics in APA format since this is the first time we cover it in the lab manual. But a more detailed account is provided in section 12.4.1.3.2. Test statistics in APA format always follow the pattern below. a letter (in italics) representing the test statistic used parentheses that enclose the degrees of freedom an equals sign the obtained value of the test statistic a comma the letter p in italics one of three symbols: =, &lt;, or &gt; a p-value Thus, the results of our chi-square analysis would be reported as \\(\\chi^2 (3)=8.44,p=.038\\). But we cannot stop there. A proper report in a results section only includes the test results as parentheticals to more everyday prose. Our results were statistically significant, \\(\\chi^2 (3)=8.44,p=.038\\). It appears that when people imagine a card picked randomly from a deck of cards, they have preferences for certain suits over others. For whatever reason, the most likely candidate disproportionality in our case is that they have a strong inclination to imagine a card from hearts (32%) and a strong disinclination to imagine a card with clubs (17.5%). The other two suits had observed frequencies that matched their expected frequencies. A post-hoc analysis would be required to determine the exact relationships with more certainty. NOTE: This first sentence in the passage above is actually a condensed version of almost the entire “overwrought” passage above that (i.e., the one above the WARNING message). 5.1.2 Advanced: Student admissions at UC Berkeley For the more advanced analysis, we will work with a well-known data set from Bickel, Hammel, &amp; O’Connell (1975) that analyzed the 1973 graduate-school admissions data to the University of California - Berkeley. The data set contains data on how many men vs. women were given or denied admission across the six largest graduate schools at Cal (Cal is the nickname for the university). At first blush, it looks like men are being favored over women in graduate admissions. But we want to find out whether this is really true. 5.1.2.1 Opening data This data set is available as one of the data sets that comes with base-R. Since jamovi is built on top of R, it has no problem opening R data sets. In fact, the module called R data sets includes this data set. Install this module if you haven’t already (see the end of Section 1.2 for how to do this). Once you have done this, all you have to do is click \\((\\equiv)\\) &gt; Open &gt; Data library &gt; R data sets &gt; UCBAdmissions (the file is near the bottom of the list). Your data should look like what is below. Table 5.1: Graduate-school admissions figures by gender and department from the University of California Berkeley in 1973 (available as the ‘UCBAdmissions’ sample data set in base-R). Admit Gender Dept Freq Admitted Male A 512 Rejected Male A 313 Admitted Female A 89 Rejected Female A 19 Admitted Male B 353 Rejected Male B 207 Admitted Female B 17 Rejected Female B 8 Admitted Male C 120 Rejected Male C 205 Admitted Female C 202 Rejected Female C 391 Admitted Male D 138 Rejected Male D 279 Admitted Female D 131 Rejected Female D 244 Admitted Male E 53 Rejected Male E 138 Admitted Female E 94 Rejected Female E 299 Admitted Male F 22 Rejected Male F 351 Admitted Female F 24 Rejected Female F 317 The data give us the numbers of men versus women who were admitted and rejected by academic department (the six largest departments only). We have what we need to start an analysis of whether men are being favored over women in admissions. 5.1.2.2 Preparing There are two things that you are going to need to do in order to prepare this data for a full analysis: Calculate relative proportions of men versus women who applied to graduate school at Cal; and Filter the data in order to count admissions only, as well as filter by department in six of the cases (Departments A-F). 5.1.2.2.1 Calculating relative proportions The default null hypothesis for a chi-square goodness-of-fit test is that all outcomes are equally likely.50 So in the admissions data we have, it would assume that men and women are equally likely to be admitted (50/50); the analysis would expect equal numbers of men and women admitted under the null hypothesis. There is a big problem with the admissions data at Cal, however. Namely, not only did the numbers of men vs. women who applied to graduate school at the university differ overall (2,691 vs. 1,835), but the numbers also differed by department. So if more men apply than women, you’d expect more men to be accepted than women, even if the university (or department) is admitting men and women without bias towards one or the other. In fact, if equal numbers of men and women were accepted, you’d likely have to conclude that the university was favoring women in admissions. As noted in Navarro &amp; Foxcroft (2019, sec. 10.1.8), this test offers a way to correct for this. We will need to do so here. To start, we will need to carry out some Descriptives on the data. First, we go to the Analysis tab, and click Descriptives. We move the variable Freq over to the Variables box, and the variable Gender over to the Split by box. Then we make sure that the only box that is checked is Sum under Central Tendency. This will add up the numbers for Admitted and Rejected applicants across all departments, but divided by Gender. See Figure 5.5 below. Figure 5.5: Setting the jamovi parameters for summing overall numbers of men versus women (admitted plus rejected) across the university for the Cal Berkeley admissions sample data set from R Core Team (2021). You should see the following results when you do this. Note that these are the numbers that we provided above for men and women across the six largest graduate-school programs at the university. ## ## DESCRIPTIVES ## ## Descriptives ## ───────────────────────── ## Gender Freq ## ───────────────────────── ## Sum Female 1835 ## Male 2691 ## ───────────────────────── With these numbers, you will need to write down the relative percentages of men vs. women in the data set. Thus, percentage of men applicants overall (across departments) is as follows: \\[\\%\\;males_{overall}=\\frac{2691}{2691+1835} = \\frac{2691}{4526}=.5945=59.45\\%\\] And therefore, for women, it’s as follows: \\[\\%\\;females_{overall}=1-.5945=.4055=40.55\\%\\] You will need these numbers soon enough. Now we need to get these figures by academic department (A through F). To do this, we just do the same as in Figure 5.5 above, but we also slide Dept into the Split by box. See Figure 5.6 below. In fact, you’ll probably want to get the variables in the Split by box in the order shown in the figure, so you may need to slide Gender out, then slide Dept and then Gender back in a again. Figure 5.6: Setting the jamovi parameters for summing the numbers of men versus women (admitted plus rejected) by academic department at the university for the Cal Berkeley admissions sample data set (UCBAdmissions) from R Core Team (2021). You should see the numbers below. ## ## DESCRIPTIVES ## ## Descriptives ## ───────────────────────────────── ## Dept Gender Freq ## ───────────────────────────────── ## Sum A Female 108 ## Male 825 ## B Female 25 ## Male 560 ## C Female 593 ## Male 325 ## D Female 375 ## Male 417 ## E Female 393 ## Male 191 ## F Female 341 ## Male 373 ## ───────────────────────────────── So now we need to calculate the relative percentages of males versus females applying to each department. Applicants to Department A \\(\\%_{males}=\\frac{825}{825+108}=\\frac{825}{933}=.8842=88.42\\%\\); and therefore, \\(\\%_{females}=1-.8842=.1158=11.58\\%\\) Applicants to Department B \\(\\%_{males}=\\frac{560}{560+25}=\\frac{560}{585}=.9573=95.73\\%\\); and therefore, \\(\\%_{females}=1-.9573=.0427=4.27\\%\\) Applicants to Department C \\(\\%_{males}=\\frac{325}{325+593}=\\frac{325}{918}=.3540=35.4\\%\\); and therefore, \\(\\%_{females}=1-.3540=.6460=64.6\\%\\) Applicants to Department D \\(\\%_{males}=\\frac{417}{417+375}=\\frac{417}{792}=.5265=52.65\\%\\); and therefore, \\(\\%_{females}=1-.5265=.4735=47.35\\%\\) Applicants to Department E \\(\\%_{males}=\\frac{191}{191+393}=\\frac{191}{584}=.3271=32.71\\%\\); and therefore, \\(\\%_{females}=1-.3271=.6729=67.29\\%\\) Applicants to Department F: \\(\\%_{males}=\\frac{373}{373+341}=\\frac{373}{724}=.5152=51.52\\%\\); and therefore, \\(\\%_{females}=1-.5152=.4848=48.48\\%\\) We are done with calculating relative proportions of applicants by gender. Now we need to create some filters. 5.1.2.2.2 Filtering Due to a quirk in jamovi, in the sole case of chi-square goodness-of-fit tests, it was most convenient to save 7 different jamovi (.omv) files. We did this after we created each filter (See section 2.5.2 for how to filter data). In the first filter, we wanted to filter in all the rows that say Admitted under Admit, and filter out all the rows that have Rejected under Admit. We left the variable Dept alone for the first filter. This filter gave us the counts for men and women who were admitted to the six largest graduate schools at Cal. This was easy to do. We just typed the following into the filter function: =Admit==‘Admitted’ (you saw this kind of formula before in Section 2.5.2). Your result should look something like Figure 5.7 below. Figure 5.7: Creating a filter for the UCBAdmissions data set (R Core Team, 2021) that restricts the rows to the counts that refer to admissions only (no rejections). As noted above, there is a strange quirk having to do with the interaction between filters and chi-square goodness-of-fit tests, so we had to save this file as CategoricalData_UCBAdmissions_AllDepartments.omv. But we needed six more filters, each of which would restrict the analysis to one department (i.e., one of A through F). To create the first for Department A, we took the CategoricalData_UCBAdmissions_AllDepartments.omv file and saved it as CategoricalData_UCBAdmissions_DepartmentA.omv [\\((\\equiv)\\) &gt; Save As]. After doing this, we could modify Filter 1 by adding to its conditions. So we re-opened Filter 1, and clicked the \\(+\\) sign on the far, right-hand sign of the line with \\(f_{x}\\). This creates a second line \\(f_{x}\\) line in addition to the one above it. It is pre-pended by the word and. The expression AND is called a conjunction in Boolean logic, and it means “this also must be true.” Ultimately, this means that jamovi will return only the rows where condition 1 (only Admitted\") AND condition 2 (only department A) are both true. You can see the results of this in Figure 5.8 below. Figure 5.8: Creating a filter for the UCBAdmissions data set (R Core Team, 2021) that restricts the rows to the counts that refer to admissions only (no rejections) AND Department A only (filtering out Departments B-F). Note that there are two new columns now: Filter 1 and F1 (2). jamovi does this automatically. F1 (2) means that it’s the second part of Filter 1. Also note that all rows are filtered out except for admissions in Department A. Now we could save this file as another, named CategoricalData_UCBAdmissions_Department.omv (for Department B, obviously). Then, we just changed the second condition to the following: and Dept==‘B’ We needed to repeat this so that we ended up with seven .omv files, each with its own unique filter. Thus: -CategoricalData_UCBAdmissions_AllDepartments.omv -CategoricalData_UCBAdmissions_DepartmentA.omv -CategoricalData_UCBAdmissions_DepartmentB.omv -CategoricalData_UCBAdmissions_DepartmentC.omv -CategoricalData_UCBAdmissions_DepartmentD.omv -CategoricalData_UCBAdmissions_DepartmentE.omv -CategoricalData_UCBAdmissions_DepartmentF.omv 5.1.2.3 Testing Now that we had the relative proportions and the necessary filters, we could proceed to the chi-square goodness-of-fit test to determine whether there was actually gender discrimination in graduate admissions at Cal Berkeley in 1973. First, we wanted to look at the graduate admissions by gender across the university. So we opened the jamovi file CategoricalData_UCBAdmissions_AllDepartments.omv. First, to reduce clutter, we clicked the “eye” icon in the filter menu to remove from view all the rows that were filtered out. All that remained visible were admissions. Then we went to the Analyses tab, and selected Frequencies &gt; One Sample Proportion Tests: N Outcomes - \\(\\chi^2\\) Goodness of fit. When we opened this, we click-and-dragged to the right the left-hand side of the window headed with Proportion Test (N Outcomes). This made the data appear on the left (as in Figure 5.9 below). We then dragged the Gender variable to the Variable box, and the Freq variable over to the Counts (optional) box.51 Next, we needed to add in the expected proportions manually. Recall, this is what we did in Section 5.1.2.2.1. The expected proportions will be the same proportions of males versus females in the total applicant pool. The observed values will be the proportions of men versus women that were actually admitted. To the extend that these proportions differ is the extent to which you have evidence of gender discrimination… possibly. The proportion of male applicants was 59.45%, which we can type in (59.45) to the Expected Proportions box under Male. The complementary proportion for females was 40.55%, so we type in 40.55 under Female. For good measure, we also checked the Expected Counts box. This is actually the reason that you have to create separate files for each goodness-of-fit analysis. The filter applies to all analyses. However, each of our analyses requires different values in the Expected Proportions box. Thus, we need to separate the analyses into different files. This is a jamovi quirk that seems only to apply to one-sample tests in jamovi, like the chi-square goodness-of-fit test. See Figure 5.9 below to get an idea of what our settings were. Figure 5.9: jamovi paramaters for a Chi-square goodness-of-fit test on admitted male versus female graduate applicants at UC Berkeley in 1973. The results of the chi-square are significant, which you can see below. From this analysis, it looks like males are being preferred in admissions over females. This is because although 712 females were expected to be admitted (under the null hypothesis that there was no discrimination), only 557 were. In contrast, although only 1,043 males were expected to be admitted (if there had been no discrimination), 1,198 were admitted (higher than the expected number). This discrepancy was statistically significant: \\(\\chi^2 (1)=56.5,\\;p&lt;.001\\). ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ───────────────────────────────────────────────── ## Level Count Proportion ## ───────────────────────────────────────────────── ## Female Observed 557 0.3173789 ## Expected 711.6525 0.4055000 ## ## Male Observed 1198 0.6826211 ## Expected 1043.3475 0.5945000 ## ───────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ──────────────────────────────── ## χ² df p ## ──────────────────────────────── ## 56.53196 1 &lt; .0000001 ## ──────────────────────────────── But we had more detailed information than just overall admissions by gender. We had information by department. This required six different analyses, where we will use the six remaining files that we created since the Expected Proportions values differed for each department (the ones we calculated in section 5.1.2.2.1). We will not display how to set the parameters in jamovi here. We will simply display the results. 5.1.2.3.1 Department A This file is CategoricalData_UCBAdmissions_DepartmentA.omv. Recall from section 5.1.2.2.1 above that the proportion of women applying was 11.58%, whereas the proportion of men applying was 88.42%. The results of the chi-square goodness-of-fit test for Department A are below. ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ───────────────────────────────────────────────── ## Level Count Proportion ## ───────────────────────────────────────────────── ## Female Observed 89 0.1480865 ## Expected 69.59580 0.1158000 ## ## Male Observed 512 0.8519135 ## Expected 531.40420 0.8842000 ## ───────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ─────────────────────────────── ## χ² df p ## ─────────────────────────────── ## 6.118683 1 0.0133760 ## ─────────────────────────────── The chi-square goodness-of-fit test for department A was significant: \\(\\chi^2 (1)=6.12,\\;p=.013\\). But if you look at observed versus expected values, you’ll see that more females (89) were admitted than expected (70), and fewer males (512) were admitted than expected (531). 5.1.2.3.2 Department B This file is CategoricalData_UCBAdmissions_DepartmentB.omv. The proportion of women applying was 4.27%, whereas the proportion of men applying was 95.73%. The results of the chi-square goodness-of-fit test for Department B are below. ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ───────────────────────────────────────────────── ## Level Count Proportion ## ───────────────────────────────────────────────── ## Female Observed 17 0.04594595 ## Expected 15.79900 0.04270000 ## ## Male Observed 353 0.95405405 ## Expected 354.20100 0.95730000 ## ───────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ───────────────────────────────── ## χ² df p ## ───────────────────────────────── ## 0.09536925 1 0.7574591 ## ───────────────────────────────── The results of the analysis for Department B were not significant: \\(\\chi^2 (1)=0.0954,\\;p=.76\\). Department B seems to have been favoring neither men nor women in their admissions. One more female was admitted (17) than expected (16), and one less male was admitted (353) than expected (354). 5.1.2.3.3 Department C This file is CategoricalData_UCBAdmissions_DepartmentC.omv. The proportion of women applying was higher this time, at 64.6%. In contrast, the proportion of men applying was down to 35.4%. The results of the chi-square goodness-of-fit test for Department C are below. ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ──────────────────────────────────────────────── ## Level Count Proportion ## ──────────────────────────────────────────────── ## Female Observed 202 0.6273292 ## Expected 208.0120 0.6460000 ## ## Male Observed 120 0.3726708 ## Expected 113.9880 0.3540000 ## ──────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ──────────────────────────────── ## χ² df p ## ──────────────────────────────── ## 0.4908472 1 0.4835496 ## ──────────────────────────────── Department C had a similar outcome to Department B. There seems to have been no gender discrimination in their admissions decisions: \\(\\chi^2 (1)=0.491,\\;p=.48\\). Six fewer women were accepted (202) than expected (208), and six more men were accepted (120) than expected (114). Not much of a difference. 5.1.2.3.4 Department D This file is CategoricalData_UCBAdmissions_DepartmentD.omv. The proportion of women applying was roughly even with the men this time, at 47.35%. The proportion of men applying was 52.65%. The results of the chi-square goodness-of-fit test for Department D are below. ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ──────────────────────────────────────────────── ## Level Count Proportion ## ──────────────────────────────────────────────── ## Female Observed 131 0.4869888 ## Expected 127.3715 0.4735000 ## ## Male Observed 138 0.5130112 ## Expected 141.6285 0.5265000 ## ──────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ──────────────────────────────── ## χ² df p ## ──────────────────────────────── ## 0.1963286 1 0.6577007 ## ──────────────────────────────── Department D also seems not to have been engaging in any kind of gender discrimination: \\(\\chi^2 (1)=0.196,\\;p=.66\\). Four more women were admitted (131) than expected (127), and four fewer men were accepted (138) than expected (142). Again, not much of a difference. 5.1.2.3.5 Department E This file is CategoricalData_UCBAdmissions_DepartmentE.omv. The proportion of women applying was higher again, at 67.29% of applicants. The proportion of men applying was down to 32.71% for this department. The results of the chi-square goodness-of-fit test for Department E are below. ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ──────────────────────────────────────────────── ## Level Count Proportion ## ──────────────────────────────────────────────── ## Female Observed 94 0.6394558 ## Expected 98.91630 0.6729000 ## ## Male Observed 53 0.3605442 ## Expected 48.08370 0.3271000 ## ──────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ──────────────────────────────── ## χ² df p ## ──────────────────────────────── ## 0.7470133 1 0.3874235 ## ──────────────────────────────── Department E as well, seems not to have engaged in gender discrimination: \\(\\chi^2 (1)=0.747,\\;p=.39\\). Five more men were accepted (53) than expected (48), and five fewer women were accepted (94) than expected (99). 5.1.2.3.6 Department F This file is CategoricalData_UCBAdmissions_DepartmentF.omv. The proportion of women applying to this department was about even with the men again, at 48.48%. The proportion of men applying was about equal at 51.52%. The results of the chi-square goodness-of-fit test for Department F are below. ## ## PROPORTION TEST (N OUTCOMES) ## ## Proportions - Gender ## ──────────────────────────────────────────────── ## Level Count Proportion ## ──────────────────────────────────────────────── ## Female Observed 24 0.5217391 ## Expected 22.30080 0.4848000 ## ## Male Observed 22 0.4782609 ## Expected 23.69920 0.5152000 ## ──────────────────────────────────────────────── ## ## ## χ² Goodness of Fit ## ──────────────────────────────── ## χ² df p ## ──────────────────────────────── ## 0.2513001 1 0.6161611 ## ──────────────────────────────── And finally, the same is true of Department F: no apparent gender discrimination with \\(\\chi^2 (1)=0.251,\\;p=.62\\). Two more females were accepted (24) than expected (22), in contrast with the males, where two fewer were accepted (22) than expected (24). 5.1.2.4 Reporting See Appendix A for a review of how to report statistics in APA format. One could report the null findings from the first pair of randomly generated variables in the following way: At first blush, it looked as if there had been gender discrimination in the the graduate-admissions process at the University of California - Berkeley in 1973. Indeed, when we applied a chi-square goodness-of-fit test to the counts of male vs. female admissions across the six largest departments (against the respective numbers that applied), we obtained significance, \\(\\chi^2 (1)=56.5,\\;p&lt;.001\\). Men were being admitted at a rate about 10% higher than expected, whereas women were being admitted at a rate about 10% lower than expected. However, when we analyzed the admissions rate by department (the six largest departments only), we found that only one of these departments showed a significant discrepancy between the expected and actual admission rates between the sexes. This was department A, \\(\\chi^2 (1)=6.12,\\;p=.013\\); all other ps &gt; .38. Surprisingly however, it was also clear that females were being favored over males in department A. 5.1.2.5 Conclusion In addition to providing some insight in how to do a chi-square goodness-of-fit test, it is actually also a famous case of Simpson’s Paradox (in fact, it is listed in the Wikipedia entry for the term). This is a phenomenon where an apparent pattern at one level gets cancelled out or even reverses itself at another level. In the 1973 graduate-school admissions data at UC Berkeley, they found evidence for preference for men if they looked at all graduate schools together, but found a small bias for women when they analyzed the data by department. Apparently, large numbers of men had been applying to schools with high admission rates (low competition), as long as they met minimal requirements, whereas women had been applying in larger numbers to departments that had lower admission rates, even among qualified candidates (high competition). This resulted in what looked to be, on aggregate, a bias for men being accepted to graduate school compared to women, but was really only the result of lots of men getting into less competitive programs. These data were originally reported by Bickel, Hammel, &amp; O’Connell (1975). A link to this reference (only) can be found at the Science website here. But if you are a student at Texas A&amp;M, you can obtain the article here. 5.2 Chi-square test of independence This test looks at whether the counts across two variables are random or whether the variables are contingent. For instance, are coffee drinkers just as likely to be larks (“morning people”) as non-coffee drinkers? Are left-handers just as likely to be diagnosed with dyslexia as right-handers? The key difference between this test and the goodness-of-fit test, is that this one looks for balance across two variables (as established by overall group size), whereas the goodness-of-fit test involved only one variable tested against known proportions. 5.2.1 Basic: The Chapek 9 data The Chapek 9 data from Navarro &amp; Foxcroft (2019, sec. 10.2) is from the imagination of the primary author of that book. But it is a good one for illustration. You can obtain the data set from jamovi by clicking \\((\\equiv)\\) &gt; Open &gt; Data library &gt; learning statistics with jamovi &gt; Chapek 9. The story goes that the residents of Chapek 9 (a city) are all robots, and to gain access, you must be a robot. So the robots design a test to determine if someone is a robot or a human.52 The test that the robots devised asked w(whether a candidate to enter the city preferred puppies, flowers, or large, properly formatted data files. But the validity of the test needed to be checked against data that were collected (in someone’s imagination). 5.2.1.1 Inspecting The first thing to do is create a cross-tabulation of the data. Go to the Analyses tab and click Frequencies &gt; Contingency Tables &gt; Independent Samples \\(\\chi^2\\) test of association. species should go in the Columns box, and choice should go in the Rows box.53 The parameters you need to choose to follow Navarro &amp; Foxcroft (2019) are as depicted in Figure 5.10 below. Figure 5.10: Parameter settings in jamovi for a chi-square test of independenc on the Chapek 9 data in Navarro &amp; Foxcroft (2019). The result should look like the following: ## ## CONTINGENCY TABLES ## ## Contingency Tables ## ─────────────────────────────────────────────────────────── ## choice robot human Total ## ─────────────────────────────────────────────────────────── ## puppy Observed 13 15 28 ## Expected 13.53333 14.46667 28.00000 ## ## flower Observed 30 13 43 ## Expected 20.78333 22.21667 43.00000 ## ## data Observed 44 65 109 ## Expected 52.68333 56.31667 109.00000 ## ## Total Observed 87 93 180 ## Expected 87.00000 93.00000 180.00000 ## ─────────────────────────────────────────────────────────── ## ## ## χ² Tests ## ─────────────────────────────────────────────────────────── ## Value df p ## ─────────────────────────────────────────────────────────── ## χ² 10.72157 2 0.0046972 ## χ² continuity correction 10.72157 2 0.0046972 ## N 180 ## ─────────────────────────────────────────────────────────── ## ## ## Nominal ## ──────────────────────────────── ## Value ## ──────────────────────────────── ## Phi-coefficient NaN ## Cramer&#39;s V 0.2440580 ## ──────────────────────────────── For good measure, we also included a barplot with species as a Split by variable in the Descriptives menu (see section 5.1.1.1 above for the right parameter settings). You just additionally slide species into the Split by column. ## ## DESCRIPTIVES In the data set you can see that there were 180 participants, 87 who turned out to be robots (in reality), and 93 who turned out to be human. The main question is whether the responses to flower, puppy, and large, properly formatted data file were equally proportioned across both humans and robots. If so, the test is no good at distinguishing them. But if not, then it might have some value as a shibboleth. The expected values are calculated by dividing the total number of participants (human and robot) who chose, say, puppy, and dividing by the total (180, then applying that ratio to the population of each group. There were 28 participants who chose puppy, which comprises \\(\\frac {28}{180}=15.55\\%\\) So the expected value for humans was \\(Expected_{human}=93 \\times.1555=14.5\\). For robots, this was \\(Expected_{robot}=87 \\times.1555=13.5\\). The same is done for the other two categories (flowers and data files). These are then compared to the observed values for each group through the formula for the chi-square test for independence, which we will get to shortly. 5.2.1.2 Hypothesizing The null hypothesis in this case is that the proportion of preferences for each object across species is the same. Or \\(H_0\\): \\(P_{human_{puppy}}=P_{robot_{puppy}}\\) \\(P_{human_{flower}}=P_{robot_{flower}}\\) \\(P_{human_{ProperlyFormattedDataFile}}=P_{robot_{ProperlyFormattedDataFile}}\\) The research hypothesis would be that any one or more of those equivalencies is false. 5.2.1.3 Testing The formula for this test is in Navarro &amp; Foxcroft (2019, sec. 10.2.1, p. 227), and is as follows: \\[\\chi^2=\\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\] NOTE: The double sigmas \\(\\sum_{i=1}^{r} \\sum_{j=1}^{c}\\) are not as scary as they look. Just replace r with rows (object preferences) and and c with columns (species) and you will see that these are just the combinations of object preferences by species. That is, i takes the values puppy, flower, and large, properly formatted data file, whereas j takes on the values human and robot. Walking through this might help. You start with \\(i=robot\\) on the left-hand sigma, then choose \\(j=puppy\\) in the right-hand sigma. With those two values for i and j (i.e., robots who chose puppies), you can calculate the formula on the right. After you do that, you keep the sigma on the left constant (i.e., \\(i=robot\\)) and change the sigma on the right by one to flower (i.e., robots who chose flowers, or \\(j=flower\\)). Then you plug the appropriate numbers into the formula again to get another number. That gets added to the last one you calculater. Then you do large, properly formated data files. When you have exhasted all the values of j for robots, you then change the sigma on the left to refer to humans (i.e., \\(i=human\\)). Then you start all over again for the different values of j. When you have exhausted all the values for j on every vaue of i, you are done. Plugging in the Expected and Observed values into the formula renders the obtained value. The degrees of freedom are determined by the total number of data points minus the number of constraints. In this case, it’s \\((r-1)(c-1)=(3-1)(2-1)=(2)(1)=2\\). See Navarro &amp; Foxcroft (2019, sec. 10.2.1, pp. 227-228) for more detail. The degrees of freedom define the particular chi-square distribution that the obtained value gets evaluated against. Obviously, this chi-square distribution looks very different from the one above with 3 degrees of freedom (with the cards data). The chi-square distribution in general has the unique feature of changing its shape dramatically depending on the degrees of freedom, something that doesn’t change very much with t-distributions (next chapter). But nothing changes with respect to its function. It defines the probabilities of an infinite set of test outcomes (from an infinite set of study replications) with 2 degrees of freedom, all under the assumption that the null hypothesis is true. With 2 degrees of freedom, you can see that the most common obtained value under the null hypothesis would be 0, with progressively diminishing probabilities for more extreme obtained values. Moreover, the critical value on this distribution (for \\(\\alpha=.05\\)) is 5.991. This means that obtained values of 5.991 or greater are less than 5% likely to occur if the null hypothesis is true. The chi-square test statistic came out as follows: \\(\\chi^2 (2)=10.7,p=.005\\). Thus, our obtained value was quite improbably if the null hypothesis were true. As a result, we have enough confidence to reject the null hypothesis and resort to the research hypothesis. Without going in to too much detail, it is pretty clear from the test results, the table, and the figure that the robots were over-choosing flower and the humans were over-choosing large, properly formatted data files. This effect was not that strong as Cramér’s V was only .244. According to Cohen (1988), who was the great grandfather of effect sizes, this would be a small-ish effect size. 5.2.1.4 Reporting Borrowing from Danielle Navarro’s imagination here a little bit, this is how we could write this up: We carried out a chi-square test of independence on the Chapek 9 data set. The results were significant, \\(\\chi^2 (2)=10.7,p=.005\\). There were roughly as many robots (87) as humans (93) who responded. Responses to puppy looked to be about equally likely across the two groups. However, humans responded with a disproportionate number of preferences for large, properly formatted data files compared to robots, whereas the robots seemed to respond with a disproportionate number of preferences for flower. It turns out that the robots had a policy to murder humans who answered honestly and identified themselves as human with a puppy or flower response. Many of the humans had caught on to this policy, and therefore chose file out of concern for their safety. The unusual number of robot preferences for flower is harder to explain, but is partly due to the fact that humans were avoiding that choice out of fear for their lives. 5.2.2 Advanced: The Narcissistic Personality Inventory We will use the for-fun, online version of the Narcissistic Personality Inventory (NPI) to illustrate how to perform more realistic version of this test in jamovi. This data is part of the Dark Triad. You can take the dark-triad survey yourself for fun here. NOTE: The data in the survey we linked you to, as well as the data we analyze below, are being collected “for fun.” You should not take any results be obtained very seriously. A more serious analysis of the NPI portion was reported by Raskin &amp; Terry (1988), located online here for Texas A&amp;M students. 5.2.2.1 Obtaining the data The data and the codebook are available both available online here. You need to scroll down to the row with Narcissistic Personality Inventory and click the link on the far right-hand side labeled NPI. This will download a .zip file with the files you need. Students in PSYC 301 at Texas A&amp;M can access these files in eCampus under Lab: Lab manual data sets &gt; CategoricalData_NPI. NOTE: This data set is fairly large currently, with over 11,000 observations. Therefore, we will not display the table in this lab manual 5.2.2.2 Preparing One could analyze a combination of almost any of two of the variables Q1 through Q40. The variables don’t have names other than Q##. Respondents simply choose one of two answers that appear on the screen. We arbitrarily chose the following questions to analyze in chi-square test of independence: Q16 and Q24. These two questions carried the following two response possibilities according to the codebook (i.e., codebook.txt): Q16 1 = I can read people like a book 2 = People are sometimes hard to understand Q24 1 = I expect a great deal from other people 2 = I like to do things for other people Thus, presumably, such an analysis would look at whether one’s self-assessed ability to understand others’ behavior (theory of mind) predicts how self-centered one is. Because… why not? The first step was to re-code Q16 according to the codebook (as above). We clicked the Data tab and double-clicked variable Q16. We typed “Theory of Mind” into the Description box, and then typed over “1” with I can read people like a book and “2” with People are sometimes hard to understand. Then we decided, in order to reduce the width of the contingency table, to replace these rather long labels with the shorter versions: Good Theory of Mind and Poor Theory of Mind, respectively. Finally, we made sure that the Data type drop-down menu was set to text. These settings are illustrated below in Figure 5.11. Figure 5.11: Settings in jamovi for the variable Q16 from the online version of the Narcisisstic Personality Inventory from Raskin &amp; Terry (1988). Figure 5.12 below shows how to do the same thing for Q24 (replacing I expect a great deal from other people with Self-centered, and I like to do things for other people with Selfless. Figure 5.12: Settings in jamovi for the variable Q24 from the online version of the NPI from Raskin &amp; Terry (1988). 5.2.2.3 Hypothesizing The null hypothesis in this case is that the proportion of those with a self-assessed good theory of mind are distributed equally across those who are relatively self-centered vs. those who are not. Or \\(H_0\\): \\(P_{good.theory.of.mind_{self-centered}}=P_{bad.theory.of.mind_{self-centered}}\\) \\(P_{good.theory.of.mind_{selfless}}=P_{bad.theory.of.mind_{selfless}}\\) … or alternatively: \\(P_{self-centered_{good.theory.of.mind}}=P_{selfless_{good.theory.of.mind}}\\) \\(P_{self-centered_{bad.theory.of.mind}}=P_{selfless_{bad.theory.of.mind}}\\) The research hypothesis would be that any one or more of those equivalencies is false. This is all we needed for an analysis. 5.2.2.4 Testing First, we reported some descriptive statistics for the survey. We wanted the counts for Q24 (“Selflessness”) split by the levels of Q16 (“Theory of Mind”). This would give us a birds-eye view of the responses out of 11,186 (as of the writing of this lab manual). See section 3.2.3 for instructions on how to do this yourself. ## ## DESCRIPTIVES ## ## Descriptives ## ────────────────────────────────────────── ## Q16 Q24 ## ────────────────────────────────────────── ## N Poor Theory of Mind 6174 ## Good Theory of Mind 5012 ## Missing Poor Theory of Mind 19 ## Good Theory of Mind 17 ## ────────────────────────────────────────── Thus, of the 11,186 responses, about 55% reported having a relatively poor theory of mind (6,174), and about 45% (5,012) reported having a relative good theory of mind. Less than half of one percent failed to provide any response, equally distributed across both levels of Theory of Mind. A cursory glance at the bar plot suggests that those reporting a relatively poor theory of mind are more likely to self-report selflessness than those reporting a relatively good theory of mind. But plots can deceive. We needed to carry out an inferential test to see if this might be a real difference or not. Carrying this procedure out was much more straightforward than the chi-square goodness-of-fit test in the last section. We started by clicking the Analyses tab, and then Frequencies. Drag Q16 to the Rows box and Q24 to the Columns box (or vice-versa; it doesn’t matter). Also click the following options. Statistics: Tests -\\(\\chi^2\\) Nominal Phi and Cramér’s V Cells: Counts Observed counts Expected counts Percentages Row Column See Figure 5.13 for a visual illustration of this. Figure 5.13: Parameter settings in jamovi for a chi-square test of independence on the online NPI data, originally from Raskin &amp; Terry (1988). Choosing these options gave us the following output: ## ## CONTINGENCY TABLES ## ## Contingency Tables ## ───────────────────────────────────────────────────────────────────────────────────── ## Q16 Selfless Self-centered Total ## ───────────────────────────────────────────────────────────────────────────────────── ## Poor Theory of Mind Observed 4521 1653 6174 ## Expected 4278.638 1895.362 6174.000 ## % within row 73.22643 26.77357 100.00000 ## % within column 58.32043 48.13628 55.19399 ## ## Good Theory of Mind Observed 3231 1781 5012 ## Expected 3473.362 1538.638 5012.000 ## % within row 64.46528 35.53472 100.00000 ## % within column 41.67957 51.86372 44.80601 ## ## Total Observed 7752 3434 11186 ## Expected 7752.000 3434.000 11186.000 ## % within row 69.30091 30.69909 100.00000 ## % within column 100.00000 100.00000 100.00000 ## ───────────────────────────────────────────────────────────────────────────────────── ## ## ## χ² Tests ## ────────────────────────────────────── ## Value df p ## ────────────────────────────────────── ## χ² 99.80693 1 &lt; .0000001 ## N 11186 ## ────────────────────────────────────── ## ## ## Nominal ## ───────────────────────────────── ## Value ## ───────────────────────────────── ## Phi-coefficient 0.09445891 ## Cramer&#39;s V 0.09445891 ## ───────────────────────────────── 5.2.2.5 Interpreting The results were “highly” significant, with \\(\\chi^2 (1)=99.8,p&lt;.001\\). We could tentatively conclude that having a good or poor theory of mind does affect your selflessness. Those with poor theories of mind showed higher counts of selflessness (4,521) than expected (4,279), whereas those with good theories of mind showed unexpectedly lower counts of selflessness (3,231) than expected (3,473). The converse was true of the counts of Self-centered. However, the significance may have been primarily an artifact of having such a large sample size (11,186). The effect-size measures (the \\(\\phi\\) coefficient and Cramér’s V) were both quite small at .095. 5.2.2.6 Reporting This is how it might have been written up: We took variables Q16 (“Theory of Mind”: Good vs. Poor) and Q24 (“Selflessness”: Selfless vs. Self-centered) and subjected the relative counts of these levels to a \\(\\chi^2\\) test of independence. All else being equal, the null hypothesis would predict that 69.3% of the respondents for both good and poor theories of mind would have reported that they were selfless, and 30.7% would have reported that they were self-centered. The results were significant [\\(\\chi^2 (1)=99.8,p&lt;.001\\)] with those who self-reported a poor theory of mind also reporting unexpectedly higher counts of selflessness (4,521, or 73.2%) than 69.3% would predict (4,279), and those who self-reported a good theory of mind also reporting unexpectedly lower counts of selflessness (3,231, or 64.5%) than 69.3% would predict (3,473). The effect size was negligible, however, with \\(\\phi=.095\\). Thus, the statistical significance could be an artifact of the large sample (N= 11,186). Incidentally, this is a good example of how a bar plot (see above) can deceive the statistically naïve. The difference in the heights of the bars was probably due mostly to the fact that there were more people overall who self-reported as having a relatively poor theory of mind (6,174 versus 5,012). It’s also an example of how a large sample size can exaggerate the inferential statistics by inflating obtained values (and thereby deflating p-values). Results should only be interpreted in conjunction with an effect size (since effect sizes are designed to temper the interpretation of test statistics by being immune to sample sizes). 5.3 Outside help on chi-square For whatever reason, there is less overall outside help on analyzing frequency data in jamovi. Still, to reinforce your learning here, you can go to datalab.cc (Poulson, 2019). The first video to watch is #45, which discusses analyzing count data in general. We only covered how to do two analyses here, though five are possible under Frequencies in jamovi. Texas A&amp;M students can find these datalab.cc tutorials in the LinkedIn Learning series, available through the Howdy! portal. Your textbook (Navarro &amp; Foxcroft, 2019) obviously has a chapter on chi-square. But it also has a short tutorial on how to create contingency tables in Section 6.1.54 5.3.1 Goodness-of-fit test datalab.cc also provides a review of how to do chi-square goodness-of-fit tests. You can find it as video #47. This is also in your main textbook (Navarro &amp; Foxcroft, 2019, sec. 10.1). 5.3.2 Chi-square test of independence The datalab.cc video for the chi-square test of independence (“Association”) is video #48. This is also in your main textbook (Navarro &amp; Foxcroft, 2019, sec. 10.2). This isn’t even likely for any data you ever collect, unless you are analyzing coin flips, or dice throws (which you are unlikely to be doing).↩︎ The reason that Counts is optional is that it is summarized data. Thus, it is not “tidy,” per se. One could also do this chi-square if each row corresponded to one person, without the Count variable. This would have been a data set with 4,526 rows. In fact, this is normally the way data is collected. In this case, jamovi would have just counted the relevant numbers of rows instead of having them provided in an extra variable.↩︎ This is clearly a spin on Alan Turing’s (1950) Turing Test↩︎ You could do this the other way around; it’s the same. But this is the way it was done in Navarro &amp; Foxcroft (2019). We do the same here so as to reduce confusion.↩︎ The data for this little tutorial in Navarro &amp; Foxcroft (2019) are from In the Night Garden, a television show for really young children that isn’t shown in the US, but the main author (Bolger) got to see extensively while living in Canada when his son was very young. It has achieved some sort of cult status even among adults, who often find it so remarkably bizarre as to be entertaining even for them (at least for short stretches). The website for it is here in case you are interested in a truly strange children’s show.↩︎ "],["ComparingTwoMeans.html", "Chapter 6 Comparing two means 6.1 The independent-samples t-test 6.2 The paired-samples t-test 6.3 Outside help on t-tests", " Chapter 6 Comparing two means This chapter is about inferential tests that compare two means, each with a normal distribution of errors. It follows the main textbook (Navarro &amp; Foxcroft, 2019, Chapter 11). Think of it this way: If you have a single outcome variable, but both men and women contributed to the responses on that outcome variable, then you can split that outcome variable into two means: one for the women, and one for the men. Now you can compare these two means (with an independent samples t-test). In fact, there are four different situations when you might compare two means: When you have collected only one sample (you can’t split your outcome into two), but you have information about a population that you’d like to compare that sample to. In the case where you have access to both a known population mean and standard deviation (known population SAT scores from the College Board), you can use a one-sample z-test In the he case where you have access to the population mean, but not the population standard deviation (just like above, but more much more likely to be your case), you use what’s called a one-sample t-test When you do have two samples that you can split your outcome variable by. In the case where the two samples do not consist of the same people (e.g., men vs. women), you can use an independent-samples t-test In the case where you have the same group of people sampled twice (e.g., time 1 vs. time 2), you can use a dependent-samples t-test, also known as a paired t-test.55 We will skip the one-sample z-test since it is unrealistic to begin with (See Navarro &amp; Foxcroft, 2019 Section 11.1). In fact, we will also skip one-sample t-test for the sake of brevity. The independent samples, and paired-samples t-tests are much more common in practice anyway. 6.1 The independent-samples t-test Carrying out independent-samples t-tests is covered in your main textbook (Navarro &amp; Foxcroft, 2019, Section 11.3). In this test, we are interested in whether two groups comprising independent observations differ with each on a single, continuous outcome variable. The standard equation for this is called Student’s t.56 Of course, you do not need to calculate Student’s t, or any other statistical equation in jamovi; the software does it for you… which is the sine qua non of statistical software in general. So let’s get to that. 6.1.1 Basic: Dr. Harpo’s statistics class For a basic introduction to how to do a t-test in jamovi, we replicated the the example from the textbook in Section 11.3. It’s the Harpo data again, which you already worked on in this lab manual in Section 4.2 under visualizing data with boxplots. 6.1.1.1 Obtaining the data To follow along yourself, you can open that file (if you saved it), or the one in the module from Navarro &amp; Foxcroft (2019): \\((\\equiv)\\) &gt; File &gt; Data Library &gt; learning statistics with jamovi &gt; Harpo.omv. Make sure to modify grade to be continuous (if necessary) and tutor to have two levels: Anastasia and Bernadette (if necessary). 6.1.1.2 Implementing the procedure We clicked the Analyses tab and selected T-Tests. We slid grade into the Dependent Variables box, and tutor into the Grouping Variable box. Under Tests we checked Student’s (for the traditional t-test), as well as the “alternative” test, Welch’s, which actually turns out to be better in most cases (and less confusing!). The research hypothesis (see Chapter 9 of Navarro &amp; Foxcroft, 2019) is non-directional, so we left Group 1 \\(\\ne\\) Group 2 (the default) checked under Hypothesis. As for Additional Statistics, we checked Effect size, Descriptives, and Descriptives plots. The top two options under Assumption Checks, can really be addressed before you get to the inferential-test stage, so we chose not to do them here. However, the Equality of variances is an interesting case in jamovi. jamovi is “smart,”57 so IF you check Student’s without checking Welch’s, then jamovi will add a footnote about the test for equality of variances (Levene’s test). This is because using the Student’s t-test is only appropriate if you have equality of variances across the two conditions. To pass Levene’s test, your p-value must be greater than .05 (not less, like in the regular inferential tests). This is because the null hypothesis for equality of variances is that the variances are equal. So you want to fail to reject that particular null hypothesis. This is counter-intuitive once you have a handle on null-hypothesis significance testing, but you’ll get used to it. Many assumption checks work the opposite way from the inferential tests. But you might also notice (if you play around a bit) that if you also check Welch’s, then Levene’s test disappears from the footnote in the output. You have to ask for it specifically (at the lower right). This is because the (smart) programmers at jamovi knew that if you also selected Welch’s that you really didn’t need the test for equality of variances in the first place since the test is irrelevant for Welch’s t-test (which internally corrects for inequality of variances). But you can still ask for it by checking the box for it at the lower right. We did that here. We went one step further and un-checked Student’s just to simplify the output. Thus, the options shown below in Figure 6.1 should be more than enough to make educated decisions about the t-test. Figure 6.1: Parameter settings for the independent samples t-test using the Harpo data from Navarro &amp; Foxcroft (2019). If you’re following along, you should see something like the following as output: ## ## INDEPENDENT SAMPLES T-TEST ## ## Independent Samples T-Test ## ──────────────────────────────────────────────────────────── ## Statistic df p ## ──────────────────────────────────────────────────────────── ## grade Welch&#39;s t 2.034187 23.02481 0.0536100 ## ──────────────────────────────────────────────────────────── ## ## ## Group Descriptives ## ───────────────────────────────────────────────────────────────────────────── ## Group N Mean Median SD SE ## ───────────────────────────────────────────────────────────────────────────── ## grade Anastasia 15 74.53333 76.00000 8.998942 2.323517 ## Bernadette 18 69.05556 69.00000 5.774918 1.361161 ## ───────────────────────────────────────────────────────────────────────────── 6.1.1.3 Interpreting the output Here you can see that the difference in grades between Anastasia’s and Bernadette’s tutoring groups are marginally significant58 using Welch’s t-test, which is the appropriate choice here. 6.1.1.4 Reporting the output As far as a write-up is concerned, this is how it might come out: The students in the group that Anastasia tutored scored a little bit higher (M = 74.5, SD = 9, n = 15) than those of Bernadette (M = 69.1, SD = 5.77, n = 18). This difference in means was marginally significant, Welch’s t(23) = 2.03, p = 054, with a medium-strong effect size (d = .74). (For a guide on how to report t-tests, see section 12.4.1.3.2) 6.1.2 Advanced 1: The conspiracy data We’re going to use another online open data set. This one is more involved, but fun. It is called the Generic Conspiracist Beliefs Scale (Brotherton, French, &amp; Pickering, 2013). If you like, you can take the survey yourself here. The online version is strictly for fun. 6.1.2.1 Obtaining the data This is an online survey designed to tap into the belief in generic conspiracy theories (there are 15 generic conspiracies covered, which were distilled down, mathematically,59 from 75). You can see these questions here, or in the Appendix of Brotherton, French, &amp; Pickering (2013). Responses to these question were on a Likert Scale, ranging from 1 (Definitely not true) to 5 (Definitely true). NOTE: The data that was collected online is not the data that was reported in Brotherton, French, &amp; Pickering (2013), which was a more serious effort. Rather, the data you have access to was collected for fun. Or as the authors put it in the codebook: “Visitors [to the online survey] completed the test primarily for personal amusement.” A major consequence of this is that you should not take any results you find here very seriously. If you want to find serious answers, read the published article. The data (data.csv) and codebook (codebook.txt) can be found online here as a .zip file, or, for Texas A&amp;M students in PSYC 301, on eCampus under Lab: Lab manual data sets &gt; TwoMeans_GCBS (a folder containing the two files). Feel free to follow along with how we analyzed this data. In this case, as we saw before, the researchers indicated non-responses with a 0. Zero, then, is the number that represents missing values. So before we imported the data, we put 0 (i.e., zero, with no quotes) into the Default missings box under preferences \\((\\vdots)\\). 6.1.2.2 Adjusting the data Once the data was imported (as we did back in Sections 2.3 and 2.8.3), we changed some variable names with some help from the codebook. We also adjusted the variable types. We chose variable Q3 (Secret organizations communicate with extraterrestrials, but keep this fact from the public) as our outcome variable (We called it Comm.Wth.Exts just to save space in the jamovi output). The changes are depicted below in 6.2. Figure 6.2: Changes made in jamovi to the variable Q3 from Brotherton, French, &amp; Pickering (2013). We chose voted (Have you voted in a national election in the past year?) as our two-level predictor variable. We changed the name to Voted.In.Past.Year. The change we made to it is shown in Figure 6.3 below. Figure 6.3: Changes made in jamovi to the variable voted from Brotherton, French, &amp; Pickering (2013). If we wanted to analyze any other variables, we would need to adjust those too. But there is no reason to do so in this manual, though you are welcome to play with the data yourself. 6.1.2.3 Implementing the procedure We used the same settings as above in Figure 6.1, except that we kept the Student’s option checked, and we also checked Equality of variances under the heading Assumption Checks. Figure 6.4: Setting the parameters for a t-test on variables from Brotherton, French, &amp; Pickering (2013). 6.1.2.4 Interpreting the output The output below is what you should see in jamovi if you had only checked Student’s t-test. Notice the footnote telling you that you can’t really use Student’s t-test. We also explicitly included the more detailed output for Levene’s test below the t-test output. There you can see that the failure of this test is a pretty spectacular failure with an F value of 35.9 (trust us, this is a large F value). We’ll get to what this all means when we analyze the final output. ## ## INDEPENDENT SAMPLES T-TEST ## ## Independent Samples T-Test ## ─────────────────────────────────────────────────────────────────────── ## Statistic df p ## ─────────────────────────────────────────────────────────────────────── ## Comm.Wth.Exts Student&#39;s t 5.035711  2462.000 0.0000005 ## ─────────────────────────────────────────────────────────────────────── ##  Levene&#39;s test is significant (p &lt; .05), suggesting a violation ## of the assumption of equal variances ## ## ## ASSUMPTIONS ## ## Homogeneity of Variances Test (Levene&#39;s) ## ───────────────────────────────────────────────────────── ## F df df2 p ## ───────────────────────────────────────────────────────── ## Comm.Wth.Exts 35.94557 1 2462 &lt; .0000001 ## ───────────────────────────────────────────────────────── ## Note. A low p-value suggests a violation of the ## assumption of equal variances In contrast, if you had only checked Welch’s60, then the output is much simpler, and safer to go with. You can see it in the output below, where we have also included descriptive statistics and plots. ## ## INDEPENDENT SAMPLES T-TEST ## ## Independent Samples T-Test ## ───────────────────────────────────────────────────────────────────────────────── ## Statistic df p Cohen&#39;s d ## ───────────────────────────────────────────────────────────────────────────────── ## Comm.Wth.Exts Welch&#39;s t 5.265362 1741.577 0.0000002 0.2172203 ## ───────────────────────────────────────────────────────────────────────────────── ## ## ## Group Descriptives ## ──────────────────────────────────────────────────────────────────────────────────── ## Group N Mean Median SD SE ## ──────────────────────────────────────────────────────────────────────────────────── ## Comm.Wth.Exts No 1672 2.147727 1.000000 1.429473 0.03495891 ## Yes 792 1.848485 1.000000 1.261016 0.04480826 ## ──────────────────────────────────────────────────────────────────────────────────── Clearly, the effect is significant. But how do we interpret that? Well it turns out that with t-tests it’s very simple in the case of a significant effect: The mean that is greater is significantly greater than the other, and vice-versa. But there’s a bigger problem. This is something that would have been caught beforehand if looking specifically at Descriptives (as we did in Chapter 3). But you can see it in two places here: 1) in the table of Descriptives that we asked for; and especially in 2) in the Descriptives plots. Do you see it? Look at the means vs. the medians. The numbers are give explicitly in the table, but the dot plot really shows you what’s happening. The means are plotted as circles flanked by confidence intervals.61 They look quite normal. However, the medians (represented as squares) are both at the bottom. This suggests, as you might expect, that the vast majority of respondents do not believe this kind of conspiracy theory. The distribution must be massively skewed to the right. The best way to see this is with a violin plot with the Data: Jittered option checked (see Section 4.3). ## ## DESCRIPTIVES ## ## Descriptives ## ──────────────────────────────────────────── ## Voted.In.Past.Year Comm.Wth.Exts ## ──────────────────────────────────────────── ## N No 1672 ## Yes 792 ## ──────────────────────────────────────────── Clearly, the distributions are very similar to each other, but they are heavily skewed positively (Remember, violin plots are like histograms [density plots, really] tilted 90 degrees counterclockwise, and then mirrored). The “violins” should be fat in the middle and narrow at the extremes. Instead, both violin plots look like narrow vases with wide bases, where the vast majority of the responses are 1, at the lower end of the scale (which is on the y-axis now, not on the left-hand side of the x-axis, as is true for histograms). The t-test gave us a significant result, but it is not an appropriate test at all. You might be asking yourself why we chose such an inappropriate data set while introducing you to the t-test. Consider this an extreme example of why it is important to check statistical assumptions. Notice that if you just look at the results of the t-test without looking at the distribution of the variables or other diagnostics, you could easily come to the conclusion that the t-test showed us significant differences. This is where the following quotation is appropriate: “There are three kinds of lies: lies, damned lies, and statistics” — Mark Twain purportedly quoting Benjamin Disraeli (but see here) Of course, the quote is both true and false. It is false because statistics cannot in and of themselves lie. After all, they’re not human, and they tell you exactly what you ask them to tell you (perfect truth-tellers in fact). But humans can lie (or more commonly, tell half-truths) with statistics. That’s how the quote is true in this case. 6.1.2.5 Reporting the output Just as you saw with the chi-square analyses in Chapter 5, the APA reporting guidelines follow an almost universal pattern for statistical tests, with the following eight elements appearing left-to-right in order: a letter (in italics) representing the test statistic used parentheses that enclose the degrees of freedom an equals sign the obtained value of the test statistic a comma the letter p in italics one of three symbols: =, &lt;, or &gt; a p-value WARNING: This report is not justified because the analysis was inappropriate, but it is how it would look if the analysis had been appropriate: “An independent-samples t-test was carried out between people who had voted in the previous year and those who had not on degree of agreement with a conspiracy theory in which secret organizations are concealing from the public their contact with aliens. Those who had voted showed significantly less agreement with the conspiracy statement (M = 1.85, SD = 1.26, n = 792) than those who had not voted (M = 2.15, SD = 1.43, n = 1,672) according to Welch’s t(1742) = 5.27, p &lt; .001. The effect size was somewhat small (d = 0.21).” For a guide on how to report t-tests specifically, as above, see Section 12.4.1.3.2. 6.1.2.6 Alternative analysis of conspiracy data Briefly, to deal with this particular problem, one could carry out chi-square analysis. It’s a bit cruder, since you lose a good deal of information, but it will do the trick. First, we would need to compute a new variable in order to do a chi-square analysis as in the last chapter in this manual, which is also in Chapter 10 of Navarro &amp; Foxcroft (2019). 6.1.2.6.1 Recoding the Likert item We created a new two-level factor from the original Likert item. This was done in jamovi through the Data tab. We just clicked Compute from there. This automatically inserted a new column, but without any values. We gave the variable a new name (Count.Comm.Wth.Exts), and inserted the following IF-THEN-ELSE statement into the function box: “IF(Comm.Wth.Exts==1,”Totally Disagree“,”Do NOT Totally Disagree“)” See Figure 6.5 below. Figure 6.5: Computing a new, 2-level nominal variable based on an old continuous variable using an IF-THEN-ELSE function in jamovi. The end result was a new categorical variable with two levels.62 6.1.2.6.2 Doing a chi-square analysis Next we did an independent samples chi-square test of association (see last chapter). The results are displayed in the table below. ## ## CONTINGENCY TABLES ## ## Contingency Tables ## ───────────────────────────────────────────────────────────────────────────────────── ## Count.Comm.Wth.Exts No Yes Total ## ───────────────────────────────────────────────────────────────────────────────────── ## Do NOT Totally Disagree Observed 805 303 1108 ## Expected 751.8571 356.1429 1108.000 ## % within column 48.14593 38.25758 44.96753 ## ## Totally Disagree Observed 867 489 1356 ## Expected 920.1429 435.8571 1356.000 ## % within column 51.85407 61.74242 55.03247 ## ## Total Observed 1672 792 2464 ## Expected 1672.0000 792.0000 2464.000 ## % within column 100.00000 100.00000 100.00000 ## ───────────────────────────────────────────────────────────────────────────────────── ## ## ## χ² Tests ## ─────────────────────────────────────────────────────────── ## Value df p ## ─────────────────────────────────────────────────────────── ## χ² continuity correction 20.83724 1 0.0000050 ## N 2464 ## ─────────────────────────────────────────────────────────── ## ## ## Nominal ## ───────────────────────────────── ## Value ## ───────────────────────────────── ## Phi-coefficient 0.09283359 ## Cramer&#39;s V 0.09283359 ## ───────────────────────────────── The chi-square test was significant. Looking at the contingency table, one can see the following. Among those who did not vote, the expected number of people who totally disagreed with the conspiracy theory was lower (867) than expected (920), whereas among those who did vote, the number of people who totally disagreed was higher (489) than expected (436). Also, among those who did not vote, there was about a 50/50 split between absolutely disagreeing and not-totally disagreeing with the conspiracy theory. In contrast, among those who did vote, those who disagreed outnumbered those who didn’t totally disagree by about 5-to-3. This test is not as potentially informative as having the convenience of 1-5 Likert scale. However, it is more appropriate in this case. You can trust these results more.63 6.1.2.6.3 Reporting the alternative output (See section 12.4.1.3.2 for a guide on how to report chi-square analyses in APA format) What follows is more legitimate report, namely, that of the chi-square analysis: “An independent-samples Chi-square test of association was calculating comparing the relative frequencies of voters vs. non-voters in their propensity to agree with a conspiracy-theory statement concerning secret societies concealing their contact with aliens. There was a significant interaction, \\(\\chi\\)2(1, N = 2464) = 20.8, p &lt; .001. The effect size was quite small (\\(\\phi\\) = .09). There were slightly fewer-than-expected voters who agreed with the statement, whereas there were slightly more-than-expected non-voters who agreed with the statement.” 6.1.3 Advanced 2: Spoken vs. written pitches You will be working on a real data set. The description of the study is below, followed by the relevant tasks you need to carry out, as well as some useful information to get you started. 6.1.3.1 Study description The following activity is a partially altered version of one developed by McIntyre (2016). Imagine you were a job candidate trying to pitch your skills to a potential employer. Would you be more likely to get the job after giving a short speech describing your skills, or after writing a short speech and having a potential employer read those words? That was the question raised by a couple of researchers at the University of Chicago. The authors predicted that a person’s speech (i.e., vocal tone, cadence, and pitch) communicates information about their intellect better than their written words (even if they are the same words as in the speech). To examine this possibility, the authors randomly assigned 39 professional recruiters for Fortune 500 companies to one of two conditions. In the audio condition, participants listened to audio recordings of a job candidate’s spoken job pitch. In the transcript condition, participants read a transcription of the job candidate’s pitch. After hearing or reading the pitch, the participants rated the job candidates on three dimensions: intelligence, competence, and thoughtfulness. These ratings were then averaged to create a single measure of the job candidate’s intellect, with higher scores indicating the recruiters rated the candidates as higher in intellect. The participants also rated their overall impression of the job candidate (a composite of two items measuring positive and negative impressions). Finally, the participants indicated how likely they would be to recommend hiring the job candidate (0 - not at all likely, 10 - extremely likely). 6.1.3.2 Your tasks Start jamovi. Import the data file in eCampus called TwoMeans_SpokeWrit.csv. CAUTION: When you import this .csv file, you will notice that there is no codebook for this data set. However, it is pretty clear that the missing values are blanks. Therefore, in jamovi under \\((\\vdots)\\) &gt; Import: Default missings you should type in \"\", as displayed below in Figure 6.6. Figure 6.6: Setting Default missings to blanks (\"\") in jamovi. Explore the data file. Note, you will not analyze all of these variables. Try to find the variables that are relevant to the study description above. Modify them if necessary. Save it as .omv file with the following name: “SpokeWritExercise_YOURLASTNAME_YOUR FIRSTNAME.omv” (where the elements in uppercase are replaced with your last and first name, respectively). You first want compare participants in the audio condition to participants in the transcript condition on the Intellect_Rating variable. Which type of analysis is appropriate, given the design described above? Next compare participants in the audio condition to participants in the transcript condition on the Impression_Rating variable. Finally, compare participants in the audio condition to participants in the transcript condition on the Hire_Rating variable. Prepare an APA-style results paragraph describing the results of the analyses performed above. 6.2 The paired-samples t-test Paired samples t-tests are used when you have a single outcome variable, but the two groups that you are splitting the outcome variable by consist of the same people (or sometimes, different people who are highly matched on key variables). The key here is that the concept that splits the two means must result in correlated values, meaning that the value for one of the “levels” gives you a good idea of what the value for the other “level” is. You might be asking yourself why we used the term concept instead of variable (after all, you split your outcome variable by another two-level predictor in the case of the independent samples t-test; don’t you do the same thing here?). You need not worry very much; we’re actually just highlighting something about the data set that is typically used for this kind of analysis. In a traditional paired samples t-test, the two levels of the predictor variable are represented as two separate columns.64 So there technically is no variable in the data set, per se, that splits the variable into its two levels. You have to understand that your data set has that structure. When you run the analysis in jamovi, you will need to select two different columns corresponding to those two levels.65 You will see this shortly. 6.2.1 Basic: The Chico data For the basic paired-samples t-test we are going to analyze the sample data provided by Navarro &amp; Foxcroft (2019) in the data set Chico.omv. To find this data set, simply go to \\((\\equiv)\\) &gt; File &gt; Open &gt; Data Library &gt; Chico. The data should look like the table below. Table 6.1: The ‘Chico’ data set from Navarro and Foxcroft (2019). id grade_test1 grade_test2 student1 42.9 44.6 student2 51.8 54.0 student3 71.7 72.3 student4 51.6 53.4 student5 63.5 63.8 student6 58.0 59.3 student7 59.8 60.8 student8 50.8 51.6 student9 62.5 64.3 student10 61.9 63.2 student11 50.4 51.8 student12 52.6 52.2 student13 63.0 63.0 student14 58.3 60.5 student15 53.3 57.1 student16 58.7 60.1 student17 50.1 51.7 student18 64.2 65.6 student19 57.4 58.3 student20 57.1 60.1 Descriptives of the data should come out as follows: ## ## DESCRIPTIVES ## ## Descriptives ## ──────────────────────────────────────────────────── ## grade_test1 grade_test2 ## ──────────────────────────────────────────────────── ## N 20 20 ## Mean 56.98000 58.38500 ## Standard deviation 6.616137 6.405612 ## ──────────────────────────────────────────────────── 6.2.1.1 Implementing the procedure To get a paired-samples t-test, Go to Analyses &gt; T-Tests tabs. Then slide grade_test2 over to the Paired Variables box. It should end up on the left-hand column of that box. Then slide grade_test1 over. This variable should end up in the right-hand column.66 Additionally, make sure that Student’s is checked, along with Effect size and Descriptive plots. Figure 6.7 below shows what you should see after doing all this. Figure 6.7: Setting the parameters for a paired-samples t-test on a simulation of the Chico data from Navarro &amp; Foxcroft (2019). The output from this procedure can be seen below. ## ## PAIRED SAMPLES T-TEST ## ## Paired Samples T-Test ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## statistic df p Cohen&#39;s d ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## grade_test2 grade_test1 Student&#39;s t 6.475436 19.00000 0.0000033 1.447952 ## ──────────────────────────────────────────────────────────────────────────────────────────────── 6.2.1.2 Interpreting The test was clearly statistically significant with p less than .001, with Student’s t at 6.48 (19 degrees of freedom). From the graph, it looks like grades at time of test 2 were significantly greater than at time of test 1. But there is one issue here that might get overlooked in the graph. Normally, one can use confidence intervals to judge significance. Specifically, when confidence intervals do not overlap, or overlap only a little, one can hazard a guess that the difference is significant. This was evident in our basic analysis of the independent-samples t-test above (Section 6.1.1), the Dr. Harpo data. Note that the confidence intervals cross just a little, and the p-value is borderline significant. However, this trick that compares confidence intervals only works with independent-samples (or between-subjects tests). When the comparisons are paired (or repeated measures), confidence intervals do not work so well because they over-estimate the error. That is, they are too wide. Notice here that the confidence intervals cross each other a lot, yet the effect is significant. This is common in paired-samples (or repeated-measures) data. 6.2.1.3 Reporting What follows is how one might report this analysis: “The 20 students in Dr. Chico’s class scored a little bit higher on the second exam (M = 58.4, SD = 6.41) than they did on the first exam (M = 57.0, SD = 6.62). This difference in means was highly significant according to a paired t-test, t(19) = 6.48, p &lt; .001, with a strong effect size (d = 1.45).” 6.2.2 Advanced 1: Randomly generated Chico data For the first advanced, paired t-test we are just going to add a small twist. We are going to modify the data so that you create the data yourself, as you may have tried in Section 2.5.1.2. 6.2.2.1 “Replicating” the data To start, for the purpose of illustration,67 we will replicate the original means and standard deviations of the Chico.omv data set with a different set of data points. Begin by opening a new file in jamovi by clicking \\((\\equiv)\\) &gt; New. Select the default variables (A through C) and delete them using Delete under the Data tab. Double-click a blank variable (at the header, or top row) and choose NEW DATA VARIABLE. Give it the name ID, and make it an ID variable type. See Figure 6.8 Figure 6.8: Creating a new ID variable. Then, double-click another blank column header, but this time choose NEW COMPUTED VARIABLE. Name it grade_test1. In the original, sample data set, the mean of this variable is 57.0, with a standard deviation of 6.62. So in the formula box (\\(f_{x}\\)), type in the following formula: =NORM(57,6.62), then press . See Figure 6.9 below. Do the same for another variable, but name it grade_test2, and type in the formula, =NORM(58.4,6.41). Figure 6.9: Step one of creading a new computed variable, consisting of observations drawn randomly from an infinite number of observations comprising a normal distribution with a mean of 57 and a standard deviation of 6.62. Now, go back to the spreadsheet and start entering values into the ID cells. Start with the top left, and enter “1” (without quotation marks). You’ll see the two computed variables automatically fill up with “random” numbers. Do this for 20 rows, with a different number for each row of ID (the easiest is to use the numbers 1-20, of course, and jamovi will not think that they’re numbers since you set it as an ID variable type). In Figure 6.10 below, you can see that it’s possible to skip rows on the ID variable. The ID variable is not that important for our purposes anyway. Figure 6.10: Filling in rows in the new, computed variable. Note the ability to skip rows on the ID variable. See the table below for OUR data (the odds are astronomically low that you would have the same values since they were generated randomly). Table 6.2: The authors’ randomly generated data set based off of the ‘Chico’ data set from Navarro and Foxcroft (2019). ID grade_test1 grade_test2 1 55.43 49.66 2 56.09 62.77 3 53.42 57.23 4 71.30 50.85 5 62.74 61.41 6 44.42 56.21 7 45.25 59.88 8 51.18 49.55 9 61.99 52.21 10 44.13 63.12 11 50.94 61.10 12 62.97 67.07 13 58.07 59.10 14 57.39 58.84 15 58.00 55.48 16 55.43 62.91 17 57.31 52.78 18 55.74 53.81 19 59.66 58.74 20 54.64 49.98 Now you should do descriptives on your data as you did in Sections 3.2.1 and 3.4. Be sure to click standard deviation. The table below shows the relevant descriptives for our data (again, yours will be different). ## ## DESCRIPTIVES ## ## Descriptives ## ──────────────────────────────────────────────────── ## grade_test1 grade_test2 ## ──────────────────────────────────────────────────── ## N 20 20 ## Mean 55.80603 57.13532 ## Standard deviation 6.638070 5.192624 ## ──────────────────────────────────────────────────── You might be shocked to notice that the means and standard deviations are not exactly the same as they are in the original Chico.omv file. This is because the =NORM() function does not replicate the data set per se. Rather, it draws values at random from a theoretical, normal distribution defined by your parameters (e.g., a mean of 57 and a standard deviation of 6.62), and those theoretical distributions consist of an infinite number of observations to draw from. It also follows that the more observations you add here, the closer you will get to the means and standard deviations of the original settings. We are now ready to analyze the data using a paired-samples t-test. We will show you the results of with our randomly generated data, but yours will necessarily be different. 6.2.2.2 Implementing the procedure You can now run the same test as above in section 6.2.1 and Figure 6.7. The output from this procedure can be seen below. ## ## PAIRED SAMPLES T-TEST ## ## Paired Samples T-Test ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## statistic df p Cohen&#39;s d ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## grade_test2 grade_test1 Student&#39;s t 0.6643604 19.00000 0.5144424 0.1485555 ## ──────────────────────────────────────────────────────────────────────────────────────────────── 6.2.2.3 Interpreting the output Our data is not even close to significant, with a p-value approaching 0.5 (recall, our alpha level is .05, not .5). This contrasts a great deal with the original Chico data from Navarro &amp; Foxcroft (2019). Their data was presumably not randomly generated, but rather artificially constructed to show a particular pattern, namely, scores at time 2 always being higher than scores at time 1 (this was the research hypothesis, in fact). You might expect this, as the two grades from each student are supposed to be correlated with each other. Correlated in this case means that, given one value for a student, you can make a pretty good prediction about their other value. Or specifically, given their score on test 1, you can make a reasonable guess that their score on test 2 will be higher. This is why we do paired-samples t-tests in fact. To determine if this was, in fact, the case, we subtracted the scores for time 1 from the scores for time 2, and called the new variable DIFF. We did this for both data sets. If students are generally doing better the 2nd time, then you should see mostly positive difference scores. This is exactly what we see for the original data set below. Table 6.3: The original Chico.omv data set from Navarro &amp; Foxcroft (2019), with ‘DIFF’ being ‘time 1’ subtracted from ‘time 2.’ id grade_test1 grade_test2 DIFF student1 42.9 44.6 1.7 student2 51.8 54.0 2.2 student3 71.7 72.3 0.6 student4 51.6 53.4 1.8 student5 63.5 63.8 0.3 student6 58.0 59.3 1.3 student7 59.8 60.8 1.0 student8 50.8 51.6 0.8 student9 62.5 64.3 1.8 student10 61.9 63.2 1.3 student11 50.4 51.8 1.4 student12 52.6 52.2 -0.4 student13 63.0 63.0 0.0 student14 58.3 60.5 2.2 student15 53.3 57.1 3.8 student16 58.7 60.1 1.4 student17 50.1 51.7 1.6 student18 64.2 65.6 1.4 student19 57.4 58.3 0.9 student20 57.1 60.1 3.0 We can do the same with our randomly generated data. Table 6.4: The randomly generated Chico.omv data set from Navarro &amp; Foxcroft (2019), with ‘DIFF’ being ‘time 1’ subtracted from ‘time 2.’ ID grade_test1 grade_test2 DIFF 1 55.43 49.66 -5.78 2 56.09 62.77 6.68 3 53.42 57.23 3.81 4 71.30 50.85 -20.45 5 62.74 61.41 -1.33 6 44.42 56.21 11.79 7 45.25 59.88 14.63 8 51.18 49.55 -1.63 9 61.99 52.21 -9.78 10 44.13 63.12 18.99 11 50.94 61.10 10.16 12 62.97 67.07 4.09 13 58.07 59.10 1.02 14 57.39 58.84 1.44 15 58.00 55.48 -2.52 16 55.43 62.91 7.48 17 57.31 52.78 -4.53 18 55.74 53.81 -1.92 19 59.66 58.74 -0.92 20 54.64 49.98 -4.66 It is clear from this second table that these scores are uncorrelated with each other. Some values are higher during the 2nd test, whereas others a lower. The underlying pattern is random, except that the values for time 2 are slightly higher, on average than the values for time 1. That makes sense because jamovi calculated random values for each variable, without any kind of underlying correlation between any two values for one student (i.e., some did better on the 2nd test; some did worse, in about equal). Apologies for leading you up the garden path. But we needed to illustrate that two randomly generated variables are not necessarily correlated (and probably won’t be). It turns out that there was a way to get time 2 correlated with time 1, with the former being slightly greater than the latter. See below. In the NEW VARIABLE formula for time 2, we should not have entered “=NORM(58.4,6.41)” for grade_test2 (see Figure 6.9) but rather “=grade_test1 + NORM(1.4, 0.97)” (which correspond the actual mean and standard deviation of the difference scores in the original data set). This new formula can be read as follows: “take grade_test1 and add to it a value randomly drawn from a distribution (of infinite values) with a mean of 1.4, and a standard deviation of 0.97.” This will generate correlated values within observations. You can see this in the table below if you look at the DIFF scores. Table 6.5: The randomly generated Chico.omv data set (corrected to have correlated levels) from Navarro &amp; Foxcroft (2019), with ‘DIFF’ being ‘time 1’ subtracted from ‘time 2.’ ID grade_test1 grade_test2 DIFF 1 55.43 58.13 2.69 2 56.09 56.96 0.87 3 53.42 57.15 3.73 4 71.30 73.72 2.41 5 62.74 64.67 1.93 6 44.42 45.08 0.65 7 45.25 47.52 2.27 8 51.18 52.34 1.16 9 61.99 61.61 -0.38 10 44.13 46.00 1.86 11 50.94 52.18 1.23 12 62.97 64.52 1.55 13 58.07 60.43 2.36 14 57.39 57.57 0.18 15 58.00 60.56 2.55 16 55.43 58.45 3.02 17 57.31 59.52 2.21 18 55.74 57.53 1.79 19 59.66 61.51 1.84 20 54.64 56.39 1.74 And if we run a paired samples t-test, we get the following results. This result is very close to the original data in Navarro &amp; Foxcroft (2019, sec. 11.5, figure 11.16). ## ## PAIRED SAMPLES T-TEST ## ## Paired Samples T-Test ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## statistic df p Cohen&#39;s d ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## grade_test2 grade_test1 Student&#39;s t 8.191378 19.00000 0.0000001 1.831648 ## ──────────────────────────────────────────────────────────────────────────────────────────────── ## ## ## Descriptives ## ───────────────────────────────────────────────────────────────────── ## N Mean Median SD SE ## ───────────────────────────────────────────────────────────────────── ## grade_test2 20 57.59122 57.84745 6.738242 1.506717 ## grade_test1 20 55.80603 55.91381 6.638070 1.484318 ## ───────────────────────────────────────────────────────────────────── 6.2.2.4 Reporting the output See section 12.4.1.3.2 for a review of how to report the specific t-test statistics in APA format. One could report the null findings from the first pair of randomly generated variables in the following way: Students scored higher on average during the second exam (M = 57.1, SD = 5.19, N = 20) than during the first exam (M = 55.8, SD = 6.64, N = 20). However, the difference in means was quite small, and there was no statistically significant effect of time of test, t(19) = 0.664, p = 0.514. The effect size was also quite small, d = 0.149. Students are doing no better or worse on average during the second exam, compared to the first. But the second randomly generated (but correlated) data set would be significant, and reported as follows: “Students scored higher on average during the second exam (M = 57.6, SD = 6.74, N = 20) than during the first exam (M = 55.8, SD = 6.64, N = 20). Although the difference in means was quite small, there was a statistically significant effect of time of test, t(19) = 8.19, p &lt; 001. The effect size was also quite large, d = 1.83. Students are doing better on average during the second exam, compared to the first.” 6.2.3 Advanced 2: Singing parents This is another real data set from a real study. The description of the study is below, followed by the relevant tasks you need to carry out, as well as some useful information to get you started. 6.2.3.1 Study description The following activity is another partially altered version of one developed by McIntyre (2016). Parents often sing to their children and, even as infants, children listen to and look at their parents while they are singing. The research here by a couple of Harvard researchers sought to explore the psychological function that music has for parents and infants, by examining the hypothesis that particular melodies convey important social information to infants. Specifically, melodies convey information about social affiliation. The authors argue that melodies are shared within social groups. Whereas children growing up in one culture may be exposed to certain songs as infants (e.g., “Rock-a-bye Baby”), children growing up in other cultures (or even other groups within a culture) may be exposed to different songs. Thus, when a novel person (someone who the infant has never seen before) sings a familiar song, it may signal to the infant that this new person is a member of their social group. To test this hypothesis, the researchers recruited 32 infants and their parents to complete an experiment. During their first visit to the lab, the parents were taught a new lullaby (one that neither they nor their infants had heard before). The experimenters asked the parents to sing the new lullaby to their child every day for the next 1-2 weeks. Following this 1-2 week exposure period, the parents and their infant returned to the lab to complete the experimental portion of the study. Infants were first shown a screen with side-by-side videos of two unfamiliar people, each of whom were silently smiling and looking at the infant. The researchers recorded the looking behavior (or gaze) of the infants during this ‘baseline’ phase. Next, one by one, the two unfamiliar people on the screen sang either the lullaby that the parents learned or a different lullaby (that had the same lyrics and rhythm, but a different melody). Finally, the infants saw the same silent video used at baseline, and the researchers again recorded the looking behavior of the infants during this ‘test’ phase. For more details on the experiment’s methods, please refer to Mehr et al. (2016) Experiment 1. 6.2.3.2 Getting the data This open data set is located on eCampus at Lab: Lab manual data sets &gt; TwoMeans_InfantMusic.csv (or as an .omv file). Again, the missing value for this .csv file is a blank. So set the Default missings to two side-by-side quotation marks: \"\" 6.2.3.3 Adjusting the data The first thing you will need to do is filter the experiments down to only those for Experiment 1. The data are deliberately set up such that you can use a filter to restrict the data. There are three variables in the data set named, respectively, exp1, exp2, and exp3. You only need the first for this. It is coded “1” if that observation (row) corresponds to Experiment 1, and “0” if it corresponds to Experiments 2 or 3. You do this by going to the Data tab in jamovi, and finding the Filters icon. Click it and a window will pop up. The filter will be named Filter 1, and a new column will appear on the left of the spreadsheet. The filter formula goes into the box to the left of the formula symbol \\((f_{x})\\). In this box, you should type in the following: exp1==1. The results of this process should look similar to Figure 6.11 below. Figure 6.11: Creating a filter in jamovi. 6.2.4 Your tasks Explore the data file. Note, you will not analyze all of these variables. Try to find the variables that are relevant to the study description above (Section 6.2.3.1. Next, you want to demonstrate that infants attended equally to the two singers during the familiarization trials. Run a paired samples t-test comparing the Gaze to Familiar Song vs. the Gaze to Unfamiliar Song. Finally, compare looking behavior at baseline to looking behavior at test, using a paired-samples t-test. Prepare an APA-style results section to describe each of the analyses conducted above. 6.3 Outside help on t-tests datalab.cc has a nice introduction to the purpose of these tests (minus the one-sample z-test). Choose video #26 (t-tests: chapter overview). You can also find this at Texas A&amp;M via the Howdy! portal as tutorials in LinkedIn Learning. 6.3.1 Independent samples t-test For a just-the-basics online tutorial on the independent samples t-test, go to the jamovi quickstart guide. datalab.cc’s tutorial has a more extensive video tutorial. Choose videos #27. You will also find online tutorials at Statistics for Psychologists (Wendorf, 2018). Choose JAMOVI &gt; JAMOVI: Using the Software. Scroll to the table of contents, and click INDEPENDENT SAMPLES T TEST. On the same website, you can find a tutorial for interpreting the output. Just go to JAMOVI &gt; JAMOVI: Annotated Output. Then scroll down to the table of contents and choose T-TEST (INDEPENDENT SAMPLES). The annotations are based on a slightly older version of jamovi, but they will suffice. Your main textbook (Navarro &amp; Foxcroft, 2019) covers this in Sections 11.3 and 11.4. 6.3.2 Paired samples t-test For a quick online tutorial on the paired samples t-test, go to the jamovi quickstart guide. datalab.cc covers paired-samples t-tests in video #28. You will also find online tutorials at Statistics for Psychologists (Wendorf, 2018). Choose JAMOVI &gt; JAMOVI: Using the Software. Scroll to the table of contents, and click PAIRED SAMPLES T TEST.You can also find there a tutorial for interpreting the output. Just go to JAMOVI &gt; JAMOVI: Annotated Output. Then scroll down to the table of contents and choose T-TEST (PAIRED SAMPLES). Your main textbook (Navarro &amp; Foxcroft, 2019) covers this in Section 11.5. In some cases, you can use a paired-samples t-test for different groups of people, but they must be highly matched (e.g., identical twins, kids from the same home).↩︎ Student’s t would have been called Gossett’s t, had it not been for the strange policies of Gossett’s employer, Guinness Brewing in Ireland. For more on this, see Navarro &amp; Foxcroft (2019), Section 11.2.1., or here↩︎ Actually, its programmers are smart.↩︎ The use of the term marginally significant is a bit controversial. It means that the p-value was close to going below .05, but didn’t quite get there.↩︎ using something called factor analysis, which is covered in Navarro &amp; Foxcroft (2019) Chapter 15, but which we don’t cover in this class↩︎ a better choice in our opinion as well as that of Navarro &amp; Foxcroft (2019); see Sections 11.3.7 and 11.4; also see Delacre, Lakens, &amp; Leys (2017), linked here↩︎ the lines above and below the circle; see Navarro &amp; Foxcroft (2019) Chapter 8 (specifically Section 8.5) for a discussion of confidence intervals↩︎ We could have split up counts of the new variable in a different way e.g., 1-2 in one group and 3-5 in another, but this is definitely the most efficient since the vast majority of respondents answered 1 (Totally disagree)↩︎ Note that if were were serious researchers trying to get this published in a peer-reviewed journal, we would probably get some resistance from a reviewer since our transformation was so extreme. They might ask us to re-collect our data using yes/no scale directly, instead of relying on a transformation. This is not unreasonable. After all, what we did was we made several assumptions about how participants would have responded if the scale had been pitched differently. But we don’t know that they would have responded that way; we can only assume so, which may not be enough for science.↩︎ This is also true of the big sister to the paired samples t-test: the repeated measures ANOVA, which we won’t get in to here, but can be found in the main textbook (Navarro &amp; Foxcroft, 2019), section 13.8↩︎ The more modern way to do this kind of analysis works more like the independent samples t-test with a Split by variable, but this approach, called multilevel modeling or mixed-effects modeling is well beyond the scope of this class. However, for what it’s worth, it is mentioned in your main textbook (Navarro &amp; Foxcroft, 2019), in section 17.1.2, under the bullet point labeled Mixed Models (apologies or the multiple names for identical concepts; it’s one of the curses of statistics).↩︎ You could do this in either order, but we are following Navarro &amp; Foxcroft (2019, sec. 11.5.3). Presumably, they did this in order to get a positive value for Cohen’s d, in order to reduce confusion for the reader.↩︎ This is actually a very useful procedure in the world of data analysis, but it only serves as an illustration here.↩︎ "],["CorrAndLinearReg.html", "Chapter 7 Correlation and linear regression 7.1 Correlations 7.2 Linear regression 7.3 Multiple linear regression 7.4 Outside help on correlation", " Chapter 7 Correlation and linear regression So far, we have only addressed cases where the predictor variable is categorical in nature (e.g., Sex: male/female; Handedness: left/right/ambidextrous). But this chapter is all about how to analyze data when both the outcome variable and the predictor variable are continuous in nature. For example, you might be interested in the proclivity of university students to attend sporting events (measured on a scale from 1-7, 1 being not at all, and 7 being every event I can possibly attend) as predicted by how much they crave social interaction (same scale, 1 being not at all and 7 being all the time). You might expect that students who provide higher values on the first question would also tend to provide higher values on the second, and vice-versa. This sort of analysis cannot be carried out with a chi-square, or t-test, or ANOVA. It can be analyzed, however, as a correlation or a simple regression.68 7.1 Correlations The first analysis in this chapter is correlation. The second is a simple regression. Both involved only two variables: a single, continuous outcome variable, and a single, continuous predictor variable. Underneath however, these two particular analyses turn out to the same thing. The correlation between two continuous variables (which is the kind of regression we deal with in this section) is essentially a special case of a simple regression between two variables. Specifically, correlation is a simple regression where both the outcome variable and the predictor variable are both standardized (i.e., into z-scores).69 Restricting ourselves to correlation, the obtained value for a correlation between two continuous variables is known as Pearson’s r, after Karl Pearson.70 The obtained value of Pearson’s r ranges from -1 to 1, with the extremes (-1 and 1) indicating opposite, but equally strong, directions of correlation, and the middle value, 0, indicating no relationship at all. The closer one is to the extremes (-1 or 1), the stronger the correlation is. The closer one is to the middle (0), the weaker the correlation is. The direction of the correlation (i.e., whether it’s positive or negative) simply indicates the how the variables are related. Specifically, if the values of X tend to increase alongside increases in Y, then the correlation is positive (e.g., height and weight are always positively correlated, though the correlation is not perfect). In contrast, if as the values of X go up, the values of Y tend to go down, then the correlation is negative. This is what you would find if you compared the amount of rainfall to the number of university students who walk or ride their bikes to school (note, that the number of students who drive to school would be positively correlated in this case – thus, the direction of a correlation [positive or negative] has nothing to do with whether the relationship is “good” or “bad”). In the textbook (Navarro &amp; Foxcroft, 2019) in section 12.1.3, the authors simplify the formula for Pearson’s r to the following: \\[r_{XY}=\\frac{COV(X,Y)}{\\hat{\\sigma}_X,\\hat{\\sigma}_Y}\\] But for whatever reason, they don’t simplify it further. This can be done. The easiest formula for Pearson’s r (at least from the perspective of the author of this lab manual) is the following: \\[r_{XY}=\\frac{\\sum{Z_XZ_Y}}{n-1}\\] Or more specifically: \\[r_{XY}=\\frac{1}{n-1} \\sum_{i=1}^n Z_{X_i}Z_{Y_i}\\] where i is an individual observation (e.g., multiply Jerry’s z-score on X by his z-score on Y, then add that product to Jennifer’s z-score on X multiplied by her z-score on Y… etc.) That is, for every observation in your data set, take the z-score for X and multiply it by the z-score for Y; add all those product up; and then divide by the number of observations minus 1 (n-1). There is also a significance test associated with correlation. You can perform this inferential test on its own,71 or as a t-test of a coefficient in a simple regression (more on this later in section 7.2 below). 7.1.1 Basic: Parenthood data This part of the lab manual simply presents the material in Navarro and Foxcroft (2019), Chapter 12.1 to 12.2, but in a more lab-oriented manner. 7.1.1.1 Importing the data The data used to illustrate correlation in this chapter is called Parenthood.csv and it is available from the jamovi module associated with this textbook. Just go to \\((\\equiv)\\) &gt; Open &gt; Data Library &gt; Parentood.csv. The data is fictional, but real in the sense that the first author has a son, who was naturally an infant at one point. It is a fictitious data set where the author recorded three things: their own sleep patterns (in total hours per night) their grumpiness over the course of 100 days (0 meaning not at all grumpy, and 100 meaning extremely grumpy) their son’s sleeping patterns (in total hours per night) This data set is actually a comma-delimited text file (hence, the title of this subsection: Importing). So although jamovi is good at guessing, sometimes it makes mistakes. You want to make sure that all the variables, except perhaps ID, are continuous. So change any mis-specified variables first. You can also re-name the variables, if you wish, into human-readable names, as Navarro and Foxcroft suggest (2019, p. 282). Do this in the top row of each variable when you double click the name. You should also probably save it as a .omv (jamovi) file. For the purposes of display, I chose My sleep (hrs.), Baby sleep (hrs.), and My grumpiness. 7.1.1.2 Familiarization Notice that there are three correlations that we can run here. Namely, correlations between the following variables: their own sleep and the baby’s sleep their own sleep and their own grumpiness the baby’s sleep and their own grumpiness The first two seem like direct relationships. the last seems like an indirect relationship. This is extremely important since there’s a well-known dictum in statistics: Correlation does not imply causation Or more precisely: Correlation does not necessarily imply causation, although all effects will be statistically correlated somehow with their causes. That said, causation is mostly a research-design issue, not so much a statistical one. Importantly, you can probably guess that there will probably be a statistical correlation between the baby’s sleep and the author’s grumpiness. However, such a clearly indirect relationship would be conceptually mediated72 by the author’s own sleep. 7.1.1.3 Analysis In this case, there are multiple possible correlations, not all of them useful (see section 7.1.1.1 above). Nonetheless, it is possible to analyze multiple correlations at the same time in jamovi. Go to the Analyses tab, and click Regression and choose Correlation Matrix from the pull-down menu. In the textbook, Navarro and Foxcroft (2019) have you put all four variables into the analysis, but we will have you only choose the grumpiness and sleep variables (leave day alone). Make sure to check the boxes Pearson under the heading Correlation Coefficients, the boxes Report significance and Confidence intervals (with the interval set to 95%) under the heading Additional Options, and the boxes Correlation matrix and Statistics under the heading Plot. These selections should look like figure 7.1 below. Figure 7.1: Setting the parameters for a correlation matrix across sleep hours (for infant and parent separately) and grumpiness (of parent), from the parenthood.csv dataset in Navarro and Foxcroft (2019). The results of this analysis should appear as below. ## ## CORRELATION MATRIX ## ## Correlation Matrix ## ────────────────────────────────────────────────────────────────────────────────────────────── ## My sleep (hrs.) Baby sleep (hrs.) My grumpiness ## ────────────────────────────────────────────────────────────────────────────────────────────── ## My sleep (hrs.) Pearson&#39;s r — ## p-value — ## 95% CI Upper — ## 95% CI Lower — ## ## Baby sleep (hrs.) Pearson&#39;s r 0.6279493 — ## p-value &lt; .0000001 — ## 95% CI Upper 0.7338535 — ## 95% CI Lower 0.4922450 — ## ## My grumpiness Pearson&#39;s r -0.9033840 -0.5659637 — ## p-value &lt; .0000001 &lt; .0000001 — ## 95% CI Upper -0.8594714 -0.4157643 — ## 95% CI Lower -0.9340614 -0.6861101 — ## ────────────────────────────────────────────────────────────────────────────────────────────── Both results are correlation matrices: one in the form of a table (the top one); the other in the form of a figure (the bottom one). Both of them have the same pattern: Variables are listed across the top from left-to-right The same variables are listed in the same order from top to bottom The intersection (the diagonal) is a variable’s correlation with itself, which will always be 1, so there is no reason putting anything there off-diagonals are correlations between different variables (what we’re interested in) the lower-left triangle duplicates the upper-right triangle, so only one usually contains information (in the table, it’s the upper-right; in the figure it’s the lower-left, though the figure also provides information in the upper-right because we asked for statistics in the plot) As noted earlier, correlations near zero indicate no relationship at all between variables, whereas correlations approaching -1 or 1 indicate strong linear relationships. So here, the confidence intervals are quite informative. Essentially, if the interval does not contain zero, you have a significant effect. This is more informative than p-values (which are also provided) because in addition to statistical significance, the confidence intervals contain a range of possible population correlation coefficients. TIP: An easy way to look at confidence intervals here is to see if both values (for one correlation) are either both negative or both positive. If they are, then the correlation is significant since the range cannot contain 0. If on the other hand, the lower bound is negative and the upper bound is positive, then the range does contain zero, and you do not have significance. Thus, to test for significance, you don’t need to actually look at the values in the confidence intervals, just the signs (+ or -). In the analysis here you can see that all the variables are correlated with each other. The strongest correlation is between the author’s sleep and their grumpiness, \\(r(98)=-0.903, p&lt;.001\\). This is indeed the logical connection to grumpiness. But we can also see that there is a statistically significant correlation between the author’s grumpiness and their baby’s sleep, \\(r(98)=-0.566, p&lt;.001\\) (surely an indirect relationship, mediated by the author’s sleep), and finally between the author’s sleep and their baby’s sleep, \\(r(98)=0.628, p&lt;.001\\), a positive correlation (the more the baby sleeps, the more the author sleeps). The degrees of freedom for Pearson’s r, by the way, is \\(N-2\\). The author’s fake data has 100 recordings of these three variables. Thus, the degrees of freedom are \\(100-2=98\\). There is a useful page online that annotates the output of correlations in jamovi. It is page four of the JAMOVI: Annotated Output document from Wendorf (2018). You can find it here, though you need to choose JAMOVI in the menu across the top, then JAMOVI: Annotated Output. 7.1.1.4 Visualization using scatr The visualization provided by jamovi in the Regression analysis tab is somewhat limited. One of the software developers at jamovi, Ravi Selker, build an add-on module for jamovi named scatr, which does a better job of creating scatterplots. To do this with the current data, make sure that scatr is installed (Modules &gt; Manage installed; find the tab Installed (if scatr is listed there, then stop by clicking the “up” arrow at the upper right); if scatr is not listed under Installed, then find the tab Available and scroll to find scatr and click INSTALL); then click the “up” arrow at the upper-right to get out of this menu. Once scatr is installed, then under the Analyses tab, click Exploration. Choose scatterplot under the scatr sub-menu (from the pull-down menu). At this point, put one of the variables you want to analyze as part of the correlation into the box labeled X-axis, and the other variable you want to analyze into the Y-axis box. Under the sub-menu called Regression Line, choose Linear and (when it subsequently appears) Standard error. Under Marginals, choose either Densities or Boxplots. We chose Boxplots for illustration purposes here (and because they are more informative than density plots). These options are shown below in Figure 7.2 for the variables My Sleep (hrs.) and My grumpiness. Figure 7.2: Setting the parameters for a scatterplot between My grumpiness and My Sleep (hrs.), from the parenthood.csv dataset in Navarro and Foxcroft (2019). The output of this procedure should look something like what is below. As before, you can see that there is a negative correlation between My grumpiness and My sleep (hrs.). As the parent sleeps less, their grumpiness goes up, and as they sleep more, their grumpiness goes down. Now technically speaking (and this really is getting nit-picky), this is the wrong graph for the Pearson’s r. So was the one above generated through the Regression analysis. This is because, as noted in section 7.1 above, Pearson’s r actually uses standardized versions of each variable, and what you see here are the variables in their raw values (just read the axes; a standardized scale would have 0 in the middle). So what’s going on? Are we trying to confuse you? No. As we also mentioned above, the statistical test will come out the same whether you standardize or not. If you do standardize, you will get Pearson’s r; if you don’t, you will get something called the regression coefficient, which works off of the raw values. But the results of the inferential test will be exactly the same. But let’s say that you wanted to get a scatterplot of Pearson’s r, and not the scatterplot for the regression. All you have to do is standardize the variables, as was demonstrated in sections 2.5.1.3.2 and 11.5.3. Once you have the z-scores for each variable, simply run the same analysis using scatr. We have done this in Figure 7.3 below. Figure 7.3: Setting the parameters for a scatterplot between My grumpiness and My Sleep (hrs), both standardized for a closer relation to Pearson’s r, from the parenthood.csv dataset in Navarro and Foxcroft (2019). And the results should appear as below. The outputs are nearly identical, visually, in this case. And again, underneath, they really are the same thing. Sometimes you will see correlations depicted without an X or Y axis, and the reason for that is straightforward: The origin (where both X and Y equal 0) now bisects the data right in the middle of the plot. So visualizing the axes in this case would be both intrusive and redundant. One thing that has changed, however, is that the slope of the line in the second scatterplot (directly above) for the standardized variables (recall that the slope of the line is defined as the change in the value on Y for a one-unit change in X) is now tightly related to Pearson’s r. In fact, the slope in the figure above is now Pearson’s r correlation coefficient, exactly. If you wanted to state it in words, you’d say, “For every standard-deviation increase of one in My sleep (hrs.), there is a corresponding decrease of .903 standard-deviation units in My grumpiness.” Or, “As I sleep more, I get less grumpy.” 7.1.1.5 Reporting the output As already shown in the case of chi-square, and both the z- and t-tests, APA requires a consistent pattern for reporting statistical output: a letter (in italics) representing the test statistic used parentheses that enclose the degrees of freedom an equals sign the obtained value of the test statistic a comma the letter p in italics one of three symbols: =, &lt;, or &gt; a p-value confidence intervals between square brackets, with the lower bound reported first followed by the upper bound (separated by a comma), along with the interval identified (usually 95%) The test letter is now r (not t, z, or \\(\\chi^2\\)). The degrees of freedom is calculated as \\(N-2\\). To report Pearson’s r in APA format, you can get most of the information you need from the correlation matrix that we requested in section 7.1.1.3 above. The test statistic is r. There were 100 days that the author (fakely) recorded these data. Thus, the degrees of freedom is \\(100-2\\) or \\(98\\). Next, we need the three obtained values for Pearson’s r itself (for the correlation between each pair of the three variables). These are also in that same table, namely: My sleep (hrs.) with My grumpiness: \\(-0.903\\) Baby sleep (hrs.) with My grumpiness: \\(-0.5666\\) My sleep (hrs.) with Baby sleep (hrs.): \\(0.628\\) Next, we need the p-values, which are also located in the correlation matrix (since we requested them). The exact values are not given in this case, but rather just the value \\(&lt;.001\\). The reason for this is that correlation matrices can get quite crowded with many variables, so this keeps the display clean-looking. It is enough for our purposes, since the APA discourages reporting exact values below \\(.001\\) anyway. Finally, we should report the confidence intervals, which we asked for in jamovi. These were \\([-0.934, -0.859]\\) \\([-0.686, -0.416]\\) \\([0.492, 0.734]\\) NOTE: The effect size for Pearson’s r is Pearson’s r itself. There is no need, therefore, to report a separate effect size. These inferential statistics were all reported above at the end of section 7.1.1.3. But we will re-produce them below: My sleep (hrs.) with My grumpiness: r(98) = -0.903, p &lt; .001, 95% CI [-0.934, -0.859] Baby sleep (hrs.) with My grumpiness: r(98) = -0.566, p &lt; .001, 95% CI [-0.686, -0.416] My sleep (hrs.) with Baby sleep (hrs.): r(98) = 0.628, p &lt; .001, 95% CI [0.492, 0.734] We can now put together a partial Results section for this one analysis: An analysis of this dataset suggests that the less the parent slept [My Sleep (hrs.)], the worse mood they were in (My grumpiness), \\(r(98)=-0.903, p&lt;.001\\), 95% CI [-0.934, -0.859]. Likewise, the less the baby slept [Baby sleep (hrs.)], the worse mood the parent was in, \\(r(98)=-0.566, p&lt;.001\\), 95% CI [-0.686, -0.416]. This correlation was almost certainly an indirect relationship, mediated by the amount that the parent slept. Finally, the more the baby slept, the more the parent slept, \\(r(98)=0.628, p&lt;.001\\), 95% CI [0.492, 0.734]. This last correlation would be unsurprising to any parent, as would probably any of these correlations. 7.1.2 Advanced SLATED FOR SPRING 2020 7.2 Linear regression Chapter 12 of Navarro and Foxcroft (2019) begins the discussion of linear regression with a simple linear regression (sections 12.3-12.4). They also note that the simple linear regression is essentially the same as Pearson’s r, but without the standardized variables. That is, the simple linear regression uses the raw values of the variables. We also noted this above in section 7.1.1.4. This changes two things. First, the intercept gets a real value. Recall that since Pearson’s r is calculated from standardized variables, the intercept of the line of best fit (i.e., where it crosses the y-axis) is known beforehand. It is located at \\(Y=0\\). In fact, the line must cross at the origin (0,0), since by definition, \\(X=0\\) as well at the intercept. So the axes are rarely represented when Pearson’s r is graphed. It would just add visual clutter in the form of a cross drawn through the middle of all the data. But since the linear regression uses the raw values, there must be a calculated value at the intercept. The reason here is that all lines must have an intercept before we can map them on to a Cartesian coordinate system (an \\(X\\) and a \\(Y\\) axis). It’s just that in the case of linear regression, this intercept must now be calculated. In Pearson’s r, it was known beforehand. Second, the slope of the line changes. Recall that the slope of a line is the change in \\(Y\\) associated with a one-unit increase in \\(X\\). In the case of Pearson’s r, the units were standard deviations since the the variables had been standardized. Thus, the slope can be interpreted as the change in standard deviation on \\(Y\\) due to an increase of one standard deviation on \\(X\\). This is great if you don’t need to make any real-world predictions on values on \\(Y\\). But if you are interested in communicating what actual values on \\(Y\\) are predicted from values on \\(X\\) (e.g., predicted college GPA based on high-school GPA), then you need a linear regression where the slope of the line will represent the change on the real-world value of \\(Y\\) given a one-unit increase in the real-world value of \\(X\\) (e.g., the change in college GPA due to a one-unit [single-grade] increase on high-school GPA). Recall from high-school algebra (or earlier) that the formula for a straight line is as follows: \\[Y=mX+b\\] , where \\(Y\\) is some value you’re trying to calculate based on some \\(X\\) value, combined with an intercept, \\(b\\), and a slope, \\(m\\). Navarro and Foxcroft (2019) use the following formula: \\[Y=bX+a\\] , where, as you might imagine, the slope is \\(b\\) and \\(a\\) is the intercept. This is just the way they do it in Australia, where Navarro is from. It’s a more convenient formula, however, since it corresponds with the way linear regression is often expressed in textbooks. Not always however. Here is a formula that you will find in more advanced textbooks on linear regression: \\[Y=\\beta_0+\\beta_1X\\] Here, \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope. The advantage to this formulation is that anything represented as greek \\(\\beta\\) (greek lowercase “beta”) must be calculated, and anything represented as \\(X\\) is given to you (from the data set). This formulation becomes more handy when you study multiple linear regression. 7.2.1 Basic: Parenthood data (again) Navarro and Foxcroft (2019) use the same data set to illustrate simple linear regression as they did correlation. This makes sense since it is informative to see the similarity across the two analysis options. 7.2.1.1 Import and familiarize As you did above (and you don’t currently have it open), just go to \\((\\equiv)\\) &gt; Open &gt; Data Library &gt; Parentood.csv. You will probably need to make some adjustments as we did for the correlation analysis. Refer to the instructions above in section 7.1.1.1. Also familiarize yourself with the data as illustrated in section 7.1.1.2 above. 7.2.1.2 Analysis Following the textbook (Navarro &amp; Foxcroft, 2019, sec. 12.4.1), go to the Analyses tab and choose Regression &gt; Linear Regression. See Figure 7.4 below. Figure 7.4: Setting the parameters for a linear regression of My grumpiness on My Sleep (hrs), from the parenthood.csv dataset in Navarro and Foxcroft (2019). You should also choose a couple of options under the Model Coefficients box, which lies below the main window (see Figure 7.5 below). These options are the ANOVA test, the Confidence interval (set at 95%), and the Standardized estimate. Figure 7.5: Adding some parameters for the linear regression of My grumpiness on My Sleep (hrs), from the parenthood.csv dataset in Navarro and Foxcroft (2019). The results should look something like the following: ## ## LINEAR REGRESSION ## ## Model Fit Measures ## ─────────────────────────────────── ## Model R R² ## ─────────────────────────────────── ## 1 0.9033840 0.8161027 ## ─────────────────────────────────── ## ## ## MODEL SPECIFIC RESULTS ## ## MODEL 1 ## ## Omnibus ANOVA Test ## ──────────────────────────────────────────────────────────────────────────────────── ## Sum of Squares df Mean Square F p ## ──────────────────────────────────────────────────────────────────────────────────── ## My sleep (hrs.) 8159.876 1 8159.87649 434.9062 &lt; .0000001 ## Residuals 1838.714 98 18.76238 ## ──────────────────────────────────────────────────────────────────────────────────── ## Note. Type 3 sum of squares ## ## ## Model Coefficients - My grumpiness ## ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Predictor Estimate SE Lower Upper t p Stand. Estimate ## ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Intercept 125.956292 3.0160692 119.971000 131.941583 41.76174 &lt; .0000001 ## My sleep (hrs.) -8.936756 0.4285309 -9.787161 -8.086350 -20.85440 &lt; .0000001 -0.9033840 ## ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── This output can be interpreted as follows: If the author gets zero sleep, they will indeed be very grumpy at 125.96 (but could as low as 119 or as high as 132). But for every hour of sleep beyond zero, their grumpiness goes down by 8.94 units (an estimate that could be extreme as -9.79 or as mild as -8.09). Recall that this was also interpretable as Pearson’s r, but only in standard-deviation units. Thus, using Pearson’s r from section 7.1.1.3 above, or just the far right-hand side of the output above (the result of checking the Standardized estimate box in our regression analysis): If the author gets their average amount of sleep, then they will be at average grumpiness. And for every standard deviation increase in sleep, their grumpiness goes down by .903 standard-deviation units. Not very concrete, to say the least. For this reason, the regression coefficients are usually more interesting to talk about. 7.2.1.3 Visualization using scatr We actually did this already in section 7.1.1.4 above. However, we will do it again below, but this time with density plots selected in scatr instead of boxplots: This visualization adds something that was a little bit more difficult to see with the boxplots for each variable in section 7.1.1.4. Namely, the shape of the distribution (as density plots) for each variable is skewed negatively. Using our powers of visual inference, it looks like, at least most of the time, the author is getting just over 7 hours/night of sleep, and has a typical grumpiness level of about 60 or so. Nonetheless, there are some bad nights with little sleep (though not less than 4.5 hours of sleep) and subsequent higher levels of grumpiness (though not higher than about 91). You might also notice the blue-ish shaded area around the linear regression line. This represents the 95% confidence interval for the slope of the line. From the output of the regression, we can see that the standard error for this slope is 0.429. Using our pretty-good-guess formula for the 95% confidence interval, we get the following: \\[CI_{95\\%}=-8.94 \\pm 1.96(0.429) = -8.94 \\pm 0.841 = [-9.78,-8.1]\\] This was actually nearly identical to the confidence interval provided in jamovi in the output above: [-9.79, -8.09], which jamovi calculated using a value from the t-distribution at 95% instead of the value we used from the z-distribution (1.96). Of course, this confidence interval suggests that if we ran this experiment 100 times, and calculated this confidence interval each time, 95 of those confidence intervals would contain the population mean. Assuming this is one of those 95 times, the true grumpiness-to-sleep relation is between -9.79 and -8.09 (using the output from jamovi), with the best estimate at -8.94. You might also have noticed that the confidence interval (the shaded area) near the ends of the line is further away from the line than it is near the middle of the line. This turns out to have a straightforward explanation: The confidence interval refers to a slope, not a fixed point, like a mean. As such, both a shallower and a steeper slope will diverge from the line more towards the ends than towards the middle. To visualize what’s going on here mentally, think of the line as lying on a fulcrum towards the middle of the line, and rotating over that fulcrum like a see-saw through the confidence interval. 7.2.1.4 Reporting the output A Results section reporting this information might appear as follows: The amount that this parent sleeps in hours significantly predicts how grumpy they are on this grumpiness scale (\\(b=-8.94, t(98)=-20.9, p&lt;.001\\)). As the parent gets more sleep, their grumpiness declines, and vice-versa. Note that the intercept was not mentioned since it is meaningless in this data. The author may indeed experience nights without sleep, but not in this data set. The regression line really only relates to the range of values it is based on. Here, the author never seems to have slept less than 5 hours or so. 7.2.2 Advanced: SLATED FOR SPRING, 2020 7.3 Multiple linear regression SLATED FOR SPRING, 2020 7.4 Outside help on correlation For outside help on understanding the output from Regression &gt; Correlation matrix in jamovi, go to the online textbook by Wendorf (2018). From the pull-down menus, choose JAMOVI &gt; Annotated Output, and scroll down to page 4. Note that there is a considerable amount of controversy here. Many would argue that Likert scales are either not truly interval scales, or (when used as predictor variables) not measured without error, or both. These are assumptions of the basic, introductory kind of regression we are doing this is class (ordinary least squares), though there are more advanced solutions to these problems (e.g., ordinal logistic regression and errors-in-variables modeling, respectively). At the same time, such questions are beyond the scope of this class. They are worth noting, however, to return to later when your statistical knowledge is sufficiently advanced.↩︎ Note that in the case of multiple regression, when there is more than one predictor variable, you cannot derive the correlation (Pearson’s r) from any of your continuous predictor variables since the math is different.↩︎ Karl Pearson should not be confused with his son, Egon Pearson, who also became an historically important statistician. Most notably, we can partly credit the son with the decision-making component (the “Neyman-Pearson approach”) of null-hypothesis significance testing that the frequentist statistics you are learning in this class revolves around.↩︎ with its own lookup table, if you happen to be in Papua New Guinea with a dead laptop battery, but a functioning hardcopy of a statistics book↩︎ When the causal relationships among variables is unclear, one can employ mediation analysis, a set of statistical procedures designed to pull apart how inter-correlated variables are directly vs. indirectly related to each other. But this also is beyond the scope of this course.↩︎ "],["ComparingSeveralMeans.html", "Chapter 8 Comparing several means 8.1 The One-Way ANOVA 8.2 Repeated-measures One-Way ANOVA 8.3 Outside help on One-Way ANOVA 8.4 Extra practice One-Way ANOVA", " Chapter 8 Comparing several means This chapter explains how to use jamovi to analyze a continuous outcome variable when it is split into three or more levels of a categorical predictor variable. It follows the main textbook (Navarro &amp; Foxcroft, 2019, Chapter 13) For example, you might analyze something like attitude towards increasing taxes to fund public education. And your participants might have been, say, Democrats, Republicans, and “Others.” In this case, you could look at their mean responses by group, and figure out if any of these means were significantly different from any of the others. 8.1 The One-Way ANOVA Carrying out One-Way ANOVA is covered in your main textbook (Navarro &amp; Foxcroft, 2019, Sections 13.1 to 13.7). In this test, we are interested in whether three or more groups comprising independent observations differ with each on a single, continuous outcome variable.73 The F-test is the calculation of a particular kind of ratio, the F-ratio (unsurprisingly!). It is the ratio of between-subjects variance (Mean Squares Between) divided by the within-subjects variance (Mean Squares Within). Each of these, respectively, is the ratio of the Sum of Squares (between or within) divided by the degrees of freedom (between or within). Thus: \\[F= \\frac{MeanSquares_{between}}{MeanSquares_{within}} = \\frac{\\frac{SumOfSquares_{betweeen}}{DegreesOfFreedom_{between}}}{\\frac{SumOfSquares_{within}}{DegreesOfFreedom_{within}}}\\] And this is a pretty complex formula if spelled out all the way. You can refer to Table 13.1 in (Navarro &amp; Foxcroft, 2019, p. 334) if you want to see how this works. In fact, Table 13.1 is in a standard ANOVA-table format, though we won’t go into that in this manual. Naturally, you do not need to calculate the F-ratio by hand. jamovi will do it for you. 8.1.1 Basic: Clinical Trial data The data set that Navarro and Foxcroft (2019) use to illustrate the One-Way ANOVA is fictional. It is a hypothetical clinical trial in which which 18 patients with moderate to severe depression are recruited to participate in a trial in which half receive Cognitive Behavioral Therapy in contrast to half who do not. Each half then receives one of three drugs: joyzepam, Anxifree (a real antidepressant), and a placebo. Three participants are in each group, across both levels of therapy type. Their mood is measured at the beginning and then 3 months later on a 10-point scale from -5 to 5. This lab manual simply presents the dataset and analysis in a more lab-like way. 8.1.1.1 Obtaining the data To follow along yourself, you can open the in the module from Navarro &amp; Foxcroft (2019): \\((\\equiv)\\) &gt; File &gt; Data Library &gt; learning statistics with jamovi &gt; Clinical Trial.omv. We are only going to look at the effect of drug on mood.gain (the outcome variable). 8.1.1.2 Implementing the procedure Simply click the Analyses tab and select ANOVA. Choose the ANOVA option there too (skipping over the One Way option as that is more limited). When the options window opens, slide mood.gain into the Dependent Variables box, and drug into the Fixed Factors box. Under Effect Size, click either \\(\\eta^2\\) or partial \\(\\eta^2\\) (which are identical in a One-Way ANOVA). See Figure 8.1 directly below for a depiction of these options. Figure 8.1: Main parameter settings for the ANOVA procedure using the Clinical Trial data from Navarro &amp; Foxcroft (2019). Next, click the arrow pointing to Model and make sure that drug is placed under Model Terms (and leave Sum of squares as Type 3. There is no built-in correction for heterogeneous variances in this procedure, but you can test the assumption using Levene’s Test for Homogeneity of Variance. This avails itself if you check the box Homogeneity tests under Assumption Checks. See Figure 8.2 below for a depiction of these options. Figure 8.2: Model settings and assumption checks for the ANOVA procedure using the Clinical Trial data from Navarro &amp; Foxcroft (2019). Under Post-Hoc Tests, make sure that drug is in the box on the right, and that the Correction is set to Holm. This is a more powerful way to conduct multiple post-hoc analyses than Bonferroni adjustments (see Navarro and Foxcroft (2019, sec. 13.5.4, pp. 343-344) for a more thorough explanation). Scheffé and Tukey are also reasonable options. Ultimately however, all choices end up leading to the same conclusion in this case, which is often the case. See Figure 8.3 below for how to select these options. Figure 8.3: Multiple-comparison, post-hoc settings for the ANOVA procedure using the Clinical Trial data from Navarro &amp; Foxcroft (2019). Finally, under Estimated Marginal Means, make sure that the following are true: drug makes it over to the box on the right under Term 1 both boxes are checked under Output (Marginal means plots and Marginal means tables) Equal cell weights is checked (the default) Error bars (under Plot) is set to Confidence interval and for fun, check the box Observed scores.74 These options are shown in Figure 8.4 below. Figure 8.4: Graphic settings for the ANOVA procedure using the Clinical Trial data from Navarro &amp; Foxcroft (2019). The results should appear as follows: ## ## ANOVA ## ## ANOVA - mood.gain ## ────────────────────────────────────────────────────────────────────────────────────────── ## Sum of Squares df Mean Square F p η²p ## ────────────────────────────────────────────────────────────────────────────────────────── ## drug 3.453333 2 1.72666667 18.61078 0.0000865 0.7127623 ## Residuals 1.391667 15 0.09277778 ## ────────────────────────────────────────────────────────────────────────────────────────── ## ## ## ASSUMPTION CHECKS ## ## Homogeneity of Variances Test (Levene&#39;s) ## ──────────────────────────────────────── ## F df1 df2 p ## ──────────────────────────────────────── ## 1.449739 2 15 0.2656941 ## ──────────────────────────────────────── ## ## ## POST HOC TESTS ## ## Post Hoc Comparisons - drug ## ─────────────────────────────────────────────────────────────────────────────────────────────────── ## drug drug Mean Difference SE df t p-holm ## ─────────────────────────────────────────────────────────────────────────────────────────────────── ## anxifree - joyzepam -0.7666667 0.1758577 15.00000 -4.359586 0.0011211 ## - placebo 0.2666667 0.1758577 15.00000 1.516378 0.1502131 ## joyzepam - placebo 1.0333333 0.1758577 15.00000 5.875963 0.0000914 ## ─────────────────────────────────────────────────────────────────────────────────────────────────── ## ## ## ESTIMATED MARGINAL MEANS ## ## DRUG ## ## Estimated Marginal Means - drug ## ──────────────────────────────────────────────────────────────── ## drug Mean SE Lower Upper ## ──────────────────────────────────────────────────────────────── ## anxifree 0.7166667 0.1243502 0.4516206 0.9817128 ## joyzepam 1.4833333 0.1243502 1.2182872 1.7483794 ## placebo 0.4500000 0.1243502 0.1849539 0.7150461 ## ──────────────────────────────────────────────────────────────── 8.1.1.3 Interpreting the output The first thing to notice is actually the second table down. For Levene’s test of Homogeneity of Variance, the null hypothesis is that the distributions of the variables is equal across groups (the assumption itself). Therefore, you want to fail to reject the null hypothesis, which is what happened here as p = .266. This also means that there is no problem interpreting the table above it, which is covered next. The second thing to notice is the table at the top. This is known as an ANOVA table. It has a very specific structure. The key value is the p-value in the second-to-last column. It is below .05, so you can reject the null hypothesis that the three drugs have an equal effect on patients. How to report the F-test in APA format is covered below in section 8.1.1.4. However, another key value is in the last column. This is the effect size, \\(\\eta_p^2\\). This statistic is also a bit redundant in the table since it is actually derived from column two. Well, when there is one predictor variable, it is equivalent to \\(\\eta^2\\) (eta squared), which is available from the table as the sum of squares between divided by the total sum of squares. The effect size here of .713 is quite large. \\[\\eta^2 = \\frac{SumOfSquares_{between}}{SumOfSquares_{total}}= \\frac{SumOfSquares_{between}}{SumOfSquares_{between}+SumOfSquares_{within}}=\\] \\[\\frac{3.45}{3.45+1.39} = \\frac{3.45}{4.84} = 0.713\\] Naturally, that F-test is an omnibus test, and only tells us that there are some significant differences among the three means, but it doesn’t tell us which. Therefore, there are also some post-hoc tests that we called for, which are located in third table down. It appears that there is a significant difference between joyzepam and each of the other two drugs, but not between the other two drugs themselves. This is clear in the figure, where you can see that the confidence interval for joyzepam does not overlap with either of the other two drugs, but the confidence intervals of the other two drugs overlap with each other quite a bit.75 Also note that the Estimated Marginal Means may be slightly different than the means that you obtain from running Exploration &gt; Descriptives in jamovi. This topic is beyond the scope of this class as it has to do with how means are calculated multiple regression. Although these are legitimate estimates, you should report the means and standard deviations from the Descriptives procedure instead of the Estimated Marginal Means from the ANOVA output. 8.1.1.4 Reporting the output As already shown in the case of chi-square, the z- and t-tests, and the correlation/regression chapter, APA requires a consistent pattern for reporting statistical output: a letter (in italics) representing the test statistic used parentheses that enclose the degrees of freedom an equals sign the obtained value of the test statistic a comma the letter p in italics one of three symbols: =, &lt;, or &gt; a p-value There will be only two differences this time: The test letter is now F (not t or \\(\\chi^2\\)) There are two degrees of freedom instead of one To report the F-test in APA format, you can get all the information you need from the second table in the output above (the ANOVA table). The test statistic is F. There are two degrees of freedom: one for the sum of squares between, and one for the sum of squares within. These are located in the third column under df. They are 2 an 15, respectively, in that order [i.e., (\\(df_{between}\\),\\(df_{within}\\))]. Next, we need the F-ratio itself, which is located in the fifth column under F. As noted above, the p-value is in the second-to-last column. We also report the effect size, which is in the last column. All together, the full report of the test statistic, in the order presented directly above, is as follows: \\(F(2,15)=18.6,p&lt;.001, \\eta_p^2=0.713\\) We can now put together a partial Results section for this one analysis: The mood improvement over the course of three months differed significantly across the three groups of patients taking either joyzepam, anxifree, or a placebo, \\(F(2,15)=18.6,p&lt;.001, \\eta_p^2=0.713\\). Post-hoc comparisons with Holm corrections revealed that there was a significant difference between joyzepam and both anxifree [\\(t(15)=-4.36,p&lt;.001\\)] and the placebo [\\(t(15)=5.88,p&lt;.001\\)]. However, there was no significant difference between anxifree and the placebo [\\(t(15)=1.52,p=.15\\)]. These differences are also clear in Figure 1, where it is clear that the 95% confidence interval for joyzepam does not overlap with those of either of the other two conditions, whereas the confidence intervals for the latter two conditions overlap substantially. joyzepam is clearly resulting in a significantly greater mood gain than the other two conditions. Additionally, the effect size (\\(\\eta^2=0.713\\)) is quite large, suggesting that this difference between joyzepam and the other two conditions is quite substantial in magnitude. 8.1.2 Advanced Everything below Scheduled for Spring 2020 8.2 Repeated-measures One-Way ANOVA 8.3 Outside help on One-Way ANOVA 8.4 Extra practice One-Way ANOVA Technically, you can also do this kind of test on a categorical variable with only two levels, something you would normally think to do an independent-samples t-test on. However, the results will be the same in this case, except that the ultimate obtained value for F (the ANOVA test) will be the equivalent of the obtained value for t, but squared. (Navarro &amp; Foxcroft, 2019) talk about this briefly in section 13.10.↩︎ 22 Oct 2019: Note however that you may get a strange message in German here: unerwartetes Symbol 1:…, which means “Unexpected symbol 1:….” This seems to be a bug in jamovi. If you get it, just uncheck the box; the error message goes away.↩︎ Note that this is an approximate rule. Sometimes confidence intervals do overlap a little between variables that turn out to be significantly different.↩︎ "],["FactorialAnova.html", "Chapter 9 Factorial ANOVA 9.1 Basic: joyzepam data 9.2 Advanced: advanced data NAME 9.3 Conclusion 9.4 Outside help on factorial ANOVA 9.5 Extra practice on factorial ANOVA", " Chapter 9 Factorial ANOVA UNDER CONSTRUCTION Scheduled for November 26, 2019 &gt;&gt;&gt; Sometimes you have more than one predictor variable. This chapter covers the case when you have two or more categorical predictor variables that might operate independently or together to account for variance in the outcome variable. This is called a Factorial ANOVA. 9.1 Basic: joyzepam data 9.1.1 Obtaining the data 9.1.2 Implementing the procedure 9.1.3 Interpreting the output 9.1.4 Reporting the output 9.2 Advanced: advanced data NAME 9.2.1 Obtaining the data 9.2.2 Implementing the procedure 9.2.3 Interpreting the output 9.2.4 Reporting the output 9.3 Conclusion 9.4 Outside help on factorial ANOVA 9.5 Extra practice on factorial ANOVA "],["SurveyDesignAndDataCollection.html", "Chapter 10 Survey Design and Data Collection 10.1 Unit Overview 10.2 Chapter Overview 10.3 What is Science? 10.4 Designing the survey 10.5 Collecting the Data", " Chapter 10 Survey Design and Data Collection 10.1 Unit Overview This chapter and the one that follows detail the tasks necessary to carry out your lab-based research projects in PSYC 301 at Texas A&amp;M University. This particular chapter outlines the tasks involved in designing your research study and collecting data. The next chapter (Chapter 11) shows you what you need to do to generate the data set and the specific statistical analyses necessary to write your papers (and then how to your papers are covered in Chapters 12 to 17, along with the Appendixes). 10.2 Chapter Overview The steps described in this section, after they are implemented, will provide you with the raw data necessary to complete the writing assignments in the lab. It is also very desirable, though not absolutely necessary, that you have a good study design. Otherwise, it doesn’t matter how much data you collect; it will be flawed. With that in mind, this chapter covers what you need to do, step-by-step, in order to maximize the potential of obtaining results that are easy to write about (i.e., interesting results). There are two parts: (1) Designing the Survey (section 10.4); and (2) Collecting the Data (section 10.5). The first of these is occupies much more space in this chapter than the second. The reasons for the more extensive coverage of study design are twofold. First, we have very specific requirements here. Whatever topic you choose, the variables in the survey need to conform to the statistical procedures that are taught in the main class. If you do not follow the guidelines here, you may not be able to complete the writing assignments. Thus, understanding and implementing the advice below is critical for you in order to pass this course. Second, research design may be the most important part of any scientific study. In order to produce meaningful results, you must collect sound data, and in order to collect sound data, you must have a sound design. Even if you don’t know how to analyze your data, you can find someone else who can (e.g., the Statistical Collaboration Center at Texas A&amp;M, which provides consulting services to faculty all across Texas A&amp;M). However, if a poor research design has resulted in the collection of poor data, there is not much even the most sophisticated statistical consultant can do. You may have wasted your time (or worse, someone else’s money [e.g., a grant]) collecting bad data. This basic requirement of any study design is captured in the phrase garbage in, garbage out. That is, if you collect bad data, you will end up with bad analyses. Thus, following the advice below is critical for you to become a good scientific researcher beyond this class. 10.3 What is Science? The scientific process could be conceptualized as follows:76 science: the collective art of answering empirical questions that is both demonstrable and replicable An empirical question is one that can be answered by means that can be experienced in the same way by other humans. This entails that the phenomena under consideration must be natural; you can ask questions about supernatural phenomena, but they cannot be answered via the scientific method. This is because the methods that you use must be both demonstrable and replicable. That is, you not only have to be able to show your results to others, but you must also be able to show them how to do it themselves in order to (potentially) derive the same results. And replicability necessarily entails objective measurement. Broadly speaking, if it can’t be measured, it can’t be part of science. Below, we expand upon the notion of measurement, which is really your focus in this chapter. But we included another term: collective. The reason for that there are collectively agreed-upon guidelines to this process that have been established over many years by practitioners of many different fields, including scientists themselves, engineers, mathematicians, statisticians, and philosophers. It is a collective enterprise. Furthermore, science relies on consensus, where, although the science is technically never settled in an absolute sense (as a proof would be in mathematics), truth is established only when particular theories reach a point where researchers no long bother to challenge them as such challenges seem almost certainly bound to fail empirically. In other words, scientific truths are theories that scientists no longer challenge. In that sense, consensus is less a component of the scientific process itself, and more like an outcome (albeit an outcome that serves as the foundation for more science and, often [hopefully], decision-making in the public sphere). 10.3.1 What is measurement? In order for science to function, therefore, the phenomena that are to be analyzed must be measured. Measurement is one component of the scientific process that even the most un-scientific among us can understand. Broadly speaking, in the scientific context, measurement is the process of assigning a reliable number or category to instances of phenomena (concrete or abstract), using some common, standard unit, in order to differentiate that instance from other instances of the same type. Importantly, the unit used must be one that anyone else in one’s community can also reliably obtain and use in their own measurement This unit can be quantitative (a number) or qualitative (a category). 10.3.1.1 Numerical measures For some phenomena, numbers can be used for measurement. And for some measures, the numbering can be quite precise, where a physical benchmark is used. The kilogram had such a benchmark until May of 2019. It was a cylinder of platinum iridium (the International Prototype of the Kilogram until May 2019. Since then, it has been based on physical constants, a more stable, but more abstract reference. For measures of more abstract things (e.g., aggression) scientists may need to establish weaker, but (hopefully) reliable measurement standards. One that is quite common in psychology (and elsewhere) is the Likert Scale.77 The Likert Scale is covered in more detail in section 10.4.1.1. There are other types of numerical scales that one might use to measure psychological phenomena. For instance, one might ask someone to rate the morality of a behavior they have been exposed to (e.g., on video) on a scale from 1-10 with 1 being not ethical at all, and 10 being very ethical. This is not a Likert scale since it does not necessarily have a middle, neutral value. So far we have presented three cases of numerical measurement in this section. But measurement also includes assigning categories to phenomena. Most people do not usually think of applying categories to objects as a form of measurement; it is though. Still, as easy as it sounds, it is fairly common among novice researchers to create nominal variables that are flawed in terms of analysis potential. This is discussed more thoroughly in section 10.4.1.2.2 below. 10.3.2 What are constructs? A construct is a scientific term that refers to the mental process of constructing a label that encapsulates common features of and/or causes for a natural phenomenon. Scientists do this constantly when describing natural phenomena and trying to come up with explanations for them. In fact, humans outside the scientific domain do this all the time as well, just not with all the formalities, constraints, and tools that scientists use. For example, new, everyday words are entering the language all the time that describe phenomena (usually human behaviors). You could just do an internet search for the word neologism (a new word in the language).78 On one European website that lists English neologisms, the first two that could conceivably be turned into new areas of psychological research seemed to be anticipointment, from 2016: anticipointment: Anticipated disappointment, often after launch of a heavily hyped product or service Another was broflake (shortlisted for 2017 Word of the Year by the Oxford English Dictionary): broflake: A derogatory term for someone, usually a white male, easily offended by views which conflict with their own world view The first neologism listed that was actually a new scientific construct was anti-nutrients (from 2017): anti-nutrients: Natural or synthetic compounds found in a variety of foods that interfere with the absorption of vitamins, minerals and other nutrients, e.g. in the human body Most of these new layperson’s constructs will not end up being objects of scientific interest, for example: Al Desko: eating at your desk But what is important here is that the notion of a scientific construct is really just an everyday human process, but contextualized scientifically. On that note, to make a construct scientific, it must be operationalized, which is next. 10.3.3 What is operationalization? Operationalization is simply the process of assigning a measure to a construct. Typically, an operationalization is flagged by the phrase as measured by. A non-controversial example would be as follows: height as measured in centimeters In psychology, we do not always have constructs that are so easily measured For example, we cannot measure conscientiousness with any physical measure, unless someday we find a way to get at its implementation in the brain (decades away, presumably). But we can get at it for now with a Likert item, or two, or three.79 For example: On a scale of 1 to 5, with 1 being strongly disagree and 5 being strongly agree, rate the following statement in terms of how it applies to you: Disorganized settings make me feel uneasy. Thus, we have operationalized conscientiousness (a small feature of it to be precise) using a Likert scale: Conscientiousness as measured by a Likert scale of 1 to 5, with 1 representing… and 5 representing… And once you have operationalized a construct, you can begin collecting data if you implement a way to do it. Once you have done so, you will have a variable, which is explained next. 10.3.4 What are variables? A variable is a quantity or quality that can adopt any value from a given range of values. For example, Height (in centimeters) is a variable that can take on any value from zero onward. A human might be 155 or 170 cm tall. With our example from conscientiousness above, we operationalized it on a Likert scale. As a result, Conscientiousness is a variable that can take on any value between 1 and 5 (but not 6 or 7, since we did not operationalize it that way). Tautology aside, even qualitative variables are variables. So biological Sex can take on two values: female and male.80 And Class Level (assuming you are an undergraduate at a US university) can be any of the following: freshman, sophomore, junior, or senior.81 Since this is a statistics course, we will also conceptualize our variables along another dimension: their role in statistical analyses. For now, we will restrict this to outcome and predictor variables. 10.3.4.1 What are outcome variables? In the correlational tradition (as opposed to the experimental tradition), an outcome variable is a measure of some construct that is fundamental to your study. It is usually the centerpiece of your research question. And it is also usually the variable over which the researcher has the least control in terms of the values that it takes; rather, the researcher gives the participant the most freedom in providing the values themselves. Ultimately (when we study linear regression), you will see that outcome variables are equivalent to the Y in the following linear equation, which should be familiar to you: \\[Y = mX +b\\] 10.3.4.2 What are predictor variables? In the same correlational tradition, a predictor variable is any variable that you use to tease apart your outcome variable in some scientifically interesting way. You use them to look at aspects of the outcome variable. For instance, if your outcome variable is reaction time, you might look at how men vs. women respond, that is, reaction time in men vs. women. If you do this, your predictor variable is Sex. You could also have a continuous predictor variable like Age (e.g., reaction time as predicted by Age). The predictor variables are often (but not always) under the control of the researcher, not the participant. Finally, predictor variables are equivalent (in linear regression) to any X in the equation above (yes, there could, and usually are, several Xs). 10.3.4.3 Notes on the terminology You can probably think of (and you will inevitably run in to) examples where the outcome variable is not really under the “control” of the participants, or is not really “provided” by them. This is entirely understandable because there is, in fact, no statistical distinction between outcome and predictor variables (see below). These labels come from research design, not statistics per se. In fact, statistics are somewhat blind to the distinction. Rather, the researcher needs to determine whether a variable is an outcome or a predictor. There is no statistical procedure to figure this out. Also note that the experimental tradition (as opposed to the correlational tradition that we are using) uses the term dependent variable for outcome, as well as independent variable for predictor. The relatively opaque terms from the experimental traditional are ultimately somewhat more confusing terms to use. Navarro and Foxcroft (Navarro &amp; Foxcroft, 2019) use outcome and predictor rather than dependent and independent. Since this lab manual is based on Navarro &amp; Foxcroft (2019), we will follow their preference for the terms outcome and predictor. But (ironically) you will notice that jamovi uses the terms dependent and independent variable. In the end, it is a simple substitution: outcome for dependent and predictor for independent. 10.4 Designing the survey This section is less about the theory underlying measurement, constructs, operationalizations, and variables, and more about how to carry this out in this course (in groups of 3-4 people). In the most general sense, you will be doing the following in this course: Designing a survey Collecting data Analyzing the data Writing an IMRaD-style research paper (IMRaD: Introduction, Method, Results, and Discussion; see Chapter 12) Broken down more into parts, in order: Via an online survey, providing your interests in psychology to your TA (allowing them to try to put you in like-minded research groups) Completing a Topic Brainstorm (another online survey) that will give you a small amount of practice in designing your own study Getting into your groups and deciding on a topic for your study Designing a research project around that topic, using a survey using Google Forms to collect the data Writing a rough and final draft of the Introduction to the IMRaD paper (Writing Assignment #1) Collecting the data Downloading the data into jamovi Preparing the data for analysis, and carrying out descriptive statistics Writing a rough and final draft of the Method section of the IMRaD paper Completing a t-test and an ANOVA from the data you have collected Writing a rough and final draft of Study 1, which will include a Results and Discussion section encompassing both the t-test and the ANOVA Completing a correlation and simple linear regression from the data you have collected Writing a rough and final draft of Study 2 (the Results and Discussion concerning your correlation and simple regression) and a General Discussion (the final summary of your IMRaD paper) To get you prepared for all of this, we discuss below the variable types that must be included in your study. 10.4.1 Variables in your study There will be a total of nine variables in your study: three outcome variables and six predictor variables. These are described in more detail below. 10.4.1.1 The three outcome variables There will be three continuous outcome variables that, together, will eventually form one composite variable. The italicized terms may be unfamiliar to you. We will explain them below, and then come back to this particular requirement of your study. A continuous variable is one whose numerical increments (e.g., whole numbers) at the very least represent equal intervals along some continuum of of measurement. For example, Age in years (or months, or days, or milliseconds) is a continuous variable. This is because the distance between 3-4 years is the same distance between 80-81 years (leaving some uncertainty within the year aside). Another example is Reaction Time in milliseconds (or seconds). The difference between 900ms and 800ms is the same as the difference between 550ms and 450ms. The continuous variables that you will be required to use in this class will be some form of Likert items, which are popular on survey questions. We explain the nature of the Likert scale below. 10.4.1.1.1 The Likert scale The Likert scale consists of a statement that the participant reads, along with some response options. Respondents typically indicate their agreement with the statement on a scale, originally numbered 1 to 5 by Likert himself. However, the scale’s range can be from 1 to any other positive odd number. The vast majority are 1-5, 1-7, or 1-9. Anything less or more presents significant problems. Response 1 2 3 4 5 Meaning Strongly Disagree Disagree Neither Agree nor Disagree Agree Strongly Agree One end of the scale typically represents strongly agree and the other: strongly disagree.82 Whether strongly agree (for example) is represented by the lowest or the highest integer is arbitrary, but it should be consistent. Above, the scale has disagreement on the left. Below, it is on the right: Response 1 2 3 4 5 Meaning Strongly Agree Agree Neither Agree nor Disagree Disagree Strongly Disagree The wording of the statement, however, may reverse the expected response from 1 to 5 or 2 to 4, or vice-versa, on any given scale. For example, a shy person might respond 5 to the first statement below (using the scale directly above), but 1 to the second: I generally enjoy passing time with strangers I generally dislike passing time with strangers The Likert scale must be both symmetric and balanced. By symmetric, we mean that the scale must have an equal number of response possibilities on either side of the middle, neutral value. Likewise, it must be balanced, which is another way of saying that the intervals between each response must appear equal in magnitude to the respondent. This allows researchers (albeit somewhat controversially) to treat the responses to Likert items as if they were interval data, and not just ordinal. You will find reasonable disagreement among researchers as to whether the numbers 1-5 on this scale truly form a continuous scale, or merely an ordinal scale (where, although there is order to the numbers, they are not necessarily equally spaced). But for the purposes of this class, they are continuous. This is not an unreasonable assumption. After all, Likert items are deliberately designed (i.e., worded) to appear as equal intervals to the participants. But note if you choose to remove the middle value (the neutral option), which you should never do in this class,83 the scale is probably tipped towards ordinal rather than interval in nature. In Google Forms, the Likert scale is implemented through the Linear scale option, one of the question-type options available on the right-hand side of any question-development box. See Figure 10.1 below. The question measures agreement with the statement Aggies are nice. Figure 10.1: The Likert scale is implemented through the Linear scale option in Google Forms The Google-Form template (named GoogleFormTemplate) that you have ready in your group’s Google folder (when it is assigned to you) for the course already contains three outcome variables scaled from 1-7. You can alter them to the specifics of your own survey. Note that if you change a scale to an even range (e.g., 1 to 4), you will not have a Likert scale. It will be an interval scale, but not a Likert scale. So please do not do this. Also note that using any other question type (e.g., Multiple choice, Checkboxes) to implement a Likert scale will result in a considerable amount of extra work for you, as you will then have to convert text to numbers in jamovi (an unnecessary task if you use the Linear scale option). Don’t make this mistake. 10.4.1.1.2 Other continuous scales You are free to use other continuous scales that are not Likert scales in your survey, but only as predictor variables. We will cover this below under section 10.4.1.2.3. 10.4.1.1.3 Final note on the outcome variables Do not mix scale ranges, where one question, say, would be on a Likert scale from 1 to 5, and another question wold be a scale from 1 to 7. This introduces unnecessary difficulties into your analyses. Specifically, you would be forced into transforming all your scores into z-scores before averaging or summing them, which is the topic of the next section. 10.4.1.1.4 The composite variable Before we move on to the six predictor variables, there is one more concept you need to know. Ultimately, each research group will average their three outcome variables into a single composite variable. A composite variable is a variable composed of other variables. The construct underlying the composite is typically a real-world phenomenon that, by itself is nearly impossible to measure reasonably with a single question. For example: How happy are you? Instead, you measure it with multiple questions that address different aspects of the complex construct. At this point however, your three outcome variables (those three questions) kind of lose their “status” as outcome variables, per se, and become indicator variables. This is because they’re no longer being analyzed as outcome variables. Instead, the new composite variable is being analyzed as the outcome variable. Let’s create a more reasonable example from the happiness discussion: student satisfaction. You could ask the following: How satisfied are you as a student? However, it’d be a pretty good guess that students would ask you in return: What exactly do you mean by that? To measure it more precisely, you might ask three more specific questions like the following: How satisfied are you with your social life at the university? How satisfied are you with your life volunteering at the university? How satisfied are you with academic life at the university? It is easy to see that the results of the three responses can be averaged or added together to obtain a more holistic, abstract measure of student satisfaction. And students would be much less likely to be puzzled by the questions. 10.4.1.1.5 Additional requirement: reverse-scoring We have one additional requirement for these three outcome variables. Specifically, one of the questions must be reverse-scored. We discussed this above in section 10.4.1.1.1. Basically, for any particular participant, one of the questions, relative to the other questions, must elicit an expected response that lies on the other end of the numerical scale. Using the examples from above, we will re-phrase them as Likert items, and then reverse-score the third: I am satisfied with social life at the university I am satisfied with my life volunteering at the university I am dissatisfied with my academic life at the university Note that if they use the scale directly below, then someone who is generally very satisfied in all aspect of their student life would respond with 5s on the first two question, but with a 1 on the last. Response 1 2 3 4 5 Meaning Strongly Disagree Disagree Neither Agree nor Disagree Agree Strongly Agree Subsequently, before averaging these items, you will then need to reverse-score this item to make it conform to the others. This may seem like busywork, but the purpose of it is twofold: It keeps the participants from getting bored with the same types of questions over and over again. This is more important on longer surveys, but we practice it here. It captures respondents who may be responding “robotically” (e.g., just indicating the same number over and over again to get through the survey, without actually reading the questions). In many cases, researchers can use such inconsistent responses to filter out disingenuous observations. Again, this is more important for longer surveys, but we practice it here. But we now need to turn to the six predictor variables. 10.4.1.2 The six predictor variables Overall, of the remaining six variables, three are continuous, and three are nominal (the categories continuous and nominal are explained below). More specifically, of the six remaining, the predictor variables carry the following characteristics: two will be given to you (each project must include these two specific variables) two will be nominal two will be continuous 10.4.1.2.1 Required predictor variables: Sex and Age Two of your nine variables are already chosen for you: Sex and Age. These two variables are almost always collected in psychology, if nothing else only to describe general characteristics of the sample of participants. Sex will have two levels: male and female; and Age will be a natural number (a positive number with no decimals).84 That leaves four other predictor variables. 10.4.1.2.2 Two more nominal predictors For the research project in this class you will need two more nominal variables. The first will be a two-level nominal predictor so that you can perform an independent-samples t-test (section 6.1). The other will also be a nominal predictor, but with 3-5 levels. This latter variable will allow you to perform a oneway ANOVA (section 8.1). Both of these analyses will be necessary in order for you to write the Results section (explained Chapter 15) for Assignment #3. 10.4.1.2.2.1 A two-level nominal predictor As just mentioned, one of your nominal predictors (the one you need for an independent-samples t-test) will have two levels (e.g., Class Status [Upperclassman vs. Lowerclassman]85) Other examples are Residence (on-campus vs. off-campus) and perhaps Extroversion (extrovert vs. introvert), though perhaps a Likert scale would be better here, under the continuous predictors covered below. An important issue to consider is your chance of getting sufficient respondents to respond with each level. That is, ideally, you want to end up with the same number of people indicating each level. Exact equality across levels is unlikely, and it is certainly not the end of the world statistically (it happens much more often than not in surveys). However, the more you can collect equal numbers across both groups, the better off you are. With a poorly thought-out variable design, you may get very few or, worst-case-scenario, no people at all to respond to one of your levels. Using a bit of an extreme example, this might happen with a falsely dichotomous variable like Love of Country (Supporter of US vs. Supporter of evil foreign country). Your first category would fill up, and you would end up with probably no one responding with the second level. You are going to need this level to perform a t-test, and if you have zero observations associated with one level of a two-level predictor, we would all need to figure something out, because you would not be able to perform that t-test. With few observations (a severely unbalanced design), it might be problematic, at least insofar as obtaining significance is concerned. Another mistake that is easy to accidentally incorporate into your survey is a question that offers incomplete levels as responses, thereby making it difficult for some of your respondents to finish the survey. This might, in turn, lead to some respondents to fail to finish the survey (bad), or provide a false response (worse). Consider the following question: Are you left- or right-handed? Most people are indeed either right or left-handed, but some are not clearly dominant on either side, and still others are ambidextrous. If you pitch a question this way with only left- or right-handedness as the options, you may miss some participants who are ambidextrous, or who can only respond if handedness were put on a Likert scale, or something. They may choose not to respond, in which case you have systematic non-response, which is bad (you do not know who is not responding, but their failure to respond is not random). Worse, they may even arbitrarily choose right- or left-handedness when they are neither. This is worse since your data now contain dishonest, and therefore, inaccurate, misleading responses. In fact, both may occur since not responding and responding with false information would vary from person to person. You would probably have both problems on your hands. The above example is that of a question with low validity. It is not really addressing handedness like it should. It is excluding information that it should include. This is known as measure deficiency.86 10.4.1.2.2.2 A 3-5-level nominal predictor As noted above, this variable will allow you to perform a oneway ANOVA (section 8.1). This variable is no different than the two-level predictor, except that there must be 3-5 levels. Typical examples here include Class Level (freshman, sophomore, junior, senior) or University Status (faculty, staff, student). We limit the number of levels to five since, although it is possible to execute oneway ANOVAs with more levels than five, it becomes increasingly unlikely that you will gather enough participants per level, the more levels you have. In other words, you seriously risk having an unbalanced design, the more levels you have. This problem is common since some nominal variables naturally have many levels and/or levels, but the proportional representation of each level in the population is very uneven. An example here is Race, as it is used by the US Census Bureau. The minimum levels of race that the Census Bureau must use are as follows: White Black or African American American Indian or Alaska Native Asian Native Hawaiian or Other Pacific Islander But if you were to choose Race as one of your variables here at Texas A&amp;M in College Station, TX, it is likely that you would only end up with responses for three of the 5 levels. This is because there are relatively few self-identified Native Americans or Pacific Islanders at Texas A&amp;M. There are some, to be sure, but you are only collecting between 30-100 observations. You’d be rather lucky to get either of those categories in sufficient numbers. For the purposes of this course, losing two of your five levels is not catastrophic. We will be requiring you to do a oneway ANOVA, which will require that you have at least three levels. However, if you start off with a nominal variable that has only three levels (or worse, two levels), and one of them occurs relatively rarely in the population, then you could have a problem. For instance, if you ask respondents what political party they affiliate with (do not do this), and your options are Republican, Democrat, and Other, you may find that either no one chooses the last category, or very few do. The first problem would be worse since you would not normally carry out an ANOVA with only two levels.87. You would be left unable to carry out that part of the requirement of the class. In the latter case (few respondents) your statistical tests lose power. In sum, you should design the variable so that you think there’s a good chance that participants will respond in roughly equal numbers to each of the 3-5 levels. Exactly equal numbers is too high a bar (unless you are doing laboratory experiments), but roughly equal is a good target for surveys. There are several ways to implement a nominal variable in Google Forms. The most straightforward way to do this is with the Multiple choice option. An example is provided below in Figure 10.2 Figure 10.2: How to implement a nominal variable in Google Forms with the Multiple choice option. 10.4.1.2.3 Two more continuous variables The last two variables that you will need are two more continuous variables. You will need these to carry out an analysis using Pearson’s r correlation coefficient (section 7.1) and a simple linear regression (section 7.2). Both of these analyses will be necessary in order for you to write the Results section (explained Chapter 15) for Assignment #4. Unlike the outcome variables (which must be Likert items), these two variables can be of any variety, as long as they qualify as continuous. That is, they can very well be Likert items (and many of you will do exactly this). However, they could also be items with response scales that are different from Likert scales. You could just have participants input a number like they do for Age (see section 10.4.1.2.1 below). This would be carried out through the Short answer question-type option in Google Forms, where you also included response validation by clicking the \\(\\vdots\\) symbol at the lower-right and checking Response validation. From there, you can choose from several parameters to “force” your participant to type in the type of number you need (e.g., decimal, integer, etc.). See Figure 10.3 below: Figure 10.3: To have the respondent provide a number, choose Short answer along with Response validation and the appropriate parameters. You could also use the Linear scale question-type option to create a different kind of scale than that of a Likert scale. For instance, you could ask a question like the following: If you had to choose, which of the following grades would you say represents your typical performance in college? A B C D F One oddity about this scale, even though it is an interval scale for the most part, is that it skips a letter (E). This means that the Linear scale option in Google Forms becomes problematic. You could ask participants to type in a number, and try to figure out how to restrict their choices through Google Form’s response-validation option. But it might be just as easy to use checkboxes, where you restrict the participant to choosing one response. You can convert the letter grades (A-D, and F) into a continuous 1-5 (or 5-1) scale later on, after data collection has ended. Figure 10.4 below is an example of how to do this in Google Forms. Figure 10.4: To ask for anything about letter grades, you can use the Checkboxes option in Google Forms, and restrict participants to checking one and only one box (Select at most 1) through Response validation. More straightforwardly, you could ask respondents about the amount of sleep they get per night on average, or the amount that they think they need on average. For this type of question, you would use the same Short answer type shown above in Figure 10.3, but perhaps with some restrictions that would prevent outrageous responses, like restricting the hours per night to somewhere between 1 and 24. This is shown below in Figure 10.5. You can also provide a message that the respondent will see if they type in a response that doesn’t conform to the parameters you specified (e.g., “Please provide a number in digits (e.g., 8.5) between 1 and 24.”). Figure 10.5: How to ask for a number between 1-24 using Response validation under the Short answer question type. Note: The message at the lower right of the image gets cut off, but it says, Please provide a number in digits (e.g., 8.5) between 1 and 24, which is the message that the respondents will see if they type in a number outside that range, or a number as text (e.g., eight). 10.4.2 More information on survey questions There are several suggestions and warnings we have for you on formulating your survey questions. We will begin with the suggestions, and follow with warnings. 10.4.2.1 Suggestions for survey questions 10.4.2.1.1 Settings There are some options in Google Forms that you should include under Settings. To get there, click on the gear icon to the left of the Send box (and make sure you are under the General tab). The required settings are as follows: Restrict to Texas A&amp;M users Limit to 1 response This prevents people from responding twice Edit after submit This allows respondents to change their answers after the fact if they feel they aren’t comfortable with the answers they gave. This is just a courtesy. They can only do this up until the moment you click the box labeled Accepting responses under the Responses tab (which only appears after you’ve collected data). This will change the screen text to say, Not accepting responses. This action will lock all the responses for the indefinite future, and prevent any further responses. So do not do it until you are certain you are finished collecting your data. Do not check any other boxes under Settings. 10.4.2.1.2 Opt-out question You should have a question in your survey at the very beginning that allows the respondent to opt out of the survey. This comes after they have had a chance to read what the survey is about. This is common courtesy, and conforms to the general principle in research ethics that research participation is voluntary, and participants always have the right to stop participating at any time. This question is already in the Google Form template provided to your research group in your group’s Google Folder. We suggest not changing this question at all since it branches. That is, it exits the survey if they indicate they do not want to continue, but proceeds to the first question if they indicate that they do want to continue. The next suggestion is also related to the same research-ethics principle noted above. 10.4.2.1.3 Do-not-analyze-my-data question You should have a question at the end of the survey that allows the respondent to indicate whether they want their data analyzed. It is too difficult to remove data via an email request once it is there (imagine if you had 10,000 anonymized participants), but it is superficially easy to tag certain observations as not to be analyzed. This is what the final question of the survey should be. This way, before you analyze your data, you can filter out any responses with this do not analyze tag. There is already such a question on the Google Form template provided to you in your group’s folder. You do not need to change anything here. 10.4.2.2 Warnings about survey questions There are several things you should not do when designing survey questions. We cover a few of them here. 10.4.2.2.1 Avoiding the unnecessary-dichotomy temptation If you look at any of your nominal variables, and you notice that the levels are actually ranges of numbers, then switch it back to numbers. Do not unnecessarily dichtomize continuous variables. You lose information this way. For instance, with Age, many novice researchers make the mistake of setting up their variable in a way similar to the following: Please indicate your age. 18-25 years old 26-35 years old 36-45 years old 46 or older The problem is that the researcher could just as easily have asked the following question instead: Please indicate your age by typing it in to the box below. Please use a whole number (e.g., 24) without decimal points. Part of the confusion here probably stems from the fact that data that is collected in its continuous form (i.e., a number, not a category) often gets reported (e.g., on the nightly news) as categories (e.g., 18-25 year-olds). This is the result of a data summary, however, not data collection. That is, you can summarize data almost any way you want if you have collected it as continuous data. The the reverse is not true: You cannot convert data that is collected as categories into continuous data. It does not work in reverse; it’s a one-way street. All that said, one should also be careful not to get too precise with personally identifying data like Age. For instance, one could collect birth dates instead of ages (as whole numbers). But this would be overkill for adults since no psychological phenomenon could ever be expected to vary between, say, two adults who differ in age by three weeks.88 Much worse, birth dates, combined with some other personally identifying information, such as Sex and, say, Academic Major, might just be enough to narrow the information sufficiently to identify the exact individual who provided the data in the first place. This is a big NO-NO in academic research. It is actually a violation of FERPA (a federal law designed to protect student anonymity) unless steps are taken before any data collection to justify the need for such data. 10.4.2.2.2 Do not turn a Likert item into a multiple-choice item This was already noted above at the end of section 10.4.1.1.1. But it bears repeating here. In Google Forms, it is easy to change the question type from Linear scale to Multiple choice or Checkboxes. But do not do this as it will just give you an extra headache. Instead of getting numbers back (as with the Linear scale), you will get text back, and you would need to convert all that text into numbers for analysis. It can be done, but it is not fun. 10.4.2.2.3 Do not create questions that are difficult to respond to An example of this problem was given at the end of section 10.4.1.2.2.1 (the handedness issue). Survey design requires extensive forethought. One of the reasons is that it is not always apparent to the researcher how respondents that have a different perspective from the researcher might perceive the question. Another is that a simple question that you might ask in conversation may not work well at all on a survey that is collecting quantitative data. Let’s take an example that you might ask in person: How satisfied are you at this university? This question is fine face-to-face because your interlocutor can break down the question into parts for you: Well, I’m happy with my classes, but some of the teachers are… And I’ve had some problems with… The problem is that you cannot analyze such responses with the statistics we are learning in this class.89 For this kind of question, you would need to break this complex construct of student satisfaction into various parts that are answerable on their own. We covered this in section 10.4.1.1.4 above, but it bears repeating here: How satisfied are you with your social life at the university? How satisfied are you with your life volunteering at the university? How satisfied are you with academic life at the university? Another example of a difficult-to-answer question would be one that is uncomfortable for some people. Here is an example of one: What is your sexual orientation? Naturally, some people might be hesitant to provide you with this information since their could be social repercussions if the information were mis-handled. That was an extreme example, one that you should not even consider at all. But here is one that is more seemingly innocuous: What is your US residence status? A. US Citizen B. US Permanent Resident C. Visa (student, visitor, etc.) D. Other Clearly, Other in this case is very suggestive of undocumented status. Some of your respondents might be hesitant to reply. Others might see this, and then protest such a question by refusing to continue with the survey. Admittedly, not even that one was very innocuous. But here is one: Have you ever been in a car accident? A. Yes B. No It could be that the answer is Yes, but that your respondent was responsible for the accident and had to serve time, pay a large settlement, etc. They may not want to revisit this. Alternatively, they may have lost a loved one in the accident. This last question probably really is okay, but we used it to illustrate the range of possibilities that you need to think about when it comes to formulating questions. In short, you do not want to ask questions that might elicit emotional triggers for some. In addition to being somewhat insensitive, such questions can distort your data through systematic non-responses. 10.4.2.2.4 Do not allow for free responses You just cannot do this in this class. Such data cannot be analyzed without sophisticated linguistic tools like sentiment analysis. The way you make this mistake is by choosing the Short answer (or even worse, Paragraph) option for your question type, and then failing to constrain the response types with Response validation (\\(\\vdots\\) &gt; Response Validation). Such questions might be okay occasionally for regular surveys, but they are utterly useless in this course. They’d be a waste of both your time and (more importantly) your respondent’s time. Don’t do this, please. 10.4.2.2.5 Don not make the respondent work too hard This mistake is easy to make. It is best illustrated by example: How many hours do you study per week? For this question, the respondent needs to calculate across seven days. This might not be so bad if the person does the same thing every day. But some people work, some do athletics, etc. Their schedule may be uneven. The solution here is to simplify down to the day. Most people can come up with an average per day, even those with uneven schedules. 10.4.2.2.6 Do not ask overlapping questions To maximize the amount of information that you gather, you should try as hard as possible to make sure that your questions do not ask for redundant information. In fact, redundant information, if contradictory, can put you in a data-analysis quandary. Let’s say that you have the following questions: I am satisfied with my college GPA I am dissatisfied with my social life in college I am satisfied with my current health I am satisfied overall with my college experience The problem is that the last of these encompasses aspects of the previous three questions. Therefore, it seems to be a waste of a question. Furthermore, doing this opens your data up to contradictions. For instance, what if the respondent had indicated on the first three variables that they were relatively dissatisfied, but indicated more satisfaction on the last question? This could be construed as a contradiction. Here, you need to make a decision: Keep the first three questions, and drop the last? Keep the last question, but drop the first three? Delete the observation? None of these is desirable for various reasons. In short, try to keep all of your questions discrete from each other. 10.4.3 Summary of Survey Design assignment You must create a survey using Google Forms with nine variables in it. There will be more questions than that, but nine of the questions will be variables with very specific requirements. They are as follows: Three outcome variables These must be Likert items on a scale of 1-5 or 1-7 They must be three components of a single construct You must ultimately average (or sum) them into a single, composite variable One of them must be reverse-scored You could add a fourth outcome variable, if you so chose, but you cannot have fewer than three Six predictor variables Two required predictors (biological) Sex (Female vs. Male) Age (in whole-number years) Two nominal (categorical) predictors A predictor with exactly two levels A predictor with 3-5 levels Two continuous predictors These can be any type of continuous predictor (e.g., Likert, count, decimal, integer) But they may not be on an ordinal scale 10.4.4 Outside help on surveys Here are a couple of links that might be useful in formulating questions. They go into more detail than we do above. How to conduct surveys Designing surveys 10.5 Collecting the Data After you have designed your survey, and critically, your TA has approved it, you can send it out. There are several ways to do this, and they all start with the Send button at the upper right in Google Forms. There are several options for sending out your survey. You can see these as tabs running across the top of the Send form pop-up screen. The one on the left is by email (an envelope icon); the one in the middle is by hyperlink (a sideways paper clip icon); and the on on the right is embedded html (double angled brackets). The most useful of these is the hyperlink, the one in the middle. Copy that link and send it out to whatever contacts at Texas A&amp;M conform to the population that you are interested in. In any communication that you send out to elicit responses, you should indicate that this is strictly for a class project, and the data will not be used for any publication. It is usually a good practice to check the Shorten URL box to get a smaller URL. Long links look intimidating and old school. Once your survey is out, you can just wait. The responses should come in. Ideally, you are hoping for at least 30 responses, but do not exceed 100. If you are near 100, then do what is explained in the next paragraph, but sooner than the due date for the data. When the data is due, you will want to go back to the survey and look across the top for the tab labeled Responses. Click it and then look for the button that says Accepting responses (it is located right under the green flag icon). Once you click it, all responses are frozen. No more can come in, and respondents can no longer edit their old responses. The next step is Data Preparation and analyses, which is covered in the next chapter, Chapter 11. There are other definitions, naturally.↩︎ This is named after Rensis Likert (1903-1981), an American social psychologist at the University of Michigan. Perhaps the strangest thing about the history of the Likert scale is its pronunciation. Likert himself pronounced his name as rhyming with wicker or sicker (but with a at the end, naturally. However, due to the spelling of the name, most people pronounce it as rhyming with hiker or striker.↩︎ Many of these are actually portmanteaus (combinations of words that are already used, like smog (smoke + fog) or brunch (breakfast + lunch). Humans like playing with their language in this way, for whatever reason.↩︎ Again, Likert items are understood by most everyone, but they are explained more formally in section 10.4.1.1↩︎ We recognize that there are grey areas here, biologically↩︎ Incidentally, this class-level system is very US-centric. For instance, Canadians call these simply first-years, second-years, third-years, and fourth-years, respectively.↩︎ Agreement is the most common scale, but others are possible like Support (Do not support at all … Support very much), or Attraction (Very attracted to… Not attracted to at all), etc.↩︎ It is possible to analyze such data (ordinal data), but we do not cover these methods in this class↩︎ Unless you are working with small children, it is usually a bad idea to record ages more exact than a natural number with no decimals. The reason has to do with collecting information that could identify participants. The more precise the age (e.g., years &amp; months, or exact birth dates), the less anonymous the data. Combined with other data you collect from the participant, you could end up uniquely identifying them. Data collection that leans in this direction is very highly discouraged because of privacy concerns. Good researchers try to anonymize their data to the maximum extent possible, given the nature of the research.↩︎ Substituting man with person in this case makes it sound like socioeconomic status (i.e., Upper-class person), so we will stick with the more sexist terms, until we can think of better ones.↩︎ The opposite (i.e., including information that should not be included) is known as measure contamination.↩︎ Actually, you can, but for the purposes of this class, this would be problematic since we will also want you to carry out post-hoc analyses if your omnibus results are significant.↩︎ Newborns, infants, and toddlers, however, are a different matter.↩︎ There are some methods to analyze such data, such as sentiment analysis, but this is well beyond the learning confines of this course.↩︎ "],["DataPreparationAndAnalyses.html", "Chapter 11 Data Preparation and Analyses 11.1 Chapter overview 11.2 Exporting data from Google Forms 11.3 Importing the data into jamovi 11.4 Cleaning up the data 11.5 Final steps before analysis", " Chapter 11 Data Preparation and Analyses 11.1 Chapter overview This section is dedicated to showing you how to get your data that you have collected from Google into jamovi, and then prepared in such a way that you can begin Writing Assignment #2 (in section 14.5). This section will proceed as follows: Exporting your data from Google Forms Importing your data into jamovi Cleaning up the data in jamovi so that only your pertinent variables remain Filtering out observations that should not be analyzed Reverse-scoring data any variables that need this transformation Calculating z-scores (if necessary) Calculating Cronbach’s Alpha and analyzing your outcome variables Creating a composite variable from your outcome variables 11.2 Exporting data from Google Forms This step is extremely easy. And it is really just a particular version of what we already covered in sections 2.3 and 2.8.3 earlier in the manual. When you are finished with data collection and you open your Google Form, you will see two tabs at the top: Questions and Responses (with a number next to it). Select Responses. See Figure 11.1 below. Figure 11.1: Example of tabs available upon opening your Google Form (assuming it has data; the number will vary depending on how much data you have collected). You will then see an option that says Accepting responses. click the button next to that. This will prevent further participants from adding data to your survey.90 Figure 11.2: How to halt all future responses in Google Forms From here, there is an option to download the data as a .csv file directly by clicking the three vertical dots. So click \\((\\vdots)\\) &gt; Download responses (.csv). This will place a compressed .zip folder on your computer. You will then need to unzip it. On a Mac, simply double-click it and it will unzip it in the same location. On Windows 10, right-click it and choose Extract All and choose an appropriate destination and click Extract at the bottom. Note down where the extracted directory is. That’s it. 11.3 Importing the data into jamovi Start jamovi and click \\((\\equiv)\\) &gt; Import. Navigate to the directory with the .csv file and double click the file. NOTE: jamovi seems to have no problem recognizing commas inside of comma delimiters in .csv files exported by Google, so you should not have the following problem. However, if there were problems importing the .csv file due to the commas being delimiters, go back to Google Forms and click the square green icon with the white cross inside it. This will create a Google Sheet. From there, you can click File &gt; Download &gt; Tab-separated values (.tsv, current sheet). This will create a tab-delimited file that jamovi should definitely not have a problem with. Save your imported data as a jamovi file with the following name: MainData_[YOUR.GROUP.ID].jmv (e.g., MainData_905C.jmv). This is the file that you will ultimately upload (as a group) to the Google Folder assigned to you, and refer to when you notify your instructor that you have done so (in eCampus, under Lab: BDDO &gt; Data). Only one person from the group needs to upload this. But first, there are some changes you need to make. 11.4 Cleaning up the data 11.4.1 Renaming variables First, Google Forms just exported the questions as your variable names. This is not convenient since they are very long, and will clutter up any statistical output, figure, or table you subsequently produce. NOTE: You do not need to do this for variables with short names, like Age or Sex. Double-click the header at the top of the relevant column in jamovi. This will open up the DATA VARIABLE window, where you can manipulate things like the levels of the variable, the name, the descriptions, etc. In this case, you are going to manipulate the variable name and description. First read the variable name at the top and think of a reasonable name for it. Don’t type over the variable name yet. Just think of a reasonable, shorter replacement and write it down. A reasonable, shorter replacement name is one that is transparent enough for figures and tables, but not so long that it creates space problems for the same elements (i.e., figures and tables). For instance, a variable with the original name of How satisfied are you with the amount of sleep you get each night could be changed to Nightly Sleep Satisfaction. Next, select the entire variable name. Then either click CTRL-X or go to Edit &gt; Cut. The long name should disappear from the top box. Now read the caution below: CAUTION: For the moment, leave that long label in your clipboard (memory). You will come back to it shortly. And do not use the copy function again until you are finished with the process below. The reason for this is that jamovi will not allow you to leave that top box blank, even just to paste what you just copied into the Description box (see below). You must provide a variable name right now or nothing else will work. How to do this is explained below. So the very next thing you must do is re-name the variable in the top box. Use the variable name you wrote down (see above). Now, place the mouse in the Description box (make sure the cursor is blinking inside there) and click CTRL-V, or Edit &gt; Paste. Now the long name of the variable is in the Description. The description in jamovi is mostly for researcher reference. It doesn’t appear anywhere in any analyses, tables, or figures. 11.4.2 Removing variables Next, remove the A, B, and C columns as demonstrated earlier in the manual in section 2.8.3 and Figure 2.28. In the same way, you will also want to delete the Timestamp variable that Google Forms creates that you do not need. Assuming you used the Google Form template, there was a question at the beginning asking participants whether they wanted to continue. If they chose yes, then their data were recorded. If they chose no, they were bumped out of the survey and thanked, but their data was not recorded. What this means is that for all or most of you, the next variable is useless [i.e., I would like to continue with this survey (by clicking “No,” you will exit the survey)]. If the survey worked right, the column should have only yes responses, and no no responses since only people who responded yes even have data there. So you can delete this variable too. However, this agreement to participate should not be confused with the variable near the end of the survey (to the far right in the spreadsheet), where participants indicated whether they wanted their data analyzed (i.e., Do you give permission for the data you just provided to be used and analyzed for research purposes?). This is different. This variable was collected after the data had undergone recording. It is an indication whether or not the participant is comfortable with you analyzing the data. If the response was no, then you need to filter out that response. This is the topic of section 11.5.1 below. But first, you may need to adjust some variables. 11.4.3 Adjusting variables When jamovi imports data from a delimited text file, sometimes it gets the variable type wrong. For instance, when we imported some data from a past project in this class, jamovi imported Age as a nominal variable. If this happened to you, just double-click on Age and click the Continuous box. That will take care of it. jamovi may have done this with your Likert data as well, as it did in our case. That data should be Continuous (Data type: Integer). Do that for all variables that were imported as nominal, but should be continuous. 11.5 Final steps before analysis 11.5.1 Filtering data Sometimes you need to filter data out for whatever reason. It could be that some of your observations have been identified as outliers. or it could be that the data was somehow corrupted, or you identified the responses as being somehow illegitimate. Presumably, none of those reasons will come up in your study. However, if the participant indicated that they did not want their data analyzed, then you very much need to filter that data out. This is an issue of informed consent. We covered how to do this earlier in section 2.5.2. You can refer back to that section, and filter out any such variables. But in short this is easy to do. The filter function is located under the Data tab. Once there, we just clicked the icon labeled Filters (at the upper-right). The icon looks like a funnel, half full of liquid. There’s a box there labeled ROW FILTERS. Assuming you changed the variable name from Do you give permission for the data you just provided to be used and analyzed for research purposes? to something like Consent, you can type in the following in the box under Filter 1, labeled \\(f_{x}\\): Consent == ‘Yes’ The double equals sign is the way that the underlying programming language, R, wants you to ask yes/no questions. Basically, Consent == ‘Yes’ translates to “Is the value in this row under the column ‘Consent’ the same as ‘Yes?’” Also note that if you have a space between a two-word variable name (e.g., Informed Consent), you will need to surround the whole variable name with single back-ticks. This is how R can deal with spaces within variables. You can see this below in Figure 11.3. On your keyboard, the backtick is located just below the tilde (~), which is in turn is located just below the Escape key in the upper-left corner. Figure 11.3: Surround any variable with spaces in it with backticks. 11.5.2 Reverse-scoring For your particular assignments, we have required that you collect three, related outcome variables that, together, form a single construct in the form of a composite variable. Thus, you will be adding or averaging these variables together. But you will need to take care of a few things before doing so. Some may not apply to you, but others will. Many or most of you have a reverse-scored item. That means that the response scale is reversed compared to the other items. For example, the responses to the statement “I am disappointed with my cumulative grade-point average” (1 - Strongly disagree &lt;—&gt; 5 - Strongly agree) would be reverse-scored relative to the responses to the statement “I am generally happy with my GPA this semester” (1 - Strongly disagree &lt;—&gt; 5 - Strongly agree). Someone who was happy with both would respond low to the first statement and high to the second statement. There is a simple formula for reverse scoring. If your variable is x, then the formula is \\(max(x)+1-x\\). That is, take the maximum possible value of x, add one to it, and subtract the value of x for that cell in the spreadsheet. In jamovi, you click the Data tab, and find a blank column on the right-hand side of the spreadsheet, and double-click the header. You will see several options, but you should click NEW COMPUTED VARIABLE. You should come up with a new name for the variable that relates to the original. Thus, if the original had been Aggression1, the new one could be Aggression1R (with an R to indicate reversed). We have done one in plain English here with spaces (thereby requiring backticks). Here we are reverse-scoring Nightly Sleep Satisfaction into Nightly Sleep Satisfaction (reversed). The scale was a 1-5 Likert scale, so we added 1 to 5, giving us 6. We then subtracted Nightly Sleep Satisfaction from 6. See Figure 11.4 below. Figure 11.4: Reverse scoring by subtracting a variable from its maximum value plus one (Note the use of backticks around the original variable, which had spaces in the name). In the figure, you can also compare the scores on the Nightly Sleep Satisfaction variable (at the lower left) with the new values for Nightly Sleep Satisfaction (reversed) variable (at the lower right). The pattern is as follows: \\(5\\to1\\) \\(4\\to2\\) \\(3\\to3\\) \\(2\\to4\\) \\(1\\to5\\) Here is a more detailed link that describes reverse-scoring and how to carry it out in jamovi. 11.5.3 Calculating z-scores Some of you may have these variables on different scales. Perhaps one of your questions is on a 1-to-5 Likert scale, but another is a count. If this is the case, then you will need to convert all the variables into z-scores before analyzing them. This was covered in a previous chapter in section 2.5.1.3.2, but in a very different context. We will provide a simple example here. We artificially created three variables for a data set of 23 observations. The first variable, Lk1to7 is contains random values between 1 and 7. The second variable Lk1to5 is the same, but only varies between 1 and 5. The third, CountUpTo40 varies between 1 and 40. Let’s say that it’s a count variable. These are shown in 11.5 below. Figure 11.5: An artificial data set with a variable that ranges between 1 and 7 (Lk1to7), one that varies between 1 and 5 (Lk1to5), and one that varies between 1 and 40 (CountUpTo40). To put these on the same scale, you need to create a new variable for each of the old ones, and apply a z-transformation, converting them into z-scores (standard-deviation units) around a new mean of zero. As you did for the reverse-scored items (section 11.5.2), double-click a blank header above any of the blank columns, and choose NEW COMPUTED VARIABLE. Type in a new variable name that represents the particular z-score, like Lk1to7_z. Click the little down arrow just to the right of the \\(f_x\\) symbol, and scroll down until you find Z and double click it. You’ll now see \\(=Z()\\) in the box to the right of \\(f_x\\). Inside those parentheses, type the name of the old variable, Lk1to7. Press and Voila! You have your new z-transformed variable. And again, for this set of scores, the mean is zero, and the scores represent standard-deviation units. We completed this for the other two variables as well. You can see the results below in Figure 11.6. Figure 11.6: The results of transforming those variables into z-scores (identified by having a _z extension). Though it is not as ideal as having all variables on the same scale, you can indeed average or sum these values together. It is also important to keep in mind that the scale has changed irredeemably. Your mean value will now be zero. You will need to be able to interpret things this way in the Results and subsequent Discussion sections. 11.5.4 Calculating Cronbach’s alpha Assuming you have taken care of any conversion of your scores to z-scores (if necessary), you will then need to carry out an analysis of how well your individual questions are working together. This is done through an analysis of Cronbach’s Alpha. If your analysis shows that you can use all three variables, then you use all three. Don’t be surprised, however, if one of your variables does not act like the others. In this case, you may want to consider analyzing that variable separately. To illustrate this, we will simply use a dataset called Personality Questionnaire: Ch15 - Personality data sample 3 (bfi_sample3) [located at the very bottom] from the datasets associated with Navarro &amp; Foxcroft (2019). In fact, this is the dataset they use to illustrate the same (pp. 461-463). Simply go to \\((\\equiv)\\) &gt; Open &gt; lsj-data &gt; Personality Questionnaire: Ch15 - Personality data sample 3. This dataset is a sample of people who took a Big-5 Personality test. The variables A1-A5 represent Agreeableness, but A1 is reverse-scored. Luckily, we do not need to reverse-score this ourselves. jamovi’s procedure for calculating Cronbach’s Alpha contains a mechanism within it to deal with reverse-scored items. When you have the data, click the Analyses tab, and choose Factor.91 From here select Reliability Analysis. Move variables A1-A5 to the Items box. Then look down below and click the window labeled Reverse Scaled Items. In that box (labeled Normal Scaled Items), select A1 and move it over to the box labeled Reverse Scaled Items. The arrangement of variables should look as they do in Figure 11.7 below. Figure 11.7: The arrangement of variables for Cronbach’s Alpha using the bfi_sample3 dataset from Navarro &amp; Foxcroft (2019) The result of the analysis should looks as follows: ## ## RELIABILITY ANALYSIS ## ## Scale Reliability Statistics ## ──────────────────────────── ## Cronbach&#39;s α ## ──────────────────────────── ## scale 0.7010436 ## ──────────────────────────── ## ## ## Item Reliability Statistics ## ─────────────────────────── ## Cronbach&#39;s α ## ─────────────────────────── ## A1  0.7220262 ## A2 0.6185412 ## A3 0.5878971 ## A4 0.6760271 ## A5 0.6448210 ## ─────────────────────────── ##  reverse scaled item Normally what you are looking for is a relatively high Cronbach’s Alpha, and any items that result in a significant increase in Cronbach’s Alpha if it is removed. In this case, the removal of A1 would result in a higher Cronbach’s Alpha than the removal of any other item, but the increase is not really severe enough to warrant removal from analysis. Had there been an item that was odd compared to the others, then you might have considered removing it and analyzing it separately. The final stage is next. 11.5.5 Creating a composite variable To create a new variable we simply sum or average them together. We can work with the same bfi_sample3 data from above to do this. We will need to reverse-score A1 first (see section 11.5.2 above. Using the same procedure as we did above for creating reversed scores (section 11.5.2) and creating z-scores (section 11.5.3), we again clicked the header above a blank column and chose NEW COMPUTED VARIABLE. Here, we chose to sum together A1R (reversed) and A2-A5.92 We did this by choosing the function SUM from the \\(f_x\\) menu. Then, inside the parentheses of SUM(), we placed all the variables we needed, separated by commas. See Figure 11.8 below for a visualization of what we did. Figure 11.8: Calculating the sum of A2-A5 and A1R (reversed) And that is it. Once you reach this stage, you have your composite variable and you are ready to upload the jamovi file to your group’s Google Folder and let your TA know (through the assignment in eCampus). This step isn’t actually required, but it can reduce confusion down the line, as one of your group-mates may decide to download more current data without you knowing, thereby giving you two or more datasets.↩︎ Cronbach’s Alpha is often carried out in conjunction with something called Factor Analysis, which you are free to read about in Chapter 15 of Navarro &amp; Foxcroft (2019)↩︎ We could also average them since they’re on the same scale. The result is the same.↩︎ "],["ReportingResearch.html", "Chapter 12 Reporting research 12.1 The broader context 12.2 Purpose of writing in this class 12.3 The genre of science 12.4 Style guides 12.5 Issues not extensively covered 12.6 Outside help on writing up research", " Chapter 12 Reporting research You already understand that writing is a major component of this lab (as well as the lab in PSYC 302 - Research Methods and Design in Psychology). This chapter provides some context and a general framework for doing well in this writing component of the lab. There are no practical exercises in this chapter, only information about how to go about writing in Psychology. Subsequent sections (Chapters 13-17) provide more detail. 12.1 The broader context There is a lot of writing that takes place in academia. Psychology is no exception. This section provides a general picture of what is happening at the university, particularly Psychology. This lab in PSYC 302 introduces you to what can be conceptualized as two interlinking circles of research and education that you are already part of by virtue of having initiated the pursuit of an undergraduate degree in Psychology. One of these circles, education, is very (dare we say, painfully) clear to undergraduate students because the students form an integral part of this circle. But the complementary circle, research, often remains hidden to undergraduates, at least most of the time. However, the research circle occupies the majority of time that professors and graduate students spend in their jobs. This is especially true at at large, research-intensive universities like Texas A&amp;M.93 We will describe these circles below, starting with the one you are most familiar with: the education circle. Note that the circle described below is characterized by a particular “flavor” that is quite unique to Psychology. It would be unusual to find this kind of undergraduate integration in other academic disciplines. 12.1.1 The education circle The education circle (simplified a bit) is what undergraduates see mostly in their classes. It goes like this. First, undergraduate students obtain textbooks for the class, and during the semester, they read the textbook and gain knowledge through that as well as lectures and activities. Second, (at least in most major Psychology programs in the US), these same students participate in laboratory research for course credit. You probably have experienced being one of these participants. Third, researchers outside the classroom (i.e., professors and graduate students in the department) take that data gained from the experiment participation, analyze it, and try to publish it, ideally in prestigious peer-reviewed journals, but sometimes in other venues. This written output in academic journals forms the backbone of what is called primary research. Fourth, another group of writers sift through the primary research, and summarize it for textbooks in such a way that it is readable for most undergraduates and much of the lay public. Those textbooks get published, and we’re back to the first step above, where students obtain textbooks. This is the education circle in a nutshell. It is important to note that the rhetoric used to communicate within this education circle is primarily aimed towards the lay public. Although the language is typically formal, it contains very little technical, academic jargon. One probably needs a high-school education to understand it, but not much more. However, the language in the research circle (described below) is very different. Here is an example of such writing, from the a recently finished open-educational-resource textbook on developmental psychology (Paris, Ricardo, Rymond, &amp; Johnson, 2019): Piaget has been criticized for overemphasizing the role that physical maturation plays in cognitive development and in underestimating the role that culture and interaction (or experience) plays in cognitive development. Looking across cultures reveals considerable variation in what children are able to do at various ages. Piaget may have underestimated what children are capable of given the right circumstances. (p. 33) You have seen this kind of writing before if you have read any textbook before. We can assume that includes absolutely all of you. What you may not have seen before is the kind of writing described next. 12.1.2 The research circle The research cycle spins off of the third step above, where professors and graduate students perform research. As you might imagine, there are several steps here as well, which we will now simplify. First, researchers “begin” by staying (or becoming) up-to-date on a given research topic, and then figuring out which questions related to that topic have not yet been answered. Researchers will usually then do their creative best to envision a study to carry out in order to answer a gap identified in the extant research record.94 Second, researchers run the experiments they have envisioned (often preceded with the application for and [hopefully] successful procurement of grant funds to facilitate the research). In Psychology, the participants in these experiments are often, but not nearly always, undergraduate psychology students in basic Psychology courses.95 Third, after the collection of this data, the researchers analyze it, usually with statistics. Fourth, they write their results up in a formal paper and submit it to a journal for publication. At this stage, it may get accepted, rejected, or somewhere in between (e.g., “revise and resubmit”). If and when the paper is published, it feeds back in to the education circle where it initially departed.96 This completes the research circle. We haven’t yet had a chance to create a diagram of these circles. Instead, we will rely on your imagination for the time being. However, there is a related image from the Understanding Science website at the University of California Museum of Paleontology (2019). You can see this below, as Figure 12.1. Figure 12.1: The overall process of research (courtesy of the University of California Museum of Anthropology. The image is copyrighted. Please contact them at understandingscience@berkeley.edu if you wish to reproduce the image.). This is a much more complex visualization than the two-circle description we provided above. Indeed, it’s a more comprehensive depiction of the scientific process. Nonetheless, you may picture yourself as a student beneficiary, gaining knowledge in the blue circle, but also contributing data to experments in upper half of the central, green circle. Thus, it is in the interaction between the blue and green circles that Psychology students (you) find themselves in. But the main take-away in this sub-section is that the purpose of this class is to bring you closer to not only the green circle, but also the pinkish and violet circles. 12.1.3 The research genre However, the particular style of language used within the research circle is definitely NOT intended to be read by the lay public, even those who are fairly educated. Rather, it is quite full of not only jargon that the uninitiated will probably not understand, but also of quite specialized concepts that are not widely understood, like the inferential statistics that are the focus of this class. Here is an example excerpt from the Introduction to an article (Barker et al., 2014) in the journal Frontiers in Psychology, an open-access journal in the field. To accomplish [complex decision-making] tasks, children must engage executive functions (EFs), the cognitive control processes that regulate thought and action in support of goal-directed behavior. EFs develop dramatically during childhood …, and support a number of higher-level cognitive processes, including planning and decision-making, maintenance and manipulation of information in memory, inhibition of unwanted thoughts, feelings, and actions, and flexible shifting from one task to another. Researchers have used a variety of laboratory tasks to measure child EFs, including table-top behavioral tasks (e.g., the classic marshmallow test, card-sorting tasks) and computerized tasks (e.g., Go/No-go, Flanker), many of which tap multiple aspects of EF. Over the past decade, EFs have emerged as critical, early predictors of success across a range of important outcomes, including school readiness in preschoolers…, as well as academic performance at school entry… and beyond. (p. 1) And that’s just from the Introduction, the least jargon-laced section of a paper. Below is an example from the Method section of the same paper. It has more jargon in it. Children completed a computerized flanker task… assessing their ability to resolve conflicting visual information by appropriately responding to a central stimulus while ignoring flanking stimuli. The Flanker task is a commonly-used measure of externally-directed EF in 6-year-olds … and has been shown to be sensitive to some interventions targeting EF in this age group… During the task, children were instructed to indicate the orientation (left or right pointing) of a centrally-presented target stimulus, via a corresponding button press. (p. 6) And finally, the following is from the Results section, which is full of jargon: Weekly and annual/typical estimates of how children spent their time… were marginally correlated, for both structured activities (r = 0.24; p &lt; 0.06) and less-structured activities (r = 0.23; p &lt; 0.071). We thus generated composite scores across weekly and annual/typical estimates to provide a more accurate and reliable measure of children’s time. Each composite measure (for structured time, and separately for less-structured time) was formed by summing z-scored time in prior-week activities with z-scored ratings from the parent survey of annual/typical child activities, within each participant. (p. 7) Clearly, this is not intended for the layperson. Rather, it is intended to be read by other experts in the field. 12.2 Purpose of writing in this class If you are an undergraduate at Texas A&amp;M, you already know that all undergraduate majors at Texas A&amp;M must take two courses that relate either to written (two “W” courses) or to both written and oral communication (one “W” and one “C” course). The link to this requirement is here. The Department of Psychological and Brain Sciences has elected to have majors complete two “W” courses. These two courses are PSYC 301 (Elementary Statistics for Psychology) and PSYC 302 (Research Methods and Design in Psychology). The focus of PSYC 301 is on understanding statistics, whereas the focus of PSYC 302 is on research methods and experimental design (which in turn assumes an understanding of statistics). But both involve analyzing data and subsequently writing up research based on those analyses. The purpose of this lab (as well as that of PSYC 302 if you are at Texas A&amp;M) is to train you in the skills required to engage in some way with the outer, research circle described directly above in Section 12.1.2. This includes not only the statistics reported in the last quoted excerpt above, but also the style, jargon, and most importantly, the specialized concepts represented across all three excerpts. The advantages of such training are at least twofold. First, you will gain valuable knowledge into statistics, which is a life skill. You are bombarded every day with statistics in the news, online, etc. Statistics is the form of math (the math of uncertainty) that allows us to make important decisions based on limited information. All fields use it, from science to business to policy to engineering. Also, without this skill, it is easy for others to deceive you. Second, if you even believe that you might one day go to graduate school, you will need these skills in in great measure. As we mentioned before, it isn’t only professors who are involved heavily in the research circle; graduate students are there as well. In this particular lab (i.e., if you are using this particular lab manual), you will be writing a single paper that is broken up into four separate assignments. These separate assignments are roughly broken up into the chapters that follow this one (13-17), which in turn correspond to the traditional sections of of a research paper (described below in Section 12.4.1.1). But first, a general introduction to scientific writing is in order. 12.3 The genre of science The first thing you should understand about research reports in Psychology (and in science in general) is that they are very, very standardized. This is very characteristic of the genre. Creativity is reserved for the content of the report, not the form*. There is a reason for this. Namely, researchers need to read through dozens, or hundreds of research reports to gather the knowledge they need in order to do their job (i.e., more research). Thus, the more that the information is conveyed in a predictable format, the easier it is to gather the information. This contrasts a great deal with the composition background that many students arrive with when they get to the university, having gained that knowledge during their secondary (high-school) education. In those contexts, students are usually introduced to reading and writing through the genre of higher literature, where creativity in both form and content is emphasized. In this genre, some of the great writers flaunted conventions to great effect. William Faulkner did this with the narrative from Benjy Compson, the first chapter in The Sound and the Fury. It is difficult to read (at least at first) because the writing is from the perspective of a man who has mental retardation and little sense of chronological time. Another example is Molly’s monologue in James Joyce’s novel Ulysses. It is the last sentence of the novel. It is dozens of pages long, and contains no punctuation whatsoever. A more recent, popular example of this is the 2000 movie Memento, where the narrative runs chronologically backwards in ostensibly unrelated snippets, which is meant to reflect the experiences of a man (portrayed by the actor Guy Pierce) trying to solve the violent crime that led to his anterograde amnesia (the inability to form new memories). It is a disorienting movie, to say the least. All of these are considered high artistic achievements (especially the first two), but they are notoriously difficult to follow. Yet importantly, there is a benefit to getting through them, which is the sense of having stepped into the unique minds of these characters. But let’s go back to the topic at hand: research reporting. In direct contrast, it is quite difficult to imagine how flaunting the conventions of narrative would somehow be of any use whatsoever in research reporting. For instance, no researcher needs or even wants to step into the minds of other researchers or their participants. The goal of scientific reporting is strictly the conveyance of information. Thus, something very different is true of writing conventions among researchers: Creativity in content is paramount (i.e., finding ways to answer questions), whereas creativity in form (e.g., flaunting narrative conventions) is eschewed. In fact, you might see at this point how codifying the narrative conventions quite narrowly would lend quite an advantage to readers of research reports. If narrow enough, constraints on expression could actually help readers predict what kind of information will be conveyed, in what order, in what form, and with which specific labels. This is no different than, say, a manager at a private company that wants quarterly results in a certain format. She wants to interpret it right away instead of having to figure out how the information is being conveyed this time by some subordinate who chooses to doing things different every time. This would be a tremendous waste of her time. Indeed, this standardization has already been taken care of for you by the American Psychological Association (APA). This will be explained next. 12.4 Style guides So this brings us to what is called a style guide. A style guide is a set of conventions (usually written down in a manual) that a particular industry recommends writers follow for communication. With respect to research reporting, for instance, any particular style guide will recommend specific conventions for organizing information in papers, for citing sources, for wording, and for the reporting of statistics, among many other constraints. Examples of style guides include The Chicago Manual of Style (aka Turabian), MLA (the Modern Language Association), AMA (the American Medical Association), and CSE (the Council of Science Editors). There are many. 12.4.1 APA style But the style guide that has taken hold in Psychology (and many other fields) is that of the American Psychological Association. The style they recommend is called APA style, and is detailed in a manual called The Publication Manual of the American Psychological Association. To obtain a personal copy of the manual, it must be purchased. However, university libraries always have multiple copies. There are multiple copies available in the stacks at the Evans Library at Texas A&amp;M, but they are often checked out. There are at least two copies in the 1st Floor Annex of Evans, however. For what it’s worth, the map to this location is displayed below in Figure 12.2. Figure 12.2: Location of reserve copies at the Evans Library (1st Floor Annex) at Texas A&amp;M of the Publication Manual of the American Psychological Association, 6th ed.. Unfortunately, the APA does not appear to make the electronic version of the manual available to university libraries. Chances are, however, that you would seldom need to consult this manual in person. Many people get by with using some combination of online sources and reference-management software (e.g., EndNote, Mendeley, Zotero, BibDesk)97 to figure out how to conform to APA style in their research reports. Currently, EndNote is available for free to students at Texas A&amp;M - College Station, through the Texas A&amp;M Software Center. Unfortunately, EndNote does not work with Google Docs.98 It seems that a new reference manager does though. The parent organization is called F1000, and it has a tool called F1000Workspace that has an Add-on for Google Docs. That said, this class will focus very little on citing sources in APA format. You really won’t have much use for a reference manager. The primary reasons that you will need to understand APA style are the following: The overall structure of a research paper General APA guidelines How to report statistics We will start with the overall structure. 12.4.1.1 Organization of APA research reports The basic skeleton of a research report in APA format is as follows: Introduction; Methods; Results; and Discussion. This is sometimes called IMRaD structure (or often just IMRAD), an acronym based on the first letter of each major section (above, plus the coordinating conjunction). This structure has become dominant in the sciences during the last 50 years (Sollaci &amp; Pereira (2004)). Here is a Wikipedia link on the format. There is a rhetorical “shape” to the overall structure of the IMRaD report as well. It has an hourglass shape. The Introduction (the first few sentences, in particular) starts “wide,” in the most general way possible for the topic at hand. Paragraph-by-paragraph, the topic narrows progressively until the end of the Introduction is reached, at which point the research hypothesis is stated in one way or another. The scope of the Method and Results sections are quite “narrow,” referring strictly to the experiment(s) unique to the current research. The Discussion section begins “narrowly,” by referring back to the results of the study, but then progressively “widens” in scope as it places the results of the current study into the broader context set up at the beginning of the Introduction. All this usually concludes by pointing to possible, future directions that the current research could go in. Figure 12.3 below is reproduced from an article by Turbek et al. (2016). (The figure refers to biology, but you can just replace BIOLOGICAL with PSYCHOLOGICAL). Figure 12.3: The hourglass shape of the IMRaD research paper (from Figure 1, p. 420, of P. Turbek et al. (2016). A more common skeleton is as follows since most research reports consist of more than one study and/or experiment: Introduction Study/Experiment 1 Method Results Discussion Study/Experiment 2 Method Results Discussion Study/Experiment …n Method Results Discussion General Discussion The rhetorical shape of this structure is not that much different from the simpler hourglass approach above. The main difference is that only the General Discussion widens to the maximum extent at the end, whereas the study-specific Discussion sections usually only broaden out enough to justify the experiment that follows (if any). In this class you will be producing one of these reports. Ultimately, it will take the following form: Introduction Methods Study/Experiment 1 Results Discussion Study/Experiment 2 Results Discussion General Discussion We will review how to write each section in Chapters 13-17. 12.4.1.2 General APA guidelines There are also some basic, over-arching formatting guidelines for all APA papers. These are easy to enumerate: document double-spaced 1-inch margins all the way around header a shortened version of the title of your paper, in all caps, flush right, at 1/2-inch down a page number to the right of this title example: “MEMORY LOSS IN APHASIC BILINGUALS 7” (i.e., page 7) headings (i.e., Introduction, Method, Results, Discussion, General Discussion) centered paragraphs aligned flush on the left margin, and ragged on the right margin (i.e., NOT “justified”) indented 5 spaces for each new paragraph font Times New Roman size 12 text almost all numbers rounded to 2 decimal places (e.g., 4.66666 to 4.67) (you will see some minor exceptions here, like “All Fs \\(\\leq\\) 1”) all statistical letters (and only letters) go in italics (e.g., t, r, F, p) grammar Method and Results section should be reported in the past tense Introduction and Discussion can use more present tense Note that in the papers for this class, we will not ask you to include a title page nor an abstract. Writing starts on the body of the paper (i.e., beginning with the introduction). 12.4.1.3 How to report statistics The reporting of statistics is consistent across descriptive statistics and inferential tests. 12.4.1.3.1 Descriptive statistics These are usually reported in parentheses in text. When this is the case, they are indicated with specific abbreviations. For example: “The typical scores for the low-income group were higher (M = 32.81, SD = 5.6, n = 42) than the high-income group, whose scores were typically lower (M = 24.52, SD = 6.4, n = 28)” Sometimes however, they are reported in the text itself. This often happens when authors are discussing the numbers themselves instead of the constructs. For example: “The mean of 32.81 for the low-income group was calculated in the context of a positive skew, whereas the mean of 24.52 for the high-income group was not.” But in general, one should try to keep descriptive statistics in parentheses so that they do not interfere with the prose too much. That is, if you omit the descriptive statistics in parentheses when you read out loud, your listener can still understand what you’re saying. Thus, the first example passage above could be coherently read aloud as follows: “The typical scores for the low-income group were higher than the high-income group, whose scores were typically lower.” In short, the information outside the parentheses should be vital to the prose, and the information inside the parentheses should not be vital to the prose. Standard APA abbreviations for typical statistics covered in this class are as follows: mean: M standard deviation: SD total sample size: N sub-group sample size: n median: Mdn z-score: z Other descriptive statistics do not get abbreviations, as far as we know, as they are rarely reported this way. 12.4.1.3.2 Inferential statistics Inferential statistics are reported with a consistent pattern in APA. The pattern is as follows, expressed as a quasi-formula: \\([\\)test-letter\\(]\\)(df) = \\([\\)obtained-value\\(]\\), p [= OR &lt; OR &gt;] \\([\\)p-value\\(]\\) test-letter: this is usually a single, italicized roman or greek letter that represents the particular inferential test that was run. Examples that are relevant for this class include the following: \\(\\chi^2\\) (for a chi-square test) t (for a t-test) z (for a one-sample z-test) r (for a Pearson’s r correlation coefficient) F (for an ANOVA) (df): This refers to the degrees of freedom. It is not in italics. There will be one value here, except in two cases. For F tests, there will be two numbers, and for one-sample z-tests, there will be no number here. Sometimes this will be reported as a subscript of the letter (referring to the test type) coming before it. This is not actually an APA recommendation from the APA style guide, so you should not subscript the degrees of freedom. =: This always precedes the obtained value of the inferential test. there are no exceptions. obtained-value: This is the number that gets calculated from the test formula. It is a signal-to-noise ratio that varies from test to test. a comma: This always comes after the obtained value of the test statistic, and separates this number from the p-value p: This letter p (always in italics) identifies the number to the right of it as the p-value [= OR &lt; OR &gt;]: That is, either =, or &lt;, or &gt;. One (and only one) of those three symbols must occur in this location, depending on whether the p-value is exactly the number to the right (=), less than that number (&lt;), or greater than that number (&gt;). p-value: This is the p-value, an number between 0 and 1.99 That looks like a lot of information (and it is), but the actual, written form that it takes is quite concise. Here are some more concrete (albeit fabricated) examples: Chi-square \\(\\chi^2 (1)=5.2,p&lt;.05\\) t-test \\(t(28)=2.43,p=.022\\) Pearson’s r \\(r(48)=.18,p&gt;.05\\) ANOVA \\(F(2,12)=4.94,p=.027\\) z-test \\(z=14.60,p&lt;.05\\) 12.5 Issues not extensively covered There are a few issues that are keys to good writing, but that we do not cover extensively in this manual due to a lack of space. 12.5.1 Critical thinking It is sad that we do not cover this here, as it is the backbone of good research writing. If you cannot think clearly, you cannot write clearly.100 The task of teaching critical thinking lies just too far beyond the scope of this manual. However, there are good outside sources on critical-thinking skills. We will cover these under Section 12.6 below. 12.5.2 “Grammar,” clarity, and mechanics 12.5.2.1 A note on “grammar” There is a whole host of writing issues that have to do with sentence structure, word choice, punctuation, etc. Chances are that teachers have been bothering you for many years on these “deficiencies” in your writing, perhaps even calling it “bad grammar.” The lead author of this manual (Bolger) is a linguist, so he cannot bring himself to allow the the term “grammar” to refer to writing. For trained linguists, grammar is the mental system that leverage duality of patterning to allow humans (uniquely) to combine smaller linguistic elements into larger and larger units (according to specific rules at each level) in order to communicate increasingly complex meanings accurately to other humans.101. All humans, with very few exceptions, acquire most of their native grammar by age 5, and pretty much all of it by age 8 or so. The acquisition process is nearly unstoppable among humans.102 When teachers have referred to your “bad grammar,” therefore, they were really referring largely to what linguists would consider an, as-yet incomplete mastery of the formal dialect of English (used in academia). This is a dialect of English that must be learned either in school, or through lots of reading of formal English, or (usually) both. But this is a continuous process, and you are undoubtedly in the throes of it. It is at this point that we will no longer use the term “grammar” for this writing issue that keeps rearing its ugly head on your teacher’s feedback to you. Rather, we will simply call it what it is: mastering formal English. 12.5.2.2 Mastering formal English So you can see that we are not just shamelessly trying to get you to study the psychology of language.103 Rather, you should conceptualize becoming a better writer in academic English as a form of foreign-language learning, especially of vocabulary. As such, it is not out of reach as the expression “You have bad grammar” might imply. Rather, you just need to learn some additional vocabulary, along with a few other minor modifications to a language you already know very, very well (i.e., English, in our case). 12.5.2.2.1 Learning the vocabulary The bulk of the learning task here is acquiring formal vocabulary, which for historical reasons having mostly to do with the 1066 Norman Invasion in England, emphasizes latinate as opposed to germanic roots.104 To view a comparison of of germanic and latinate roots in English, go here. The germanic/latinate pattern that characterizes the informal/formal split in English should be obvious. It is well beyond the scope of this manual to delineate the differences between the informal and formal vocabulary of English. However, the link above gives you some insight. And we will provide some links at the end of this chapter. 12.5.2.2.2 Learning the style It isn’t just the vocabulary of formal English that is different from informal English. Rather, it is also the style. For instance, there are far more subordinate clauses in formal English than there are in informal English. Formal English is more complex than informal English. Compare the following: Formal: Due to the fact that there is a Thursday football game, non-essential university staff are encouraged to leave early on that day. The first clause there is a subordinate clause that can’t itself form a complete sentence. Informal: There’s gonna be a football game on Thursday. So employees that don’t need to be here should leave early. In this case, both clauses (before and after the comma) form complete sentences. They are independent clauses. More importantly, academic English is also highly hedged. Hedging is the process of backing off from over-confidence (or feigned confidence) in your claims. For example, it would be more common in academic English to say something like, “The results here are fairly clear,” instead of “The results here are indisputable.” Consumers of such research understand how to subtract out the hedging in order to match the original researchers’ original level of confidence. This is actually a wise strategy that protects researchers. There is a famous expression: “Extraordinary claims require extraordinary evidence,” which is a major tenet of science (see here). Academics often try to steer away from extraordinary claims since it is safer, professionally, to come up with too much evidence for a relatively weak claim than insufficient evidence for a relatively strong claim. This is also the source of some confusion for writers first getting used to this genre. Most of you, to be sure, were told at some point to make your writing strong. For instance, you were probably told at some point to have a strong thesis statement, or to write a strong hook as your first sentence. This approach to writing might be appropriate for journalism, or business, but it is not appropriate for science. This is not to say that scientific writing is weak. On the contrary, it is strong in general, but it is subtly strong. Again, it places more emphasis on conveying strong evidence, and away from the form of the writing itself. For example, Here is the first line of the paper from Watson &amp; Crick (1953), an article that relayed the discovery of DNA, thereby changing biological science forever: “We wish to suggest a structure for the salt of deoxyribose nucleic acid (D.N.A.). This structure has novel features which are of considerable biological interest.” They could have said: !!!!!!!! We just nailed down the basic chemistry of not only how life perpetuates itself, but also how life forms evolve! This is going to change the world! But they didn’t. Instead, they used expressions like “wish to suggest” and “considerable biological interest.” zzzzzzzzz. This is unfortunate in at least one way. Namely, the lay public does not necessarily understand how scientists are going about their business, and instead interpret weak claims (when they happen to come across them) as just that: weak claims.105 Moreover, they don’t usually understand how to interpret the relative weight of evidence (e.g., small p-values or large effect sizes). As a result, they may interpret a paper (or interview) that is quite persuasive to other scientists, as un-persuasive. That unfortunate aspect of the genre aside, you still need to abide by it. Maybe it will change one day, but for now, you will be expected to understate your case a bit when you report your own research, and trust your (educated) reader to infer the true strength of your research (or lack thereof). Traditionally, writing in the sciences and Psychology has also tended to be impersonal, avoiding the use of the pronouns I, we, or you, preferring instead formal pronouns like one, or avoiding pronouns altogether by using the passive voice (e.g., “Responses were submitted online” instead of “Participants submitted responses online.”). However, this is changing. There is much more use at least of we in science these days. In this class in fact, you should feel free to use the pronoun we. However, avoid using I or you.106. This will allow you to make your writing more concrete in the mind of the reader as you would otherwise need to use a lot of passive voice, which can make reading difficult since it conveys weak imagery (e.g., “The survey was sent out” [weak imagery] versus “We sent the survey out” [stronger imagery], or much more famously “Mistakes were made” versus “We made mistakes”). But just as in the case of vocabulary, the specific stylistic differences between informal and formal English are quite numerous, and beyond the scope of this manual. But we do have some links to some outside help at the end of this chapter. 12.5.3 Pacing the writing process There are other causes of “bad grammar” that don’t have to do with one’s knowledge, per se, of the dialect of formal, academic English. Most importantly, you can utter language (orally) faster than you can write it or type it. This is simply a biological fact. Predictably then, our working memory is designed naturally to work with the spoken language (which is naturally acquired starting at birth, and not learned in school like reading and writing are). One consequence of this is that it is quite easy for us to forget what we were writing and/or how we wanted to write it while we are writing it. As a result, it is not uncommon that writing often becomes disjointed, failing to connect well with what preceded it or follows it in print. It also often results in clarity problems or awkward constructions, like misplaced modifiers: Since they had to answer the survey in less than 10 minutes, our results are sometimes incomplete. (Obviously, your results didn’t take the survey. Rather, your participants did.) Good writers recognize the discrepancy between the speed, on the one hand, of uttering thoughts through language (or just generating thoughts), versus writing them out, on the other. They are also aware of the writing problems that can arise from this discrepancy. As a result, they develop strategies to check and double-check (*revising\") their writing (ultimately, without getting bogged down in micro-editing, which can also impede your writing). WARNING: Writing at the last minute is only going to exacerbate such working-memory errors as you go into panic mode. Start writing early, at a thoughtful pace, to circumvent this kind of writing crisis. In fact, you can only learn how to write efficiently this way if you practice it. So again, start early! Another strategy to deal with this discrepancy is to break the writing process up into different stages, starting with pre-writing. This is mentioned at the end of this page on pre-writing strategies from the renowned Online Writing Lab at Purdue University (which we will often reference in this manual). However, the thought process that would go into this differs depending on the section you are writing. So we will return to this within each writing assignment (Chapters 13-17). 12.6 Outside help on writing up research One of the oldest, online sources for help on research writing is the Online Writing Lab, or OWL, at Purdue University. Writing the Scientific Paper 12.6.1 Research papers in APA format Main Body from Wikihow Headings and seriation 12.6.2 Writing up research Dr. Jeffrey Kahn’s summary of how to report individual inferential tests. Here is a link to OWLs general tutorial on writing an experiment research report- general outline] Template for apa paper in Google Docs Formatting the body of an APA paper Some universities are much more teaching focused. They tend to charge more in tuition.↩︎ In fact, the Introduction sections to research papers are, in an indirect way, just a summary of this research-gap identification processes.↩︎ This unusual level of student integration was referred to above as something that is almost unique to the field of Psychology.↩︎ Note that this research doesn’t just feed the education circle in the form of resource material for textbook writers; rather, it also feeds other researchers within the research circle itself, naturally. In fact, this latter function is probably its primary purpose.↩︎ Also be aware of the dangers of citation generators. The Online Writing Lab at Purdue University has good information on this.↩︎ recall that we suggested students use Google Docs back at the very beginning of this manual in Section 0.2.2.↩︎ When multiplied by 100, the p-value is the number of times out of 100 that you would expect to get the obtained value that you did (or one that is more extreme) if you not only repeated the experiment/study an infinite number of times, but the null hypothesis were also underlyingly true.↩︎ That said, just because someone may be a good critical thinker does not necessarily mean that they are a good writer.↩︎ Students at Texas A&amp;M can learn more about this if they take PSYC 346 - Psychology of Language, which Dr. Bolger teaches occasionally, if you haven’t already guessed.↩︎ You can stop it, but you would need to deprive a child of virtually all linguistic input, as was the unbelievably sad story of [Genie](https://en.wikipedia.org/wiki/Genie_(feral_child), which you may have read about in your first Psychology class. Otherwise, children will find a way to acquire language, as anyone who made it through Kindergarten has↩︎ Well, maybe a little.↩︎ Latinate roots are words that have their origins in Latin (which were adopted into English mostly through Old French after 1066), whereas germanic roots are words that have their origins in the germanic dialects spoken about 1,500 years ago in the area currently occupied by the Netherlands and northwest Germany. In the 5th or 6th century, people from these areas invaded the British Isles and either displaced or assimilated the ancient Celtic-speaking people who had for hundreds of years been ruled by the Roman Empire (which had recently collapsed in this area around the early 5th century). The British Isles were ripe for invasion, and Germanic tribes took advantage. English is a direct descendant of these old germanic dialects. In fact, most of the words you use in daily life are of germanic origin (e.g., ask), whereas the words you use to write formally emphasize more latinate words (e.g., inquire). Of course, the Vikings didn’t want to be totally left out of the invasion spree, so they invaded the British Isles a couple of hundred years later, and also left a linguistic mark on English.↩︎ To be perfectly honest, the lay public usually only encounters this style of rhetoric in the media, for instance, when scientists are interviewed for magazine/internet articles or news segments. For better or worse, scientists tend to carry the understated rhetoric of the written scientific genre into the public sphere (which also includes university classrooms, by the way).↩︎ … at least until you have a lot of status in your field↩︎ "],["WritingIntroductions.html", "Chapter 13 Writing introductions 13.1 The purpose of the Introduction 13.2 Structure of introductions 13.3 Examples of introductions 13.4 Outside help on introductions 13.5 Practice writing exercise 1 13.6 Practice writing exercise 2 13.7 Writing Assignment #1", " Chapter 13 Writing introductions This chapter covers how to write the first part of an empirical research paper in the field of Psychology. The chapter culminates with Writing Assignment #1 (Section 13.7), which will be the first of four writing assignments for this course. 13.1 The purpose of the Introduction The primary purpose of an Introduction in a research report is to justify the study that is about to be reported as something either worth publishing (to reviewers before publication) or reading (to the general science community after publication). There are other purposes, but they are secondary. Introductions do this by ultimately identifying a research gap that needs to be filled. This gap is usually identified clearly near the end of the introduction, just before the introduction of the study in the Method section. In line with our earlier statement in Section 12.3 that writing in the sciences is very standardized (recall the hourglass analogy), the overall organizational structure of the Introduction is extremely predictable. For original research papers, there really is only one kind of structures to introductions: the general-to-specific pattern.107 13.2 Structure of introductions Introductions almost always follow a general-to-specific pattern of development in three parts. In part 1, the author starts off by introducing the topic. In part 2, the author covers a series of topics arranged from general to specific. This narrowing continues until the author reaches part 3, which will consist of minimally a research question, and possibly a research hypothesis. The vast majority of part 2 (the part that lies between the beginning and end) is a literature review, which explains what researchers have studied before, and crucially (near the end), what they haven’t. NOTE: Although you will not be required in this lab to do a proper literature review (there simply is no time), we will ask you to follow a general-to-specific structure in your Introduction. Instead of previous literature, you will use various strategies to write this section. These will be covered in the instructions for the main writing assignment (Writing Assignment 1 in Section 13.7. Note also that you can do a literature review if you so choose, but it is not required. Below we will provide several real-world examples of these three different components of the Introduction. 13.3 Examples of introductions Even though you probably won’t be doing a proper literature review for this class, you should see how one normally develops. After all, all the real-world examples of this type of writing will include literature reviews, so there really is no way around this. 13.3.1 Examples of opening sentences You were probably told by a teacher back in the day that you should have a good hook in your essay in order to get the reader’s attention. This is partially true of IMRaD papers, but usually in a more “boring” sort of way. To illustrate that this is the case, we will randomly select the first five papers that appear in the journal Frontiers in Psychology, when we restrict the search to papers that have already been published (on 07 August 2019) and fall under the category Most viewed.108 They are as follows: Lilienfeld et al. (2015): The goal of this article is to promote clear thinking and clear writing among students and teachers of psychological science by curbing terminological misinformation and confusion. Schäfer, Sedlmeier, Städler, and Huron (2013): Why do people listen to music? Lakens (2013), which you might want to read yourself since it is relevant to this class: Effect sizes are the most important outcome of empirical studies. Beetz, Uvnäs-Moberg, Julius, and Kotrschal (2012): During the last decade it has become more widely accepted that pet ownership and animal assistance in therapy and education may have a multitude of positive effects on humans. And finally, Barker et al. (2014): Executive functions (EFs) in childhood predict important life outcomes. You can see how psychologists are not using colorful hooks. These are not like newspaper headlines. Instead, these initial sentences are pretty dull, and even sometimes (deliberately) commonplace (e.g., “Why do people listen to music?”). This doesn’t seem to be an explicit recommendation on the part of the APA. Our guess that the reason for this is that scientists do not want to come across as salespeople. Doing so would alert wise reviewers (in peer-reviewed research) to the possibility that these authors are attempting to persuade them with strategies (e.g., colorful writing) other than the presenting sheer merit of their work in an objective manner. Another likely reason is that researchers do not typically scan research papers randomly for interesting things to read. Rather, they search in a deliberate manner across large databases for papers related to their own research. Catchy hooks serve no purpose for a paper that is deliberately sought rather than encountered by chance. Instead, such language distracts researchers, and is therefore discouraged in general. Nonetheless, you can also see from the examples given above that the authors do highlight the main topic of the paper. They get right to the point, sometimes directly, sometimes indirectly, but always right to the point. There are practice exercises on this in section 13.5.2 below. 13.3.2 Examples of the literature review After the opening sentence, most introductions evolve quickly into an extensive literature review. Although we will not be covering the literature part of the literature review in this class due to time constraints, we will cover the structure of the review. Specifically, underlying this literature review is almost always progress from general topics to more specific ones. It is a better idea to see this than to keep explaining it. The following excerpt consists of the first two paragraphs of the Introduction in Kaufman, Yaden, Hyde, and Tsukuyama (2019). For readability, we omitted all references to outside work, and replaced them with ellipses (i.e., …). We all have, within each of us, both a light and a dark side. We all vary, however, in the extent to which we consistently exhibit light vs. dark patterns of thoughts, feelings and behaviors in our daily lives. For over the past 15 years, there has been a flurry of empirical research on a number of “dark traits” that are associated with ethically, morally, and socially aversive beliefs and behaviors… There is an emerging consensus that the “dark core” (or so-called “heart of darkness”) of these dark traits consists of an antagonistic social strategy characterized by high levels of interpersonal manipulation and callous behavior… While there are some newcomers on the dark trait scene (e.g., sadism and spitefulness), the most studied and validated dark traits are indexed by the now infamous “Dark Triad” of personality: narcissism, Machiavellianism, and subclinical psychopathy… Since the initial paper proposing a Dark Triad of personality…, research on the topic has increased every year, with two thirds of the publications on the Dark Triad appearing in 2014 and 2015 alone… While each of the three members of the Dark Triad have unique features and correlates…, there is enough overlap among these socially aversive personalities that researchers have argued that they “should be studied in concert”… If you had to summarize each of these paragraphs into a single sentence, you would see that the second is more specific than the first. Here are our summaries: Scientists have found that everyone, to varying degrees, feels, thinks, and behaves anti-socially at times according to a basic set of dark traits. The “Dark Triad” is the most studied subset of these basic, dark traits, and we should continue to study this subset as a single group. As single phrases, the summary might go as follows: A. Universal bad behavior among humans and its core personality correlates B. The reliability of the dark triad as a subset of this core Clearly, the latter is more specific than the former in both cases. There is an exercise on writing progressively more specific paragraphs below in section 13.5.5. The take-home message from these examples is the organization, not the content. If you look outside this manual for help on writing literature reviews (or just reading literature reviews in journal articles yourself), you will see a lot of tutorials on how to refer to outside sources (e.g., summarizing, paraphrasing, citing). This is because the typical Introduction in Psychology consists of summaries of other studies or at least references to them (as in the two passages above). In fact, you can write an introduction that is almost purely summaries of other studies sequenced from most general to most specific. This is quite common actually. But it is not what you will be doing. For this class, due to a lack of time, you will not be summarizing outside research in your Introduction. Rather, you will use different strategies to write a general-to-specific introduction. These strategies will be covered in the instructions for the first writing assignment for this class, located down below in Section 13.7. But that will have to wait. The next section is about the very end of the Introduction. 13.3.3 Examples of research questions and hypotheses Before you read further, you should review the main textbook (Navarro &amp; Foxcroft, 2019), Chapter 9, especially Section 9.1, which explains in detail how a research hypothesis and a statistical hypothesis are two different things. In writing up research, we always state our research hypothesis (or hypotheses), but only rarely state our statistical hypotheses explicitly. This is because, unlike research hypotheses, statistical hypotheses are always clear implicitly once you understand how they work. Thus, since research reports are written with other researchers in mind (who understand statistics), there is little reason to write them out in the report. But again, writing out either a research question or a research hypothesis is a requirement in your writing. So that is what we will discuss below. Eventually, the progressive narrowing of topics along the progression of the Introduction culminates in a research space. This is a gap in the research, some area of inquiry that has not yet been addressed.109 The author typically claims at this point that their research project (to be reported subsequently) will go some way to filling this space. This is then followed by a brief description of the study and a research question or hypothesis. Below is a three-paragraph example from the end of the Introduction of Steinmayr, Weidinger, Schwinger, &amp; Spinath (2019, pp. 3–4), where the different components that make up the end of the introduction are quite explicit. The example here starts at paragraph ten of the introduction on page 3. … Taken together, although previous work underlines the important roles of expectancy and value components of motivation for school students’ academic achievement, hitherto, we know little about the relative importance of expectancy components, task values, goals, and achievement motives in different domains when all of them are assessed at the same level of specificity as the achievement criteria (e.g., achievement motives in math → math grades; ability self-concept for school → GPA). The Present Research The goal of the present study was to examine the relative importance of several of the most important achievement motivation constructs in predicting school students’ achievement. We substantially extend previous work in this field by considering (1) diverse motivational constructs, (2) students’ intelligence and their prior achievement as achievement predictors in one sample, and (3) by assessing all predictors on the same level of specificity as the achievement criteria. Moreover, we investigated the relations in three different domains: school in general, math, and German. Because there is no study that assessed students’ goal orientations and achievement motives besides their ability self-concept and task values on the same level of specificity as the achievement criteria, we could not derive any specific hypotheses on the relative importance of these constructs, but instead investigated the following research question (RQ): RQ. What is the relative importance of students’ domain-specific ability self-concepts, task values, goal orientations, and achievement motives for their grades in the respective domain when including all of them, students’ intelligence and prior achievement simultaneously in the analytic models? It’s not difficult to see that the first paragraph of the excerpt highlights a gap in the research. the second paragraph explains how the current research will fill that gap. And the third paragraph is even explicitly labeled as a research question. Note that their study was more exploratory than confirmatory, and as a result, it was difficult for them to formulate specific research hypotheses. Instead, they opted for a research question.110 Here is an example of a true research hypothesis (coming from a more confirmatory study) from the end of the Introduction by Dong, Liu, Jia, Li, &amp; Li (2018, p. 2). Note that it is worded as a statement, not a question. Based on previous literature…, we hypothesized that the effect of the cues were simultaneously interactive during trust judgments, and the effect of facial expression would be susceptible to social context, while the effect of facial gender would be stable in different settings. This is the kind of research hypothesis you should really aim for, though the research-question approach (as opposed to research-hypothesis approach) might be unavoidable in this lab, given the parameters of the research project we ask you to carry out. Taking a typical example of a research project in this class, a research question might be as follows: We wanted to find out how satisfaction with sleep relates to satisfaction with grades. But a research hypothesis would look more like the following: We predict that satisfaction with sleep will correlate positively with satisfaction with grades. So you can see how research hypotheses are preferred in the world of research, but sometimes, if there are no great grounds for predictions one way or another, a research question will suffice as a minimum requirement. Again, this has nothing to do with statistical hypotheses, which will always be part of a study involving inferential statistics. Down below are practice exercises on writing research questions (section 13.5.3) and writing research hypotheses (section 13.5.4). So we just highlighted the three components of the introduction. We will now point you to some outside sources of writing introductions, followed by some practice exercises, and finally, Writing Assignment #1. 13.4 Outside help on introductions Wortman-Wunder &amp; Kiefer (2012) https://writing.colostate.edu/guides/page.cfm?pageid=1561&amp;guideid=83 Bazerman (2010) https://writing.colostate.edu/textbooks/informedwriter/chapter15.pdf Driscoll (2011) https://wac.colostate.edu/books/writingspaces2/driscoll--introduction-to-primary-research.pdf 13.4.1 Opening sentences There really isn’t too much to say here other than you need to make a general statement related to your topic. However this is an entertaining page from Writing Commons on writing hooks 13.5 Practice writing exercise 1 These practice exercises work best in pairs. Form a pair (or a triad if necessary). Since you will be working in pairs or triads, we will simplify here and just call these working groups.111 Come up with a name for your group (e.g., SoccerHeads, or TheHellaCoolPair112). 13.5.1 Setting up your groups and docs You can form these working groups now. Once in your working groups, one of you should start a Google Doc in your own Google Drive. Figure 13.1 below shows you the two steps you need to take in Google Drive to start a new Google Doc. Figure 13.1: Creating a new Google Doc. Title the Google Doc as follows: [YOUR GROUP NAME] Chapter 13 Exercise 1, where [YOUR GROUP NAME] is replaced by the group name you chose above (excluding the square brackets). So for example: SoccerHeads Chapter 13 Exercises. Note that there is no Save button in Google Docs. It’s saved automatically as you go. Next, share the document with the other member(s) of your working group (the share button is at the upper right in Google Docs). Be sure not only to use their university email ID, but also to grant them editing privileges (i.e., Can edit). For good measure, make sure that all members of the group are logged out of outside Google accounts (e.g., JohnDoe AT gmail DOT com113). Being logged in to outside Google accounts can cause strange access problems. Also, critically, share it with your TA, using their university gmail. Also give them Can edit privileges. Do this now. See Figure 13.2 below for what we mean by editing privileges in Google Docs. Figure 13.2: Sharing a Google Doc with Can Edit privileges. You are now ready to start the practice writing exercises. 13.5.2 Opening sentence These exercises are short. But use them to practice not only how to start a research report, but also how to use appropriate academic language, as well as how to use Google Docs to collaborate and give feedback. Consider these a set of warm-up exercises. But first, type the following heading in: Opening Sentence. 13.5.2.1 Write two opening sentences Let’s say you’re creating a survey about quality of life in student dormitories in the first year of college. Referring back to the opening sentences in section 13.3.1 as guides, work together on two possible opening sentences (on paper) to start the paper on dormitory life.114 NOTE: This topic is a random suggestion. You can choose any topic to write on as a lab. However, everyone in the lab should agree upon the topic. Avoid uncomfortable topics that might make some students feel both uncomfortable and unwilling to let everyone know that they are uncomfortable with it. The topic should be quite benign. Your TA will act as a consultant here. 13.5.2.2 Feedback across working groups When done writing your sentences, someone from your group should raise their hand. Look for another working group that also has a hand up. Get their university emails and share your Google Doc with them electronically (i.e., share it with them in Google Drive). When you add them, add them with commenting (Can comment) privileges. They will reciprocate, giving you commenting privileges on theirs. Read that group’s sentences (as they will yours). Do any of the sentences use overly ornate or “catchy” language? Add comments to their paper, where you deem it worthwhile. Be professional; your TA is monitoring this. When finished commenting, place a final comment near the end of your counterpart working group’s sentences that says, “We are done commenting.” 13.5.2.3 Class review Very briefly, discuss this as a class. 13.5.2.4 Revise opening sentences Afterwards, insert comments in your own document where you feel necessary (Insert &gt; Comment). There’s also an icon in the toolbar for this ().115 You can also reply to other comments, or resolve them (if you, in fact, resolved them). Note that if you resolve it, it will disappear from your screen, but you can always retrieve it by clicking the Open comment history at the upper right (; Note: you may have to Show the menus by clicking the down arrow at the upper right to reveal these options). Also, come up with a final, polished sentence that you would be comfortable using later on. 13.5.3 Research question This is actually the third main element of the Introduction, but we are practicing it second in the practice exercises since this will make sense of the subsequent block of exercises in Section 13.5.5. 13.5.3.1 Write up research question First, type in the following heading: Research Question. Now, think of a research question on the the dormitory topic that you might pose (being careful to distinguish between research questions and research hypotheses). Use the examples (of research questions) provided above in section 13.3.3 as guides. Type in your research question together in your working group. 13.5.3.2 Feedback across working groups When finished typing in your research question, check in with your counterpart working group. If they’re finished too, look at what they wrote. Give them constructive comments. For example, evaluate whether it is more specific than the opening sentence; it should be. Is it a research question that others would be interested in? Give them advice as comments on their document. Show the other group when you are finished by typing in “We are done commenting” close to the end of their writing. 13.5.3.3 Class review Briefly, discuss this as a class. This will take somewhat longer than the last class review since more goes in to formulating a good research question. 13.5.3.4 Revise research question After the class review, either insert, or reply-to, or resolve comments in your own document where you feel necessary. Work with your partner(s) to come up with a final, polished version of the research question that you would be comfortable using later on. 13.5.4 Research Hypothesis This is the an alternative, or second part, to the third and last element of the introduction, but we’re doing it before we do the middle of the introduction (below) in order to make that exercise more productive. The hypothesis goes either in the exact same place as the research question (if no research question is explicitly stated), or right after the research question (when the research question is explicitly stated). Importantly, it is a more specific version of the research question, thus continuing the general-to-specific progression of the Introduction. 13.5.4.1 Write up research hypothesis First, write in the following heading for this section: Research Hypothesis. For the dormitory topic, together with the other members of your working group, generate a specific hypothesis, but this time using the hypotheses in section 13.3.3 as guides (not the research questions). Type the hypothesis in to this section. 13.5.4.2 Feedback across working groups When you are done, check in with your counterpart working-group. If they’re finished too, have a look at their research hypothesis. Give them constructive feedback via the commenting function. For example, does their hypothesis make specific predictions that could be confirmed or disconfirmed by inferential statistics? Or was their hypothesis still a bit vague in this respect? Again, indicate when you are finished by typing in a final comment near the end: “We are done commenting.” 13.5.4.3 Class review Briefly, discuss this as a class. This might take a little bit longer than the class review on research questions since a research hypothesis is more specific. 13.5.4.4 Revise research hypothesis After the class review, either insert, or reply-to, or resolve comments in your own document where you feel necessary. Work with your partner(s) to come up with a final, polished version of the research hypothesis that you would be comfortable using later on. 13.5.5 General-to-specific paragraphs This practice exercises is a very simplified version of what you would be doing in your own papers (Writing Assignment #1 in (section 13.7 below). The section between the opening sentence in your paper and the research questions or hypotheses forms the bulk of the writing in that assignment, ultimately occupying about a 1.5 to 2 pages. However, most introductions in IMRaD reports are much more extensive. A randomly chosen article from cognitive science (Dambacher &amp; Kliegl, 2007) looks to have an average-sized Introduction. It is about 1,300 words long, which translates to about 5.5 pages (Times New Roman, font size 12, double-spaced). Back to the current in-lab assignment. 13.5.5.1 Write an “in-between” paragraph First, write the following heading in to your Google Doc: In-between paragraph. Importantly place this just above the heading that you already have: Research Question. You will write below this heading. In the end, all of the paragraphs will be correctly ordered (albeit incomplete when compared to what your full introductions will look like in Assignment #1). This time, write separately (in different paragraphs; it doesn’t matter whether your above or below). Write a 3-4 sentence paragraph that will be more specific than the opening sentence, but more general than both the research question and the research hypothesis. You can talk about a general sentiment that you suspect is shared by students in that environment, or a news story that is relevant, or an anecdote (a story that occurred), or whatever. 13.5.5.2 Feedback across working groups When you are finished, check in with your counterpart working group. If they are done, have a look at their document. Is each paragraph of 3-4 sentences more specific than the opening sentence, yet broader than the research question/hypothesis? Could you rank each of the paragraphs in terms of their specificity? If so, would they fit together in an introduction that was progressively moving from general to specific? If not, could you modify them to make it so? Provide comments in their document where appropriate. Yet again, indicate when you are finished by typing in a final comment near the end: “We are done commenting.” 13.5.5.3 Class review Briefly, discuss this as a class. The TA will ask for a few examples. This will take significantly longer than the previous class reviews since there is much more writing involved. 13.5.5.4 Revise in-between paragraphs Afterwards, either insert, or reply-to, or resolve comments in your own document where you feel necessary. Come up with a final version of your paragraphs. If you were able to determine whether one paragraph was more specific than the other, move the more general one above, and the more specific one below that. Try to formulate a transition sentence between the two, and place it at the end of the upper paragraph. If you have time, try to come up with a final sentence or two that identifies a gap in your knowledge. Normally, this is a gap in the research. But since we are not doing actual literature reviews, you should insert what you believe to be a gap in your own knowledge. This gap won’t follow directly from the other paragraphs, though we expect that in your own paper submissions, the gap will emerge naturally from the preceding paragraphs. 13.6 Practice writing exercise 2 Here, you can repeat the practice above, but with a topic of your group’s choice. However, it would probably be better to do this in one fell swoop. That is, do the following all at once: Write an opening statement about the topic Write a couple of more specific paragraphs Write a research question Write a research hypothesis Start a new Google Doc to carry this out. You may want to work with a different counterpart working group as well. Congratulations! You’re done with the practice assignments for writing Introductions! Your TA might ask you to download this document as a .pdf onto your lab computer, and then upload the .pdf as your attendance assignment for the day. Moving on, below is your first major writing assignment. 13.7 Writing Assignment #1 13.7.1 General Instructions Fair warning: This writing assignment isn’t designed exactly like a normal Introduction. There just isn’t time for each of you to do a literature review on your topics. This is also not the main topic of the class; statistics is. However, your Introductions will be written “from the top of your head,” (i.e., just including your ideas, musings, anecdotes, hopes, news stories, internet stories, etc.) but organized from general to specific. It should be about 1.5 to 2 pages long, double-spaced, Times New Roman, 12 font. Do give it a title related to the topic, but do not include a header called Introduction. The introduction in an APA paper doesn’t have a header.116 Also, do NOT put your name on it. It will be submitted for anonymous peer review through Peerceptiv, and including your name makes it non-anonymous. Your TAs will know exactly who you are since it is submitted electronically, and Peerceptiv indicates who you are (to the TAs and the instructor only). 13.7.2 Specific Instructions The canonical Introduction proceeds as follows: It begins with a very general, commponplace statement that is clearly related to the topic, but not that informative overall; It weaves previous research (via summaries) into a narrative that progressively becomes more specific as the paper proceeds (though you will be substituting anecdotes, news stories, personal impressions, etc. for the literature review); Near the end of this narrowing, the introduction identifies a research “gap” (a gap in scientific knowledge in genuine scientific papers; a gap in your own knowledge for this assignment); It is usually convenient to tail this gap off with the research question at this point (i.e., “What are some interesting questions that emerge from this gap?”); This research question is then followed by a brief description of how the researcher proposes to address these questions, namely, a very short description of the methods of the study (in your case, your survey); Finally, at the end of the Introduction is a prediction or two. These predictions may be weak or strong. A weak prediction is not “weak” in the grading/assessment sense. Rather, weak here simply means that the researcher does not necessarly commit to a particular outcome. In fact, a weak prediction may be the most appropriate (and get a higher grade) if in fact you don’t have a good reason to make a strong prediction. A strong prediction is one that commits to variables coming out one way or another in the study. This is usually desirable in the real world of science, but is not really expected of assignments like this one. There is an example of such a paper in Appendix A. That paper starts off by broadly introducing the topic of the anti-vaccination movement. It then starts broadly by talking about the historical development of the polio epidemic. It gets a little more specific by describing how the vaccination era brought polio under control in the 1950s and 1960s. This is followed by a specific event in 1998 that seems to have triggered the beginnings of an anti-vaccination movement. Finally, it gets even more specific by pondering what sort of psychology underlies someone who engages in this kind of thinking. A lack of understanding of this kind of conspiracy thinking is identified as a reseach gap, even though that lack of understanding may or may not apply only to the author.117 This is followed by a brief description of the study that was designed to help fill that gap. The author ends the Introduction by making some weak predictions about the outcome of the survey, along with the potential impact or meaning of the study. The rubric that will be used to evaluate this assignment is in Appendix B. TIP: We suggest that you write this as a Google Doc as well (in your university Google account only). The reason is that before you upload it to Peerceptiv (and ultimately Turnitin for the final draft), you can “Name (the) current version.” Google Docs allows versioning, which is a way of taking a snapshot of the document along with a timestamp. This is useful for many reasons, among which is when you want to provide evidence to your professor that you actually had something done when you said you did. This is much, much more difficult with documents that are written on individual computers. To create a version, just click File &gt; Version history &gt; Name current version. Give it a name like YourName_PSYC301_Introduction_RoughDraft. You can see the process of saving a version below in Figure 13.3 Figure 13.3: Saving a named version of a Google Doc. 13.7.3 Uploading the rough draft When you have your rough draft ready to submit, double-check that your name has been removed, and that there is no heading named Introduction. The blind peer review in Peerceptiv doesn’t really work if you reveal your identity. On the day that you are supposed to upload your draft to Peerceptiv (via eCampus only!!), you must do so by the time specified by your instructor. If you submit it late, but within the first 24 hours of the initial due date/time, then you will get half credit for the assignment. That is, if you would normally have gotten 24 points out of 30, you would get 12 points out of 30. This is called a grace period by Peerceptiv. After the grace period, there is almost no chance that your assignment will get reviewed by one of your peers. As a result, Peerceptiv won’t let you upload it. However, you still need to submit it (It’s a requirement of W courses at Texas A&amp;M that you submit a rough and final draft). This means that you will need to email it to your TA, who will then give you direct feedback (but again, your maximum score would be 15/30). However, for each additional 24-hour period after the initial grace-period expires, you lose 10% off your final draft. And even if you are down to 0 points, you still must submit a final draft for feedback before you can submit a final draft. Again, these are rules for the W courses at Texas A&amp;M. NOTE: You may be able to submit your paper directly from Google Docs by copying the unique URL for the document (e.g., https://docs.google.com/document/d/15ApECWiuvoWgQysyQaKKWfNierx-yULT5udKVXdIj9B), but we are not quite certain. If that does not work, you can always download the Google Doc as a .pdf (File &gt; Download &gt; PDF Document (.pdf)), then upload it to Peerceptiv from your computer. That will always work. The Peerceptiv Student User Videos have several links to how-to videos. You should watch all of them (there are only four currently). The third one, Login and Submission is the most important when you have finished your rough draft. After you have uploaded your draft, further instructions for Peerceptiv will be given to you in class. 13.7.4 Uploading the final draft Your final draft must be an honest attempt to revise your rough draft according to feedback from your peers in Peerceptiv, as well as possibly that of your TA (TAs will spot-review certain peer reviews to make sure the peer reviews proceed accurately and honestly). On the day that the final draft is due (a week after the rough draft), the same time deadline is in effect: 6 am. However, instead of uploading to Peerceptiv, you will be uploading the document to Turnitin. For the final draft, late penalties accrue at a rate of 10% per day. This is on top of any penalties from the rough draft. It is easier to “upload” (“sideload?” Cloud-to-cloud) a Google Doc into Turnitin as it is explicitly one of the three upload options on the upload screen: Choose from this computer Choose from Dropbox Choose from Google Drive WARNING: For Google Drive, there may be a screen that asks you whether you grant permission to Turnitin to interact with your Google Drive. Please note that if that happens (it probably will), that IS NOT THE UPLOAD PROCESS. Only AFTER you give Turnitin permission to interact with your Google account, can you THEN upload. There are some other patterns, like the problem-solution pattern, but they are much less common.↩︎ We did exclude one on precognition, due to personally held principles on not covering absurd topics.↩︎ Replication studies (which are despairingly rare) do not create a unique research spaces, per se. But such cases could easily be justified as simply a lack of sufficient replication up to that point in time.↩︎ Exploratory studies are all well and good (especially if the authors are honest about it), but they do tend to be less preferred by journals than more confirmatory approaches.↩︎ … but not lab groups since that could be confused with the research groups you are in.↩︎ The head author of this manual is from Northern California, where hella is common.↩︎ This strange approach to writing email addresses was used in order to avoid prevent the markdown file from inserting it as an actual email address.↩︎ Working on a Google Doc together can be a little disconcerting at first since you will see others working on the same electronic document as you. But you will get used to it.↩︎ The Google icons used in this paragraph were provided by Google (https://google.github.io/material-design-icons/) and covered under the Apache License 2.0. You are free to re-use these icons in your own work, but do not sell them.↩︎ The first main header comes under Methods↩︎ And this is okay since we are not requiring a bonafide literature review, one of the purposes of which is to assure the reader that the researcher is not trying to re-invent the wheel.↩︎ "],["WritingMethodSections.html", "Chapter 14 Writing Method sections 14.1 Purpose of the Method section 14.2 Structure of Method sections 14.3 Outside help on Method sections 14.4 Practice writing exercise 1 14.5 Writing Assignment #2", " Chapter 14 Writing Method sections This chapter covers how to write the second part of an empirical IMRaD research report in the field of Psychology: the Method section (singular Method, not plural). The chapter culminates with Writing Assignment #2 (Section 14.5), which will be the second of the four writing assignments for this course. Note that Before carrying out either Practice Writing Exercise 1 (section 14.4) or Writing Assignment 2 (section 14.5), you will probably need to have already performed the following procedures on your survey data: filtering out participants who did not want their data analyzed generating descriptive statistics about basic demographics, namely counts by sex and mean age (and standard deviation) reverse-scoring any reverse-coded items transforming variables on different scales evaluating internal reliability (Cronbach’s Alpha) calculating the composite variable How to do each of procedures (a) and (c)-(f) is covered in Chapter 11 in section 11.5. How to carry out procedure (b) is in Chapter 3, section 3.2. Justifications for these preliminary analyses are below. We included these procedures (again, see section 11.5) because of the parameters of the study we had you carry out. First (a above), there is always the possibility that a participant or two doesn’t want their data analyzed. If you recall, you placed this question near the end of the survey. We can use the values provided here to remove those observations with a filter. You need to do this first as it will affect any descriptive statistics you carry out in (b). Second (b above), all studies involving human participants report the numbers of participants by sex, and the mean and standard deviation of their ages in years. You may also want to carry out descriptive statistics on each of your outcome variables. You can see how, for the example in Appendix C, this mattered a little bit. It depends on your study, however. Third (c above), one of your outcome variable is probably reverse-coded. This means that it is answered in the opposite way from other questions For example, whereas two of your variables might be worded as, “I like X’s very much” (1 = Agree, 5 = Disagree), a reverse-coded question might be worded as, “I dislike Y’s very much.” Most longer surveys contain reverse-coded questions in order to either make the survey more interesting overall, or to detect dishonest (mechanical) responses, or both. Fourth (d above), you may need to transform one or more of your three outcome variables into z-scores before averaging them together. The reasons for this is that it makes no sense to average things that are on different scales (like averaging someone’s weight with their height). However, z-scores are a standardization technique that allows you to do so. But if your three outcome variables are all on the same scale (e.g., a 1-5 Likert scale), then you don’t need to worry about z-scores. Fifth (e above), you will need to calculate Cronbach’s Alpha. Cronbach’s Alpha is a way of evaluating how well a set of scores correspond with each other in terms of participant responses (item consistency). Presumably, your three outcome variables should correlate well with each other since they purportedly measure different aspects of the same thing (a single construct). Therefore, it is useful (and generally expected) to calculate and evaluate Cronbach’s Alpha anytime you have a set of items in a composite variable. The last step (f above) will be to create the composite variable. This can be either a sum or an average (mean). What is imporant for later discussions is that you remember exactly which one you chose. So if you have not yet performed these procedures with your data, please go to sections 3.2 and 11.5. You will need to have done so prior to beginning the writing practice (section 14.4) and assignment (section 14.5) down below. In fact, doing all the procedures above in a single jamovi file corresponds to Statistical Output for Assignment 2 in eCampus. For now, we will move on to the purpose and structure of Method sections. 14.1 Purpose of the Method section The Method section is often under-valued by the casual reader. But it is extremely important for two main reasons. First, it is the section that reviewers for journals (and skilled researchers in general) read carefully in order to evaluate the validity (and to some extent, the reliability) of a study. There is a famous saying in research design: garbage in, garbage out. This means that no matter how much you try, you won’t be able to correct a bad research design, even with sound statistics. A bad research design will result in bad results (though a good research design does not necessarily entail good results). The Method section is where a reviewer or a careful reader would detect the garbage-in half of the saying. Paper submissions to journals are often rejected solely on the basis of the Method section alone. Second, the Method section serves as a set of instructions for future researchers to replicate the study. Although replications are woefully under-published, they are a very important, indeed, indispensable part of the scientific process.118 Therefore, you will devote a lot of time and space to this section in your own papers. That said, it is easy to go too far in a Method section by providing too much detail. You probably will not have this problem in your own papers since your class projects are inherently small. But as you “graduate” to more elaborate studies, this is a danger. Conveniently, the Method section is quite straightforward in how it is written. This is made easier by the fact that, with the exception of very brief reports, Method sections are broken into various sub-sections. 14.2 Structure of Method sections As noted above, the Method section is all about spelling your study out so that others can either evaluate your study design, or “replicate” the study altogether.119 In terms of content, the Method section describes… the who: a description of the sample that was used the what: a description of the materials or instruments used the how (researchers): a very general description of how the researchers analyzed the data statistically, including the research design the how (participants): a description of what participants actually did this last sub-section acts as a transition to the Results section that follows Accordingly, the Method section is split into various sub-sections, with fairly predictable labels, along with a fairly predictable order. We cover them in turn below. 14.2.1 Participants This sub-section was traditionally called Subjects, but that more passive label fell out of use as the more active Participants label better reflected the shift to higher standards of informed consent over the years.120 The sub-section details the general demographics of the sample. In reports several numbers, including those referring to the total number of participants analyzed, proportion of participants by sex, and the mean and standard deviation of age (in years for adults; years and months for children). It also reports any exclusion criteria (a priori reasons why participants would be removed for analysis). Normally, the sub-sub-section on exclusion criteria contrasts the total number of participants who completed the study versus the final number of participants analyzed. It also reports on any details about the participants that would be critical to the study at hand. If the study includes many subgroups (e.g., low socioeconomic status children with dyslexia vs. high socioeconomic status children with dyslexia), those would need to be reported here. In this class, you do not have such a situation. As a result, your Participant sub-section will be relatively short. Finally, this section often details any incentives that were offered participants. Incentives can include anything from monetary remuneration to course credit. Sometimes, researchers report that the study had obtained approval from a Research Ethics Board. But we are not sure why since this does not affect the study itself in any way, and could easily be reported in a footnote or endnote. But you will see instances of this here and there where it is reported directly in this sub-section. 14.2.1.1 Examples of Participants Here is an example from Weisberg, DeYoung, and Hirsh (2011). This study was a replication of previous studies (in general) that looked at gender differences in Big Five personality traits. Naturally, it was a survey, and so the demographics of the respondents mattered. Participants Participants (N = 2643; 892 male, 1751 female) were drawn from a number of research projects, for which they received either monetary compensation or university course credit. Much of the data was collected in a large Canadian metropolitan area, either as an online survey or as a part of laboratory studies (N = 1826; 537 male, 1289 female). Some participants (N = 481; 200 male, 281 female) were members of the Eugene-Springfield community sample (ESCS). Lastly, 336 participants were recruited via Amazon’s Mechanical Turk (MTurk; 155 male, 181 female) and completed the measures online. Participants ranged in age from 17 to 85 (M = 27.2, SD = 14.4). The majority of participants identified as White (39.9%) or Asian (27.5%), with 1% or less identifying as Native American, Hispanic, and Black. Twenty-five percent of participants identified as “other,” and 5% did not specify ethnicity. The demographic data for a number of our samples allowed participants to choose from only the above five ethnicity classifications or specify their ethnicity as “other.” Therefore, the classification of “Asian” contains individuals of both South-Asian and East-Asian ethnic backgrounds. Though South-Asian and East-Asian cultures are markedly different in many ways, both are more collectivist than Western cultures… and therefore provide an interesting contrast to the White/European ethnic background. The example above is a moderately sized sub-section on participants. Notice that everything is in the past tense. This tendency will continue through the Results section. There are longer and shorter versions, as exemplified below. There is something that is really important to notice about the example above, as well as all the examples below. Namely, most statistics about samples, especially means and standard deviations, are usually (though not always) reported between parentheses. For example (from above): Participants ranged in age from 17 to 85 (M = 27.2, SD = 14.4) Also note that the letter M stands for mean and the letters SD stand for standard deviation. These are standard APA guidelines. APA guidelines also stipulate that these statistical acronyms (or more precisely, initialisms) should be in italics, though the authors here did not do this for the standard deviation for whatever reason. You should, however. Below is an even more detailed participant-description sub-section (shortened as much as possible; deletions indicated by ellipses[…]). It is from Macdonald, Germine, Anderson, Christodoulou, and McGrath (2017). Their study was a survey sent out to a very large number of educators and the general public in the United States about belief in neuroscience myths (e.g., being left- vs. right-brained, learning styles, etc.). As with any complex survey like this one, it was important to spell out details on who actually responded vs. who was actually analyzed, and how. Participants The neuromyths survey and associated demographic questionnaires were hosted on the website TestMyBrain.org from August 2014–April 2015. TestMyBrain.org is a citizen science website where members of the public can participate in research studies to contribute to science and learn more about themselves. In addition to these citizen scientists, we also explicitly advertised this study to individuals with educational or neuroscience backgrounds and encouraged them to visit TestMyBrain.org. Advertisements were distributed through professional and university listservs and social networks (i.e., Council for Exceptional Children, Spell-Talk, Society for Neuroscience DC chapter, American University (AU) School of Education, AU Behavior, Cognition, and Neuroscience program, AU College of Arts and Sciences Facebook page), as well as professional and personal contacts of the authors. In order to increase participation from educators, we used a snowball sampling technique in which each participant that completed the survey was asked to share the survey link with an educator that he/she knew through various social media (i.e., Twitter, Facebook, LinkedIn, email, etc.)… The starting sample included surveys from 17,129 respondents worldwide. Only fully completed surveys were logged for further analysis. Participants were excluded if they reported experiencing technical problems with the survey (735 dropped), reported taking the survey more than once (e.g., based on a yes/no question at the end of the survey; 2,670 dropped), or reported cheating (e.g., looking up answers on the internet, discussing with an external person, 17 dropped). Further exclusion criteria were age &lt;18 years (2,121 dropped), missing data on gender or educational background (361 dropped), and non-US participant (according to IP address or self-endorsement; 6,899 dropped). Next, we plotted overall survey accuracy by time to complete the survey to filter individuals who might have rushed through the survey without reading the questions (22 dropped). On the demographics questionnaire, participants were asked if they were currently enrolled or completed middle school, high school, some college, college degree, or graduate degree. We excluded individuals with a middle school or high school degree from the analyses because of inadequate representation of these individuals in the sample (N = 424 general public, 3 educators, 0 high neuroscience exposure). Therefore, our analyses were limited to those with “some college” experience or beyond. These filtering and data cleaning steps brought our final sample size to N = 3,877 (N = 3,045 general public, 598 educators, 234 high neuroscience exposure). One of the goals of this study was to compare the neuromyths performance of three groups: the general public, self-identified educators, and individuals with high neuroscience exposure. We defined “high neuroscience exposure” using a question on the demographics questionnaire which asked, “Have you ever taken a college/university course related to the brain or neuroscience?” Answer options for this item were “none,” “one,” “a few,” or “many.” Those who indicated “many” were categorized in the neuroscience group, unless they also reported being an educator, in which case they remained in the educator group (N = 53 educators also reported taking many neuroscience courses). We prioritized educator status over neuroscience exposure in this grouping in order to understand the full range of training backgrounds in the educator group. The “general public” group was composed of all individuals who completed the survey but did not self-identify as an educator or having high neuroscience exposure. We use the term “general public” to refer to the citizen scientists who participated, but we acknowledge the selection artifacts inherent to this group and address these limitations in the discussion section. Table 1 shows the demographic features of the three groups… Obviously, that was quite long. Your papers will not approach anything like that. But you should know about the diversity of these sub-sections. Some are quite long, and some are quite short. It depends on the nature of the study. Survey studies usually require more information about participants; experiments usually less. Next is an example from an experiment carried out in a laboratory setting. And finally, below is a significantly shorter description of participants from Jung, Wranke, Hamburger, and Knauff (2014). This study consisted of a series of laboratory experiments at a university. Although they included a survey to gauge the emotional state of the participants, it was done so in an experimental fashion, and no demographic variables were recorded. Thus, as you can see, the description of participants is quite minimal since the demographics of the participants are already quite well known (college-age students):121 Participants Thirty students from the University of Giessen participated in this study (mean age: 22.93 years; range: 19–30 years; 18 female, 12 male). They did not participate in any previous investigations on conditional reasoning and they received a monetary compensation of eight Euro. The participants came from a range of disciplines and none of them were psychology students. They were all native German speakers and provided informed written consent. Such short sub-sections on participants is not uncommon in laboratory-based research with neuro-typical adult participants (e.g., college students). Next, we move on to the Materials sub-section. 14.2.2 Materials First, we have used the term Materials as one example of several that might be appropriate. It depends on the study. Other terms that are commonly used to head this sub-section are Stimuli, Instruments, and Measures, among others. What these different sub-heading labels share, however, is that they describe anything and everything that participants interact with consistently during a study or experiment. In the case of surveys, the participants are exposed to questions. In the case of a lexical-decision task, participants make decisions about letter strings (words and pseudowords, usually). As was true in the case of describing participants above, the length and detail of this sub-section will vary with its complexity. For highly complex designs, where it would be virtually impossible to cover each and every item, researchers usually resort to supplementing this sub-section with tables and/or appendixes. Descriptions that might be covered in this sub-section vary as much as different studies and experiments differ. For instance, anyone working with lexical-decision tasks (the bread and butter of psycholinguistics), would need to include the following: The total number of items The total number of words vs. pseudo-words The mean frequency of occurrence of the words The mean letter-lengths (in number of letters) for both kinds of stimuli The procedure used to select both words and pseudo-words (you cannot select them all) The nature of the words and pseudo-words (e.g., high-frequency vs. low-frequency animal labels, like dog and okapi, respectively; or pronounceability of the words like syllabic complexity; or semantic characteristics like concrete vs. abstract]). Anything goes here really; it depends on what the study is trying to test. A list of the words and pseudo-words used (often in an Appendix, or available somewhere online) But the description of materials for a psycholinguistic study is going to differ dramatically from that of a survey. A survey is going to list elements like the following: A. the total number of questions B. the constructs that the questions relate to (brief mention; the computation of the constructs is better placed in the Design/Analysis sections) C. the number of items within each construct D. the questions themselves (if not too numerous) E. the potential range of scores if the response to the question is continuous (e.g., a Likert item on a scale of 1 to 5, with 1 meaning strongly disagree and 5 meaning strongly agree, and 0 meaning neither disagree nor agree) F. the order of the questions on the survey G. any branching that occurs during the survey (subsets of questions that only appear if a respondent answers a certain way on another question) There could be more in these sections, respectively. It depends on the study. There are a few examples below. 14.2.2.1 Examples of Materials The study below by Rollero and DePiccoli (2017) is about self-objectification and how higher-order personal values (e.g., self-enhancement, openness to change) influence the extent to which people self-objectify. They use the term Measures instead of Materials. These two terms are almost synonymous. This consisted of a fairly quick survey, so it is probably more like what you will be doing in Writing Assignment #2 in section 14.5. Measures Data were gathered by a self-reported questionnaire which took about 15 min to be filled in. The following variables were assessed: Self-objectification: Body Shame The Body Shame subscale of the Objectified Body Consciousness Scale (McKinley and Hyde, 1996) was administered. It is an eight item scale used to measure self-objectification and feelings of shame when one’s body does not conform to cultural standards. Participants responded to a 7-point scale ranging from “strongly disagree” to “strongly agree” (Cronbach’s \\(\\alpha=0.83\\)). (e.g., “When I can’t control my weight, I feel like something must be wrong with me”). Self-objectification: Body Surveillance The Body Surveillance subscale of the Objectified Body Consciousness Scale (McKinley and Hyde, 1996) was also used. It measures the frequency with which participants monitor their physical appearance and consists of eight items on a 7-point scale ranging from “strongly disagree” to “strongly agree” (Cronbach’s \\(\\alpha=0.84\\)) (e.g., “I rarely think about how I look” – reversed item). Personal Values Based on the Schwartz’s Values Survey (Schwartz, 1992, 2006) respondents rated 56 values “as a guiding principle in my life,” using a 5-point scale ranging from “opposed to my values” to “of supreme importance.” Following the Schwartz’s model, items were grouped into four subscales, referring to the higher order values: self-enhancement (\\(\\alpha=0.81\\)), conservation (\\(\\alpha=0.72\\)), self-transcendence (\\(\\alpha=0.84\\)), and openness to change (\\(\\alpha=0.79\\)). Body Mass Index Participant reported their height and weight, which were used to calculate BMI (\\(kg/m^2\\)). In the case above, the questions were drawn from published, external question batteries. As a result, they only needed to provide the references to the original papers that described the survey in more detail. If you create your own questions on a survey, you would need to spell out each question in more detail (assuming there are only a few of them). The example below is from a German study about effortful control and bilingualism (Keller, Troesch, Loher, &amp; Grob, 2016). The study involved extensive testing of 4-5 year-old children on multiple measures involving cognitive control and language. As a result, the description of materials is quite extensive since one has to be very careful and very precise in one’s interactions with such young children. Measures Effortful Control Effortful control was assessed using the German version of the Child Behavior Questionnaire … by childcare and kindergarten teachers at \\(T_1\\) [Time 1]. The Child Behavior Questionnaire is a widely used assessment of temperament used in early to middle childhood. The present study used the subscale EC, consisting of 12 items (i.e., “When drawing or coloring in a book, shows strong concentration” and “Is quickly aware of some new item in the room”). Children’s EC ability was rated on a 7-point Likert scale ranging from 1 = extremely true to 7 = extremely untrue. In the current sample, internal consistency of the subscale was rated as good, with a Cronbach’s \\(\\alpha=0.84\\). Items were averaged to form an EC score. Language At \\(T_1\\), German language competence was measured with the standardized language development test SETK-2 (Sprachentwicklungstest für zweijährige Kinder…), which was designed for monolingual German-speaking children aged 2 years. The SETK-2 assesses children’s expressive and receptive vocabulary, as well as morphological and syntactical aspects of the German language. A pilot study indicated very low German language competence in DLLs [dual language learners]…. Therefore, the SETK-2 was applied despite a higher chronological age of the DLL sample compared to test norms. This procedure allowed us to prevent floor effects in DLLs with regards to their substantially lower competence spectrum. Although the SETK-2 was applied to both monolinguals and DLLs, for hypotheses testing, DLL children’s data were used exclusively due to ceiling effects in monolinguals. The SETK-2 consists of four subtests: Word Comprehension, Sentence Comprehension, Word Production, and Sentence Production. Both language comprehension subtests of the SETK-2 are similar in structure to the Peabody Picture Vocabulary Test … where the child was presented four colored pictures from which the child had to choose the correct alternative form when orally presented a word or sentence. In both subtests of language production, children were presented with picture cards that depicted either objects or actions, which had to be named or described. The four subtests Word Comprehension, Sentence Comprehension, Word Production, and Sentence Production range from 0–8 points, 0–9 points, 0–24 points and 0–77 points, respectively. In the present study, standard values (T-scores) were used as dependent variables, based on the German monolingual 30- to 35-month-old norm sample (Grimm, 2000). In the current sample, internal consistencies were \\(\\alpha_{Word Comprehension}=0.85\\), \\(\\alpha_{Sentence Comprehension}=0.81\\), \\(\\alpha_{Word Production}=0.95\\), and \\(\\alpha_{Sentence Production}=0.93\\) At \\(T_2\\) [Time 2], language competence was assessed in the entire sample with three subtests of the SET 5–10 (Sprachstandserhebungstest für Kinder im Alter zwischen 5 und 10 Jahren…), a language development test for children aged 5–10 years. In the present study, the subtests Language Comprehension, Picture Naming, and Picture Story were applied. The subtest Language Comprehension assesses the comprehension of complex sentence structures (main and subordinate clauses). Children were thus read 12 sentences that had to be replayed with toys. The subtest Picture Naming assesses expressive vocabulary. Children were presented 40 picture cards of objects and actions that had to be named (e.g., stamp, thermometer, or painting a wall). In the subtest Picture Story, children were asked to tell a story based on five consecutive pictures. This narrative was subsequently analyzed based on predefined semantic and syntactic criteria. The three subtests Language Comprehension, Picture Naming, and Picture Story range from 0–12 points, 0–40 points, and 0–8 points, respectively. However, for all further analyses, standard values of the SET 5–10 (T-scores) were used…. In the current sample, internal consistencies were \\(\\alpha_{LanguageComprehension}=0.94\\), \\(\\alpha_{PictureNaming}=0.80\\), and \\(\\alpha_{PictureStory}=0.74\\). In order to gain a broader verbal competence score, a global score was built, based on the average of the three subtests. The internal consistency of the global score was Cronbach’s \\(\\alpha=0.78\\). Naturally, you are not working with children in this class, so your Materials sub-section will not be nearly as long. It would be more like the first example above. The next sub-section of the Method section is normally Design and/or Analysis, or both. This is covered next. 14.2.3 Design and Analysis This sub-section briefly outlines the research design and the statistical analyses that will be used. But see the note below: NOTE: At this point of the class, we have not really discussed the inferential statistics that you need to describe here. Therefore, you cannot be expected to write all of this sub-section well. So we will not require you to write the entire sub-section. Rather, for Writing Assignment 2, we will only require you to write about how you combined your three outcome variables into a single construct and used Cronbach’s alpha to evaluate the result. You will need to include how you reverse coded any items here as well. If on top of that you had outcome variables on different scales, and therefore needed to transform your outcome variables into z-scores before averaging or summing them, then you would also need to include that in this assignment. We will have you add the part about statistical analyses in the final paper (Writing Assignment 4). There is more information on this below in section 14.5. Sometimes this sub-section simply goes by Design, Analysis, Data Analysis, or Variables. Each of these labels can also appear as combined or separate sub-sections in the Method section. That is, the labels that appear in Method sections can be quite flexible, depending on the nature of the study. Another factor that affects sub-section labeling is the extent to which researchers need to spell out each sub-section. Often, if a sub-section is not going to occupy very much space at all, it makes sense to combine it with another sub-section. However it’s organized, this sub-section (or these sub-sections) can be quite technical. And this is because it lays out for the reader, reviewers, and researchers who might want to replicate the experiment if and how variables were created (e.g., averaged into a composite), or managed (e.g., transformed), and then analyzed. We provide some examples below, but we do not expect you to understand all of it yet, since all of it includes the statistical analyses that we have not yet covered in class. 14.2.3.1 Examples of Design and Analysis The first example is from Dekker, Lee, Howard-Jones, and Jolles (2012). This paper is on the same topic as the paper by MacDonald et al. (2017) referred to above in section 14.2.1.1, but Dekker et al. was a study on educators in the UK only, and pre-dates MacDonald et al. Data Analysis The data was analyzed using the Statistical Package for the Social Sciences (SPSS) version 17.0 for Windows. For all analysis, a statistical threshold of \\(\\alpha=0.05\\) was used. Independent t-tests were performed to examine differences between countries (independent variable) in percentage of neuromyths and percentage of correct responses on general statements (dependent variables). To examine which factors predicted neuromyths, a regression analysis was performed for percentage of myths (dependent variable) with country, sex, age, school type (primary/secondary school), reading popular science, reading scientific journals, in-service training, and percentage of correct answers on general assertions (predictors). A second regression analysis was performed to examine the predictors of neuroscience literacy. Percentage of correct answers on general assertions was the dependent variable, and predictors were country, sex, age, school type (primary/secondary school), reading popular science, reading scientific journals, and in-service training. Although you perhaps do not understand what t-tests and regression analyses are yet, you can certainly tell which variables are outcome variables and which are predictor variables. Interestingly, these authors switch back and forth between the terms independent variable and predictor. Switching like this is not generally recommended, but you can see that it does happen. Just remember that dependent variable = outcome variable and equivalently, independent variable = predictor variable. The example below is from Christophel and Schnotz (2017). They used a survey to examine how men and women differ in how they approach online STEM-related coursework. It is not an ideal example of Design or Analysis. In fact, it is named Variables and Materials. However, we include it here because it is rather close to what you will need to do in your own Design sub-section: namely, 1) describe the statistical parameters of your design (i.e., the composite outcome and individual, predictor variables); and 2) report on how you constructed your composite variable from the three outcome variables. One difference though is that you would really only report one Cronbach’s alpha, namely: the one that assesses the composite outcome variable, consisting of the three outcome variables you collected in your survey. You might also report on z-scores, if you needed to create them before averaging/summing the outcome variables into a composite. We will provide you with a more relevant example of how to write your own papers (including this sub-section) in Appendix C. Variables and Materials We used gender (female vs. male), strategic competences, and arithmetic-operative competences as independent variables and mental effort and situational interest as dependent variables. The values for Cronbach’s Alpha were assessed with 214 participants from all studies within the OML project (see values below). To assess the strategic competences, we used three scales (planning, regulation, and monitoring) of the Kieler learning strategy inventory… which has a four-step answer format (example item: “If I prepare myself/learn I make myself a list with the important things and learn it afterwards”). The value for Cronbach’s Alpha was 0.88. The arithmetic-operative competences were assessed with two task packages from the Berlin intelligence structure test… which challenge students to solve mathematical problems in a fixed time frame, moderated by an experimental assistant (example item: “A worker earns €15.20 per hour. How much does he earn if he works 5 h?”). The value for Cronbach’s Alpha was 0.99. Mental effort was collected with an item of Paas et al. (1994) and an item of Wagner (2013). These items capture students’ judgment of the amount of effort required and of the task difficulty (example item: “How much did you exert yourself while solving the learning environment?”). Both items included a nine-step answer format. The value for Cronbach’s Alpha for mental effort was 0.56 and, therefore, low; this might be because the scale measures both estimated effort and perceived difficulty. The situational interest of students in the completion of the learning environment was explored with adapted items from the questionnaire in order to capture the actual motivation in learning and performance situations… The value for Cronbach’s Alpha was 0.81. The FAM questionnaire included a seven-step answer format (example item: “In this learning environment I like the role of the scientist who discovers relationships”). You may have noticed that authors wrote out Cronbach’s Alpha in its entirety instead of using the greek letter \\(\\alpha\\). This is fine for your papers as well, unless you really want to play around with your word processor by inserting special characters. 14.2.4 Procedure This sub-section of the Method section describes what the researchers did between designing the study and analyzing the data, and how the participants interacted with the materials during the study. In other words, what happened during the data-collection phase. This is important to reviewers and wise readers since often the order in which participants carry out tasks, or even the lighting conditions in a room, can sometimes plausibly make a difference in the outcome of an experiment. Be sure to include what participants were doing from beginning to end, and any role in the process that the researchers played. Naturally, you do not want to provide excruciating details here. Rather, provide enough so that a serious scientist could replicate what you did. 14.2.4.1 Examples of Procedure The example below by Colzato, Ozturk, and Hommel (2012) is from a study on how different styles of meditation affect thinking. Procedure and Design The participant and the coach laid on two separate mats (at a distance of about 1 m) on the floor; half sitting with the back against a back-jack. Eyes were closed in all three conditions. The same instructor, certified in Samatha, Mindfulness, and TB [Transformational Breath] training, provided the instruction for all three sessions. Participants served in three 45-min sessions separated by 10 days. In one session they performed under the supervision of a certified meditation coach the FA [focused-attention] meditation (35 min) and completed for 10 min (5 min each) a short version of the RAT [Remote Association Task (Convergent Thinking)]… and the AUT [Alternate Uses Task (Divergent Thinking)]… In the other two sessions the method was the same except that participants performed the OM [open-monitoring] meditation and the baseline session and completed new items of the RAT and AUT. The order of these three types of sessions was counterbalanced across participants by means of a Latin square. The RAT and AUT were scored by two independent readers blinded to the experimental conditions. [more details on the RAT and AUT] Statistical Analysis Mood scores and five measures (from the two tasks) were extracted for each participant: originality, fluency, flexibility, and elaboration scores from the AUT, the number of correct items from the RAT. All measures were analyzed separately by means of repeated-measures ANOVAs with Session (OM vs. FA vs. BA) as within-subjects factor and order of Session as covariate (in order to account for possible order effect). A significance level of p &lt; 0.05 [sic]122 was adopted for all tests. The example below of this kind of sub-section is from Kachel, Steffens, and Niedlich (2016), who were trying to establish a build a faster, more reliable tool to assess gender roles. The instrument employed was a series of survey questions from various test batteries. This would be more typical of your papers in this class. Procedure Participating students were tested at the University of Trier in a lab cubicle equipped with an iMac. The participants recruited via the snowball technique were tested individually in their homes or offices (as they wished) using an iBook. The instructions, the implicit tests, and the questionnaires were presented by a self-composed HyperCard computer program. Initially, participants were asked to report their age, educational background, and size of hometown. Then, they started with the IAT. IAT task order was held constant because of the correlational nature of the study…. All participants did the self-masculine/others-feminine task first. After the IAT, the questionnaires were presented in the order described in the Materials Section—accordingly, data for the TMF was collected before all other scales. Finally, participants were debriefed and thanked. In your own write-ups, you would be describing how the participant received the survey, and what the participant saw on the survey, starting at the beginning. 14.3 Outside help on Method sections The most extensive treatment we have found on Method sections is here from Verywell Mind, the psychological branch of the Verywell website hosted by Dotdash (formerly About.com), and associated with the Cleveland Clinic. The nicest, bullet-point style summary we have seen of what goes into a Method section is in Method portion the table linked here. This is part of a short paper that was written for health-care researchers (Perneger &amp; Hudelson, 2004) about how to write IMRaD papers in general. You can find it here, where you can scroll to the discussion of the Method section. The following link from The Visual Communication Guy has a nice list of what to do and not to do in IMRaD papers. The part on the Method section is informative. 14.4 Practice writing exercise 1 Form working dyads (or triads) like you did for the last assignment of this type (section 13.5). However, this time, work with someone from your own research group, if you can. NOTE: If your research group consists only of three people, you may want to work in a triad. Otherwise, one of you will end up in another group, which can be done, but it is not as convenient. Now come up with a unique nickname for your group and open a Google Doc as you did then, but give it the following name: [YOUR GROUP NICKNAME] Chapter 14 Exercise 1. Then share the document with the other members of your working group, as well as your TA. You are now ready to start the practice writing exercises for the Method section. 14.4.1 Participants As explained above in section 14.2.1, one thing that this section does, if the participant group consists of humans, is to report basic demographics. This usually means the numbers of each sex in the sample of participants, as well as their mean age (and standard deviation). But beyond that, the best way to think of this sub-section of the Method section (and, indeed, all the sub-sections that follow it) is as a road map to another researcher who might want to replicate the study. 14.4.1.1 Brainstorm on participant reporting Take a few minutes and brainstorm with your working-group partners on what elements of participant description would be useful to an outside researcher who wanted to replicate123 the study. Type in your ideas under the header Ideas for Participant sub-section. Discuss this as a class once you are done brainstorming. 14.4.1.2 Write a participant sub-section for your data Now look at your own data (or the data from one of your research groups if you’re not all from the same research group). You probably have the basic statistics you need from jamovi for the Method section at this point, so refer back to that. If not, you can fabricate the statistics based on what you expect. Together, write out a Participant sub-section for your data. Naturally, you must include information about sex and age, but also include anything else about your sample that your group and the class in general brainstormed in the exercise above (section 14.4.1.1). 14.4.1.3 Feedback across working groups As you did the last time you engaged in this kind of activity with the Introduction, someone from your group should raise their hand. Look for another working group that also has a hand up (this should not be another pair from within your research group unless that is the only option). Get their university emails and share your Google Doc with them electronically (i.e., share it with them in Google Drive). When you add them, add them with commenting (Can comment) privileges. They will reciprocate, giving you commenting privileges on theirs. Read that group’s sentences (as they will yours). Give the other groups helpful comments with respect to what you think might be helpful for you if you had to replicate their study on a new sample of participants (from the same population). Also feel free to offer comments about the organization of information in this sub-section. Finally, make sure you read the note after the next paragraph. When finished commenting, place a final comment near the end of your counterpart working group’s sentences that says, “We are done commenting.” NOTE: When you comment, focus on content and organization of the sub-section. Try to steer away from commenting on little details like whether standard deviation should be represented as SD or s, or whether something should be in italics or boldface. The TAs will be grading your final draft, ultimately, on what you include (content), and how you organize your information. But they will not be grading you on APA conventions at this point (whereas they will in the Results section in the next writing assignment). For now, they may add non-evaluative notes for you on your final draft to help you along in this regard, but for now, we are more concerned with what you are writing and how you are organizing it. 14.4.1.4 Class review As a class, discuss any issues that came up that might benefit from an expert’s perspective (that of your TA). 14.4.1.5 Revise subsection Take a few minutes to revise any part of this sub-section that you think should be changed. Resolve comments where possible. You should end up with a final, polished sentence that you would be comfortable using later on as a model for your own Participant subsection. WARNING: Do not re-use the wording of this document for your own papers! Once it is written in lab, it is considered the paper of someone else for the purposes of this class. Copying it for your own writing will thus be considered a form of plagiarism. Moreover, do not copy this document and try to change words progressively until it looks like you have a different document. This is, in fact, the most dangerous form of plagiarism, because if you get caught, there is evidence of you not only plagiarizing, but also evidence of you knowing what you were doing was wrong. This is checkmate (I assume) as far as the Aggie Honor System Office is concerned. Please read their section on plagiarism. There is also a handout in eCampus under the appropriate folder titled “Types of Plagiarism (from Turnitin).” This is a more detailed set of descriptions for different kinds of plagiarism. The changing-a-few-words strategy is listed there as number 3 (“Find - Replace”). It is our opinion that this kind of plagiarism is much worse than indicated by Turnitin in the sense that it is also quite “self-incriminating.” The best way to use the current lab-based document as a template for your own paper is to read each section for the content. Then distract yourself for a little bit with an unrelated task (e.g., making a cup of coffee, washing a couple of dishes). Return after a couple of minutes and try to re-write what you just read without looking at it. There is actually psycholinguistic evidence (e.g., Sachs (1967)) that our memory cannot hold on to what was said exactly for very long, but we can hold on to the meaning of what was said for quite some time. You can take advantage of this in your own writing to get around this sort of plagiarism. 14.4.2 Materials, Design, and Procedures Repeat the sequence above for the following sub-sections: Materials, Design, and Procedures. There are a few notes below as to how the activity might change a little by section. 14.4.2.1 Materials The only thing different about this section is that it will be far more extensive than participants. This is because you administered a survey. Therefore, what you put on the survey becomes extremely important to anyone who wanted to replicate your experiment. However, restrict yourself to the survey itself. If there are changes to the raw data later on (e.g., transformation, composite variables), then they would go in the Design subsection. 14.4.2.2 Design The unique part of this section is that we are not going to ask you to present what specific statistical tests you intend to perform. Since those come up later in the course, we cannot expect you to spell them out here. However, in an authentic section such as this (which might be named Design and Analysis), you would spell this out as part of your research plan. 14.4.2.3 Procedures There is nothing unusual about this section relative to the others. The way to think about it is as a walk-through. Your reader, who could have in mind replicating your experiment, should be able to visualize what happened during the study, mostly from the participant’s perspective, but also partly from the researcher (to the extent that they were directly involved in the data-collection itself). In a sense, it’s what happened between the materials development and the data analysis. 14.5 Writing Assignment #2 This is part of the main writing assignments in your lab. It is the second part (Method) of the complete IMRaD research paper that you will write. 14.5.1 General Instructions Unlike Writing Assignment #1 (the Introduction), this second assignment will be more authentic. That is, since you will have both your data and your research designs available, it should be no problem to write out a fully fleshed-out Methods section. Like the Introduction in Writing Assignment #1, it should be about 2 to 3 additional pages long, double-spaced, Times New Roman, 12 font. NOTE: There is an example of a Method section in Appendix C. It turns out to be 3 pages long when it is double-spaced. This example was based on the survey and data from one of the research groups formed in PSYC 301 for Spring 2019 (Fettig, López Fuentes, &amp; Villarreal, 2019). But unlike the Introduction, this section does indeed get a header: Method (singular, not plural). It will also have sub-headers, as will be explained below. NOTE: Remember, do NOT put your name on this assignment. It will be submitted for anonymous peer review through Peerceptiv, and including your name makes it non-anonymous. Your TAs will know exactly who you are since it is submitted electronically, and Peerceptiv indicates who you are (to the TAs and the instructor only). You do not even need to put your name on it for the final draft. We will know who you are. Finally, for the peer-review (Peerceptiv) assignment you should just add this section to the current version of your Introduction. It will be easier to review if the reviewer (i.e., your peers and your TA) have access to your Introduction as well. Otherwise, they will be “operating in the dark.” 14.5.2 Specific Instructions Your methods section should contain four sub-sections with the following sub-headers: Participants, Materials, Design, and Procedure. Each of these sections should contain the details spelled out in their respective section above: Participants: see section 14.2.1 Material: see section 14.2.2 (specifically the lettered list A-G) Design: see section 14.2.3 Procedure: see section 14.2.4 As was indicated above (in a note at the beginning of section 14.2.3), we do not expect you at this point to be able to spell out the particular statistical analyses that will be used throughout the rest of your paper.124 For now, you will detail the particulars of the design, but not the statistical analysis. A good strategy for this paper would be to explain in as much detail as possible, the details of each section. We did note at the beginning of this chapter (section 14.1) that in a more elaborate study, it is easy to over-do the Method section in terms of detail. That is not really possible in your case, unless you start going in to painful detail about every single step you took in the Procedure sub-section (try to keep that reasonable). In addition to the example Method section in Appendix C [based on Fettig, López Fuentes, and Villarreal (2019)], there is also a rubric for this assignment in Appendix D. The upload instructions and tips detailed for Writing Assignment #1 (located in section 13.7.3) also apply here in exactly the same way. Please review those. For an explanation of why replications are rarely published, enter either the expression publication bias or file drawer effect into a search engine.↩︎ Note that replicate does not mean reproduce verbatim. Rather, it technically refers to performing the same study (or nearly the same study) on a new sample of participants drawn randomly from the same population.↩︎ Naturally however, it would be more appropriate to use Subjects if the source of your data were entities who were less capable of active consent or assent, like monkeys, rats, neurons, etc.↩︎ Incidentally, this would be a pretty typical write-up for the experiments that you might participate in when you are recruited through any Psychology department’s participant pool.↩︎ Note that the significance level is set as \\(\\alpha=.05\\), not \\(p&lt;.05\\)↩︎ Note that being able to “replicate” the participants really refers to having sufficiently accurate information to draw participants randomly from the same population as in your study. So you are trying to provide information about your population of participants, which could be affected by the way you sampled them (e.g., convenience sampling, snowball sampling, etc.).↩︎ However, if you are curious, they are as follows: 1) an independent-samples t-test for your two-level predictor variable; 2) a One-Way ANOVA for your 3-5 level predictor variable; and 3) a correlation and a simple linear regression for your two continuous predictor variables (your choice as to which variable goes with which analysis).↩︎ "],["WritingResultsSections.html", "Chapter 15 Writing Results sections 15.1 A note on Writing Assignments 3 and 4 15.2 Structure of Results sections 15.3 Practice writing exercise", " Chapter 15 Writing Results sections 15.1 A note on Writing Assignments 3 and 4 First, it is important to realize that the Results and Discussion sections are separate sections of an IMRaD paper. However, unlike Writing Assignments #1 and #2, Writing Assignment #3 (detailed in section 16.3 at the end of Chapter 16) requires you to write both a Results and a Discussion section. In short, Assignment #3 will subtitled Study 1, and will include the Results and Discussion sections for your t-test and your ANOVA analyses (and Assignment #4 in turn, subtitled “Study 2,” will consist of your correlation and regression analyses). This will be explained further in Chapter 16. For now, the current chapter will focus on how Results sections are structured. It will end with some links to outside resources on writing these sections, as well as a series of writing activities. In short, the Results section simply reports the results of whatever analysis you carried out. Much, much, much more often than not in Psychology, this means reporting your statistical analyses. 15.2 Structure of Results sections To be honest, the structure of a Results section is minimal, pretty much only providing a separate paragraph or two for each relevant analysis. That said, the section is also very formulaic (meaning that most Results sections go about conveying statistics in very, very similar fashion) The principle requirements in writing this section are five-fold: Creating a separate paragraph or two for each analysis; Reporting the statistics in appropriate APA format; Properly formatting tables; Properly formatting figures; and Making sure that the primary text is in plain English To add to point #5 above, it would be incorrect to think that one could write out a Results section in bullet-point format. That is not allowed. The Results section contains legitimate paragraphs, with real sentences. However, embedded in those sentences (often between commas or within parentheses or square brackets) are statistical results as numbers (e.g., “\\(t(32)=4.25,p&lt;.05\\)”). But a reader should be able to cross out the numbers and read the sentences as normal, human sentences. We will exemplify this below with one of the three passages. 15.2.1 Examples of Results sections 15.2.1.1 Example 1 Here is an example from Experiment 1 of Ward and Wegner (2013), who were looking at mind-blanking, or “an extreme decoupling of perception and attention, one in which attention fails to bring any stimuli into conscious awareness” (p. 1). Experiment 1 here is about establishing mind-blanking as a mental state distinct from other mental states, namely: mind-wandering and being focused. Participants were probed with questions about their mental states at random while reading from the first six chapters of Leo Tolstoy’s War and Peace. Results Participants reported experiencing all three mental states: mind-blanking, mind-wandering, and being focused, with a mean response time for each mental state probe of 3.14 s. Means and standard deviations for each mental state are presented in Table 1 [represented in figure 15.1 below]. Each mean represents the number of times each mental state was reported out of a total of 6 possible probes. Each percentage represents the percentage of mental state probes that were responded to with an answer of “yes,” indicating that the identified mental state was being experienced immediately prior to the appearance of the probe. Each probe asked about only one mental state at a time; thus, any answer of “no” could indicate that the participant was experiencing either of the two other mental states (e.g., a participant answering “no” to a focus-probe could have been either mind-blanking or mind-wandering). Figure 15.1: (Not a figure, but rather a table) Table 1. Means and SDs of conscious states in Experiment 1, from Ward &amp; Wegner (2013) We also tested for correlations between mental states. Mind-wandering was negatively correlated with being focused, \\(r_{(23)}=-0.71,p&lt;0.001\\). Mind-blanking, on the other hand, was uncorrelated with both mind-wandering, \\(r_{(23)}=-0.06, p=0.80\\), and being focused, \\(r_{(23)}=-0.26, p=0.24\\). Irrespective of the quality of this study, there are a few key points worth noting. First, notice that this Results section begins with a summary of certain general statistics that may or may not impinge on other findings. For instance, in the case here, if the researchers here had, in fact, found that there were differences in the average time it took for participants to evaluate each probe type, then that might suggest that the the different mental states are not equally available for introspection, or that that the various states do not have clear definitional boundaries. Second, key statistics are summarized in either a table or a figure. These help the reader organize information quickly. In the past when graphical options were either difficult to produce (mid 20th century and before), or limited in sophistication (latter half of the 20th century), tables were resorted to quite commonly. However, good graphics are now fairly easy to use. That, coupled with the fact that humans are better at taking information in visually, one should first resort to a graph if possible, and only resort to a table when the information does not lend itself to graphing in either complexity (i.e., too simple or too complex) or importance. I suppose here, the researchers thought that the information was too simple to waste time producing a graph. (Also note that tables and figures have specific formatting guidelines that are not transparently adhered to in the Frontiers in… collection of journals. See section 15.2.2 below for more information on formatting tables in APA, and section 15.2.3 for figures.) Third, note that there are statistical tests reported. This is a sine qua non of the Results section if there there are statistics in your study. Consult section 12.4.1.3.2 for more details here on how to report particular statistics. Fourth, notice how all statistics are reported between or after commas. The reason for this is that the reader should be able to mentally “delete” the reporting of statistics and read the rest as prose. Commas are an option here, as well as parentheses (which are a more explicit convention to achieve this effect). Parentheses are used more explicitly to this effect in the examples below (sections 15.2.1.2 and 15.2.1.3). One interesting, but quite minor thing about the report above is that the researchers subscripted the degrees of freedom (e.g., “\\(r_{(23)}=-0.26, p=0.24\\).” This is a convention that you see often outside of Psychology (apparently among statisticians themselves), and sometimes within Psychology.125 But technically, it is not APA format as it is not specifically mentioned in the APA style guide at all. See the examples below, where the researchers do not subscript the degrees of freedom. In sum, do not subscript your degrees of freedom, unless your professor (or a journal) asks you to. Also note that this is about the most minor quibble one might bring up in APA formatting. 15.2.1.2 Example 2 Below is the Results section from a study by Schroeder and Epley (2015) that looked at how fake and real job recruiters evaluate written vs. spoken job-application pitches. That is, fake job applicants gave spoken and written pitches on why they should be hired These pitches were then evaluated by either fake or real recruiters as to their quality. No single recruiter received both the spoken and written pitches from the same candidate. Below, we provide the Results section of the final study (Experiment 4) with 39 professional recruiters. Results The pattern of evaluations by professional recruiters replicated the pattern observed in Experiments 1 through 3b (see Fig. 7). In particular, the recruiters believed that the job candidates had greater intellect – were more competent, thoughtful, and intelligent – when they listened to pitches (M = 5.63, SD = 1.61) than when they read pitches (M = 3.65, SD = 1.91), t(37)126 = 3.53, p &lt; .01, 95% CI of the difference = [0.85, 3.13], d = 1.16. The recruiters also formed more positive impressions of the candidates—rated them as more likeable and had a more positive and less negative impression of them – when they listened to pitches (M = 5.97, SD = 1.92) than when they read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p &lt; .01, 95% CI of the difference = [0.55, 3.24], d = 0.94. Finally, they also reported being more likely to hire the candidates when they listened to pitches (M = 4.71, SD = 2.26) than when they read the same pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p &lt; .01, 95% CI of the difference = [0.41, 3.24], d = 0.86. Figure 15.2: Fig. 7. Results from Experiment 4: professional recruiters’ ratings of the job candidates’ intellect, their general impressions of the candidates, and their likelihood of hiring the candidates. Results are shown separately for the audio and transcript conditions. Error bars represent ±1 SEM. … Finally, the recruiters did not spend significantly different amounts of time engaging with the stimuli in the audio condition (M = 173.85s, SD = 145.97) and the transcript condition (M = 137.86s, SD = 197.49), t(37) = 0.65, p = .52, d = 0.21. Although 6 recruiters did not complete the memory test, those who did complete it wrote similar amounts about the candidates in the audio condition (M = 53.94 words, SD = 30.06) and the transcript condition (M = 58.67 words, SD = 15.07), t(31) = −0.55, p = .59, d = 0.20. There are a couple of interesting characteristics of this particular write-up that should be noted. First, as noted previously, one should present statistics in either parentheses or between commas. This is so that the prose that remains is readable as normal text. Below, we present the first paragraph in the example above with the parentheticals and comma-offset sections replaced with ellipses [i.e., …]): The pattern of evaluations by professional recruiters replicated the pattern observed in Experiments 1 through 3b… In particular, the recruiters believed that the job candidates had greater intellect – were more competent, thoughtful, and intelligent – when they listened to pitches… than when they read pitches…. The recruiters also formed more positive impressions of the candidates—rated them as more likeable and had a more positive and less negative impression of them – when they listened to pitches… than when they read pitches… Finally, they also reported being more likely to hire the candidates when they listened to pitches… than when they read the same pitches… Second, notice that the figure in this case isn’t allowed to “speak for itself.” More broadly, the rule is that no table or figure can be “dropped in cold” to a manuscript. If you insert a table or a figure into a manuscript, that table or figure is mentioned nearby in the main text (as well as described, preferably). Finally, notice that they report statistics pretty comprehensively. Namely, not only do they report the descriptive statistics in parentheses: (M = 5.63, SD = 1.61) but then in addition to the test statistics itself, they also report the 95% confidence intervals (between square brackets) and effect sizes: t(37) = 3.53, p &lt; .01, 95% CI of the difference = [0.85, 3.13], d = 1.16. 15.2.1.3 Example 3 The following (abridged) Results section comes from Experiment 2 in James et al. (2015). In this study the researchers tested the promise of a therapy in which potentially intrusive memories of a experimental traumatic event (a 12-minute traumatic film) are (hopefully) blocked by means of pairing reactivation of those traumatic memories (of the film) with a visuo-spatial task (Tetris) that requires a relatively heavy allocation of working memory. There were two groups initially: an experimental group that watched the films and received the therapy, and a control group that was identical but did not receive the therapy. Each group was tested in several ways on intrusive-memory activation, once at 24 hours after the film viewing, and once at 7 days after the film viewing. After having found that the therapy worked in this limited context, the researchers wanted to to test whether either the intrusive-memory-activation task or the Tetris task had independent effects on the outcomes. Thus, they added two more groups: a Tetris-only group, and a reactivating-only group. They compared the four groups using One-Way ANOVAs. They reported their results as follows: Results Groups were matched at baseline for age and gender, as well as self-report-questionnaire scores for trait anxiety, depression, and trauma history. Mood deterioration during film viewing, postfilm distress, attention to the film, demand ratings, and diary compliance were also matched (see the Supplemental Material). Intrusive memories preintervention First, prior to the intervention (over the first 24 hr after viewing the film: Day 0), we confirmed that the four groups experienced a similar number of intrusive memories, F(3, 68) = 0.16, p = .92 (Fig. 4a). Figure 15.3: Fig. 4. Results from Experiment 2: mean number of intrusive memories recorded in the diary during the first 24 hr following viewing of the experimental trauma film (i.e., preintervention; a), mean number of intrusive memories recorded in the diary across totaled over the 7-day period after the intervention (b), and mean score on the intrusion-provocation task on Day 7 (c). In each graph, results are shown separately for the four groups. Asterisks indicate that results for the reactivation-plus-Tetris group were significantly different from results for the other three groups (*p &lt; .01). Error bars represent +1 SEM. Intrusive memories postintervention Second, and critically, for the 7-day diary postintervention, there was a significant difference between groups in overall intrusion frequency in daily life, F(3, 68) = 3.80, p = .01, \\(\\eta_p^2=.14\\). (Fig. 4b). Planned comparisons demonstrated that relative to the no-task control group, only those in the reactivation-plus-Tetris group, t(22.63) = 2.99, p = .007, d = 1.00, experienced significantly fewer intrusive memories; this finding replicated Experiment 1. Critically, as predicted by reconsolidation theory, the reactivation-plus-Tetris group had significantly fewer intrusive memories than the Tetris-only group, t(27.96) = 2.52, p = .02, d = 0.84, as well as the reactivation-only group, t(25.68) = 3.32, p = .003, d = 1.11. Further, there were no significant differences between the no-task control group and the reactivation-only group, t(32.23) = 0.22, p = .83, or between the no-task control group and the Tetris-only group, t(30.03) = 1.01, p = .32. Third, a similar pattern was seen on a convergent measure—the frequency of IPT intrusions on Day 7 in the laboratory, for which there was an overall significant difference between groups, F(3, 68) = 5.57, p = .002, \\(\\eta_p^2=.20\\) (Fig. 4c; Day 7). Planned comparisons showed that there was a significantly lower intrusion score in the reactivation-plus-Tetris group compared with the no-task control group, t(68) = 2.92, p = .005, d = 0.97, which replicated the results of Experiment 1. Critically, in line with reconsolidation theory, the reactivation-plus-Tetris group also differed significantly from both the reactivation-only group, t(68) = 3.92, p &lt; .001, d = 1.31, and the Tetris-only group, t(68) = 2.56, p = .01, d = 0.85. Further, the reactivation-only group, t(68) = 1.00, p = .32, and the Tetris-only group, t(68) = 0.36, p = .72, did not differ significantly from the no-task control group. The example above was chosen because is it is an example of a One-Way ANOVA (an F-test) with planned comparisons as follow-ups (t-tests). This corresponds to what you will be doing with your ANOVA (the nominal, predictor variable with more than two levels). Notice that some of the degrees of freedom have decimal values [e.g., t(22.63)…]. This has to do with choosing t-tests that corrected for heterogeneous variance (e.g., Welch’s t). It is also a good example of how figures are discussed in text. Figure 4 in the original (reproduced here as figure 15.3) comes in three parts. Each part (4a, 4b, and 4c) is described in the text immediately surrounding the figure. 15.2.1.4 Examples from earlier Earlier in this lab manual in the chapters on specific statistical tests, we provided you with examples of Results sections. For the independent-samples t-test, you can see examples in sections 6.1.1.4 and 6.1.2.5 in Chapter 6. 15.2.1.5 Example in Appendix E Finally, there is an example in Appendix E of a Results section, written with the same data as the Method section in Appendix C (Fettig, López Fuentes, &amp; Villarreal, 2019). 15.2.2 Formatting tables in APA For now, you do not need to know that much about formatting tables properly in APA. The only reason for this is that the output from jamovi is close enough. To export the table from jamovi, right- or control-click the table itself, and choose Table &gt; Copy. Then you can paste it right in to your word processor. See figure 15.4 below. Figure 15.4: How to copy a table from jamovi. However, the table caption is not available as part of the pasted object (nor as part of the image, if you choose Table &gt; Export, an option that is less word-processor friendly). To insert a caption in MS Word, select the table, then choose Insert &gt; Caption. Table captions go above the table, so make sure that the Position option is set to Above selected item. None of this seems to work with the online version of Office 365. To do this in Google Docs, you need to install an Add-on called Captionizer. Once you install it, you put the cursor somewhere inside the table, go to Add-ons &gt; Captionizer &gt; Tables &gt; Number this table. The caption will appear at the top where it should be. Give the table a caption title that describes well what the table contains. In the table embedded in figure 15.1 above, the caption is quite simple, but adequate and to the point: Table 1. Means and SDs of conscious states in Experiment 1. 15.2.3 Formatting figures in APA As was the case with tables, there is no reason to go in to too much detail about APA formatting guidelines for figures; the output from jamovi is sufficient. One difference is that it seems it’s better to export (to file) an image of the figures in jamovi rather than copy-and-pasting them. One then inserts an image into the word processor. To do this, simply right- or control-click the image in jamovi and select image &gt; export. Find a file location on your computer that you will remember, and select .png for a file format, and give the file a name (e.g., image1.png). This stage is depicted in Figure 15.5 below. Figure 15.5: How to export an image from jamovi. In MS Word, choose Insert &gt; Pictures &gt; Picture from File. Then find the location where you saved the image and double-click it. In Google Docs, choose Insert &gt; Image &gt; Upload from computer, and double-click the image. For captioning, nearly the same procedures apply as above. In MS Word, select the image and choose Insert &gt; Caption. But this time make sure that the Position option is set to Below selected item instead of Above. In Google Docs, select the image and go to Add-ons &gt; Captionizer &gt; Images &gt; Number selected image. The default is to put the caption below the image. However, it is also the default to label it image 1. You can either write over the word Image with Figure, or change the default. To change the default, choose Add-ons &gt; Captionizer &gt; Options. Under Options for Images, change Image to Figure. Then click Make default at the bottom. As far as the title of the caption is concerned, the same simple rule applies: simple and to the point. Figure 7 from Schroeder and Epley (2015), reproduced as Figure 15.2 above, is more complex than the much simpler table (figure 15.1) from Ward and Wegner (2013): Fig. 7. Results from Experiment 4: professional recruiters’ ratings of the job candidates’ intellect, their general impressions of the candidates, and their likelihood of hiring the candidates. Results are shown separately for the audio and transcript conditions. Error bars represent ±1 SEM. However, it abides by the same rules. All of the critical information in the figures is described in as concise a manner as possible. The same is true in the caption from Figure 4 (embedded in Figure 15.3) from James et al. (2015). This is an even more complex caption. However, we will not reproduce it here. The reader can refer back to it. 15.3 Practice writing exercise This writing exercise relates to part of Writing Assignment #3 in your lab (see section 16.3), but just the first part: Results. 15.3.1 Forming groups Form working dyads (or triads) like you did for the first two exercises of this type. Work with someone from your own research group again, if you can. Come up with a unique nickname for your group and open a Google Doc as you did before, but give it the following name: [YOUR GROUP NICKNAME] Chapter 15 Exercise. Then share the document with the other members of your working group, as well as your TA. Shortly, you will be ready to start the practice writing exercises for the Results section. But first, you are going to need to get some statistical output ready (in order to have something to write about). 15.3.2 Get the statistical output ready The first thing you will need to do here is to get your data analyzed in jamovi. 15.3.2.1 Get your data Recall that you have two nominal predictors in for your data: one with two levels and one with three or more levels (preferably not more than 5 levels). The former will be used for the independent-samples t-test, and the latter for the One-Way ANOVA. In this analysis, your outcome variable for both the t-test and the One-Way ANOVA is the composite variable that you calculated. If you were not able to calculate a composite variable, then you can use one of your three outcome variables for this writing exercise. 15.3.2.2 Do your analyses Conduct an independent-samples t-test as spelled out in section 6.1 in Chapter 6. Then conduct a One-Way ANOVA as illustrated in section 8.1.1 of Chapter 8. For the t-test, use Welch’s option that does not assume equal variances. In the ANOVA, test for equality of variances using Levene’s test. Also be sure to include post-hoc tests for the ANOVA. Use the Holm correction. Finally, include not only descriptives, but also plots for both the t-test and the ANOVA. jamovi will give you dot plots with error bars. These are superior to bar charts (even with error bars) since they take up far less unused space. Note that the limits of the “error bars” on the dot plot that jamovi provides actually form the confidence interval. It is common to refer to these as error bars. However, that term is slightly misleading since the bars themselves usually do not directly represent the standard error (which is sort of implied by the term), but rather the 95% confidence interval, which for \\(N&gt;30\\) is approximately \\(\\overline{X} \\pm 1.96 \\times SE_{mean}\\).127 15.3.2.3 Save your figures If you just created graphs, then you should save the output of those graphs to your hard drive. See section 15.2.3 for instructions on how to do this. Make sure you not only give thee files transparent names (e.g., TTestFigure.png and ANOVAFigure.png), but that you also remember where you put them. 15.3.2.4 Insert figures into document Next, insert those images into your Google Doc. Leave a few blank at the top, then insert the figure for the t-test. Then leave a few more blank lines, and insert the figure for the ANOVA. You can refer back to section 15.2.3 for instructions on how to do this, as well as how to insert a caption by installing the Captionizer Add-on in Google Docs. If a figure is not appropriate for some reason, you can insert a table instead using the instructions laid out in section 15.2.2. 15.3.3 Writing The rest of this writing exercise is actually about writing. But before you begin, type in the Results header at the top. 15.3.3.1 Simple description of the figures or tables Simply describe what the figure seems to be showing without referring to means, standard deviations, or any of the statistical tests. Be sure to mention the figures/tables by number when describing your data. In other words, generate a simple story from your data. Here are some suggestions: Report the results from the perspective of your participants (e.g., The participants reported having…, They showed a preference for…, etc.) Use hedged wording here (e.g., seems as if, looks like, etc.) Alternatively, let your variables “do the talking.” (this would be like examples 1 and 3 above in sections 15.2.1.1 and 15.2.1.3, respectively. Here, you would use wording such as The mean for group X on sleep satisfaction was lower than…, or As predicted, sleep satisfaction seemed to be different across the three groups, with group X showing…. Generally speaking though, readers find it more engaging when participants do things than when data “do” things. In this case, you can really exploit more active verbs like reported, showed, rated, indicated, felt, preferred, experienced, etc. The goal is to end up with a skeleton of the Results section that looks something like our comment at the end of example 2 above (section 15.2.1.2), the one where we inserted ellipses (…) instead of the statistics. You should begin with the two-level factor (the t-test) and then finish with the factor with 3+ levels (the ANOVA). You should refer to the figures/tables by their type and number, and those tables/figures should have captions (above tables, but below figures). If there are two figures, the figure for the t-test will be Figure 1 and the figure for the ANOVA will be Figure 2. The same is true if there are two tables. However, if there is one table and one figure, then you will have Table 1 and Figure 1. Tables and figures are enumerated separately in APA manuscripts. Everything you write here should be in formal, but simple English. Do not try anything fancy here. The Results sections already tends to be densely packed with information (which we will proceed to do later), but this plain-English part should be quite simple to read. 15.3.3.1.1 Feedback across working groups As before, someone from your group should raise their hand, and look for another working group that also has a hand up (this should not be another pair from within your research group unless that is the only option). Get their university emails and share your Google Doc with them electronically (i.e., share it with them in Google Drive). When you add them, add them with commenting (Can comment) privileges. They will reciprocate, giving you commenting privileges on theirs. Read that group’s passages (as they will yours). Give the other groups helpful comments with respect to how well their prose describes their figures (or tables) and how easy it is to directly connect their description with the exact figures/tables (i.e., do they explicitly refer to the figures/tables in their prose?). When finished commenting, place a final comment near the end of your counterpart working group’s sentences that says, “We are done commenting.” 15.3.3.1.2 Class review As a class, discuss any issues that came up that might benefit from an expert’s perspective (that of your TA). 15.3.3.1.3 Revise Take about 10 minutes to revise (as a pair) any issues that came up either in your counterpart-group’s comments or during class discussion. 15.3.3.2 Add statistical results Now, you are going to add some descriptive and inferential statistics to the results you have already written. 15.3.3.2.1 Add descriptive statistics Begin by providing the descriptive statistics, namely, means and standard deviations for each level of each variable. For example: Men showed a greater preference for violent video games (M = 4.5, SD = 1.2, n = 13) than women (M = 2.5, SD = .96, n = 21). Do this for every level of every nominal variable that you have. The reporting pattern will not vary here, by the way, only the statistics themselves. More specifically, you can choose among several ways to report these descriptive statistics, but you need to stick to that form throughout. Finally, note that the symbols M, SD, and n are all in italics. Example 2 above (section 15.2.1.2) above illustrates the points above. 15.3.3.2.2 Add inferential statistics Next, after each mention of any kind of comparison across groups, insert a statement as to whether the difference was statistically significant along with the the 95% confidence interval (itself expressed between square brackets), and the effect size (Cohen’s d). Here’s are two examples, one statistically significant; the other not: This difference, tested with an independent-samples t-test, was statistically significant, t(32) = 3.5, p &lt; .001, 95% CI of the difference = [0.95, 2.53], d = .55. or However, an independent-samples t-test showed that this difference was not statistically significant, t(32) = 1.56, p &gt; .05, 95% CI of the difference = [-1.5, 1.34], d = .55. Notice the use of “However” at the beginning. This was used since the main text had probably highlighted a difference in means (there almost always is). Thus, Men reported having more reservations about same-sex marriage. However, an independent-samples t-test showed that this difference was not statistically significant, t(32) = 1.56, p &gt; .05, 95% CI of the difference = [-1.5, 1.34], d = .55. You can also report these within parentheses or brackets, though using square brackets seems to be better since degrees of freedom are already reported in parentheses:128 Analyzed using an independent-samples t-test, this difference was statistically significant [t(32) = 3.5, p &lt; .001, 95% CI of the difference = [0.95, 2.53], d = .55]. Thus, our suggestion is that you use either commas or square brackets to set off the inferential statistics. The sentence structure can get redundant here, so try to mix the language up a bit. For example: The comparison between these groups was statistically significant [t(32) = 3.5, p &lt; .001, 95% CI of the difference = [0.95, 2.53], d = .55] using an independent-samples t-test. or This difference, compared using an independent-samples t-test, reached statistical significance [t(32) = 3.5, p &lt; .001, 95% CI of the difference = [0.95, 2.53], d = .55]. or An independent-samples t-test showed that these two groups were statistically significant [t(32) = 3.5, 95% CI of the difference = [0.95, 2.53], p &lt; .001, d = .55]. etc. Below is an example for an F-test (where the effect size is captured by a statistic called \\(\\eta^2\\), pronounced “eta squared,” where eta rhymes perfectly with beta): A One-Way ANOVA showed that there were one or more significant differences across the four groups [F(2,45) = 6.53, p = .02, \\(\\eta^2=.79\\)] NOTE: It may be difficult for you to find a way to type in \\(\\eta^2\\). Therefore, feel free to just write “eta-squared,” as in the following: [F(2,45) = 6.53, p = .02, eta-squared = .79]. But if you want to try, in Google Docs go to Insert &gt; Special characters, then type in eta into the search box. Choose the eta that doesn’t have diacritics attached to it, and click it. it should show up in your document. Then type a 2 adjacent to right of the eta symbol, highlight the 2, and go to Format &gt; Text &gt; Superscript. Speaking of F-tests, if you have a significant F-test, you will want to follow up any One-Way ANOVAs with subsequent post-hoc comparisons. This is covered in section 13.5 of Navarro and Foxcroft (2019). Below is an example of this: A One-Way ANOVA showed that there were one or more significant differences across the four groups [Welch’s F(2, 45) = 6.53, p = .02]. Post-hoc comparisons (using the Games-Howell correction for unequal variances) showed that freshmen were significantly more passionate about volunteer work than all the class levels above them (all ps &gt; .05 for all three comparisons), but that sophomores, juniors, and seniors were not different from each other at all (all three ps &gt; .05). However, if you do not have a significant F-test, then there is no need (or justification, for that matter) to run any multiple comparisons between the levels of that variable. Example 3 above (section 15.2.1.3, after the figure) provides an example of multiple comparisons after the omnibus,129 One-Way ANOVA. Note however, that the researchers there used something called planned comparisons, which are different from post-hoc comparisons. Navarro and Foxcroft (2019) cover this in a footnote in section 13.5.2, and then more extensively in Chapter 14, section 14.9. However, that write-up is still a good model. 15.3.3.2.3 Feedback across working groups Work with your counterpart group again. But this time, give the other group feedback on details, namely: Did they report the descriptive statistics in the right way? (see section 12.4.1.3.1) Did they report their inferential statistics in the right way? (see section 12.4.1.3.2) Is the text of the Results section readable as plain-English sentences if you delete (mentally) the statistics? That is, are all the statistics set off by commas, parentheses, or square brackets? When finished commenting, place a final comment near the end of your counterpart working group’s sentences that says, “We are done commenting.” 15.3.3.2.4 Class review As a class, discuss any issues that came up that might benefit from an your TA’s perspective. 15.3.3.2.5 Revise As a group, take about 10 minutes to fix any issues that came up either via the comments from your counterpart group, or during class discussion. In fact, there is an internally consistent logic behind subscripting degrees of freedom. In linear regression, it is common to use subscripts (even in Psychology) to refer to specific instances of whatever the subscript is attached to. For instance \\(X_{1_i}\\) would be the score on variable \\(X_1\\) (as opposed to variable \\(X_2\\)) for participant i (e.g., “Joe”). The degrees of freedom work the same way, in that they refer to specific distributions as defined by the degrees of freedom. So in the same way, \\(t_{32}\\) or \\(t_{(32)}\\) would refer to a t-distribution as defined by 32 degrees of freedom, which is different from, say, a t-distribution as defined by 20 degrees of freedom. Nonetheless, the APA, for whatever reason, does not explicitly advocate subscripting degrees of freedom. Credit goes to Brenton Wiernik at the University of South Florida for pointing out the notational consistency of reporting degrees of freedom as subscripts.↩︎ Recall that there were 39 professional recruiters. So the degrees of freedom for a n independent-samples t-test is N-2, or 37 in this case↩︎ jamovi will be more exact than 1.96, of course. The formula that jamovi uses is actually based on the t-distribution, not the z-distribution \\(\\overline{X} \\pm t_{95\\%}(N-2) \\times SE_{mean}\\)↩︎ You can see how enclosing everything in regular parentheses is slightly more confusing with the degrees of freedom also in parentheses: “This difference was statistically significant using an independent-samples t-test (t(32) = 3.5, p &lt; .001).”↩︎ You will often hear the term omnibus ANOVA. This term simply emphasizes the fact that the F-test itself is blind to which differences among its 3+ means rendered it significant overall. To figure out which ones made it significant, you need post-hoc, or planned, multiple comparisons. Another interesting etymological note is that the bus you might ride to and from school in was traditionally an abbreviation of omnibus, a vehicle for carrying lots of people. The analogy here is that the omnibus F-test carries within it a number of possible comparisons, as a bus holds a number of passengers.↩︎ "],["WritingDiscussionSections.html", "Chapter 16 Writing Discussion sections 16.1 Purpose and structure of Discussion sections 16.2 Practice writing exercise 16.3 Writing Assignment #3", " Chapter 16 Writing Discussion sections 16.1 Purpose and structure of Discussion sections The Discussion section can take one of two forms, and depending on which form it takes, it will convey somewhat different information. 16.1.1 Single-study Discussion sections Specifically, if it is a single-study paper (i.e., there are no subheadings similar to Study 1, Study 2, or Experiment 1, Experiment 2, etc.), then this section synthesizes the entire study. It usually starts with the predictions found near the end of the introduction, summarizes the findings (from the Results section), and then finally tries to make sense of the predictions, given the results. An outline of this kind of paper can be seen near the top of section 12.4.1.1 above. In fact, its structure corresponds perfectly to the acronym IMRaD (hence, the acronym), Section 12.4.1.1 is re-produced directly below: Introduction Methods Results and Discussion 16.1.2 Multiple-study Discussion sections But if there are multiple “studies” within the paper (e.g., Study 1, Study 2, or Experiment 1, Experiment 2, etc.), then there is typically a Discussion section for each study, followed in the end by a General Discussion. An outline of this kind of paper can be found near the end of section 12.4.1.1 above. But we will re-produce it below: Introduction Study/Experiment 1 Methods Results Discussion Study/Experiment 2 Methods Results Discussion General Discussion130 In this case, the study-specific Discussion sections restrict themselves to the predictions and results concerning that particular study/experiment, followed by a transition to the next study/experiment, if any. This make sense, of course. There are several reasons that a researcher might split a paper up into separate studies like this. One reason is that it is easier for the reader to digest the information this way. Essentially, it is the same reason that you might organize a smaller paper (e.g., a five-paragraph essay) into separate paragraphs that deal with different aspects of the overall argument. Another reason to do this is that the studies/experiments actually were separate studies/experiments. Often, researchers have to investigate, say, three aspects of the same thing, which entails that they probably need to do it three different ways. These would be reported as three separate studies/experiments. Another version of the distinct-study approach (and a much more common version, from what we can tell) is when there really is only one main study, but the main study is not “strong” enough to stand on its own, so the other studies serve to supplement the main one by addressing possible weaknesses or ruling out alternative explanations to the main study, etc. Whatever the case, as mentioned above, these Discussion sections often end with a transition to the next section, if there is one. This transition gives the raison d’etre for the subsequent section, and often provides a quick one-sentence preview of what the next study/experiment looks like. When there is no subsequent study, naturally this means that the General Discussion (which is mandatory in this type of multiple-study paper) directly follows. In this case, this transition section is simply skipped as the reader understands that, at least according to the writers, everything that needs to be addressed by another experiment or study (at least within the larger context of the paper) has already been addressed at that point. 16.1.2.1 The General Discussion section Among other things, the General Discussion primarily synthesizes the findings from all the separate Discussion sections into one, which then analyzes the more general predictions laid out at the end the Introduction. However, just how to write this section is covered in a separate chapter (Chapter 17). Therefore, we will restrict this chapter to simple Discussion sections. Below are some examples of Discussion sections of the type that you will be writing: individual discussion section leading up to a General Discussion. 16.1.3 Examples of Discussion sections 16.1.3.1 Example 1 Guekes, Gaskell, and Zwitserlood (2015) describe the results of their Experiment 1 in the Discussion section excerpted below. This experimental paradigm concerned foreign-language vocabulary acquisition and long-term memory consolidation. Participants were indirectly taught associations between pseudo-words in German (i.e., things that could be words, but are not, like flimp in English) and colors. The participants then performed a series of stroop tasks at various delays where they needed to indicate the color of the font a letter string is presented in, ignoring any meaning the letter string might have. When the meaning interferes, it is called a congruency effect, and it is evidence of automatic semantic (word-meaning) activation. The researchers purported that any stroop effects between the pseudo-words and the colors that lingered well after the learning phase would be evidence for memory consolidation at work in long-term (neocortical) learning. The Discussion section of The Experiment 1 is presented below. Note that the very first sentence summarizes the design of the experiment. Experiment 1… Materials and Methods… Results… Discussion Experiment 1 was designed to test whether novel words that have recently been associated with native color words via lexical association are already able to produce a congruency effect in the Stroop paradigm. The response-time findings show that this is indeed the case: Immediately after learning as well as 24 h later, novel color words generated sizable congruency effects. Given that learning in this experiment consisted of a word-word-association procedure that neither required nor encouraged deep semantic processing of the novel words, the presence of a Stroop effect seems notable. The fact that we see the effect immediately after learning suggests that, under these conditions, consolidation is not necessary for the effect to emerge. We further found that the change of the congruency effect between the two sessions was not identical in the two stimulus languages: The congruency effect in the German words decreased by 17 ms on the second day compared to the first day’s Stroop session, while in the novel words the effect increased by 9 ms. Thus, in both languages, congruency effects are present on both days, but the significantly contrasting pattern of overnight changes in the Stroop effects… points to the possibility that, during the 24 h interval, the two classes of words are processed in a qualitatively different way. Experiment 3 will address the question of time and consolidation effects more directly. The learning run in this first experiment, although based on a relatively shallow learning task, contained a large number of trials per word and thus resulted in a classification performance that approached ceiling levels. It is therefore unclear whether the novel word congruency effect crucially depends on such a large number of learning trials or whether a significant reduction of the trial number will lead to a qualitatively similar result. Furthermore, so far the Stroop sessions only contained congruent and incongruent trials but no neutral control stimuli, rendering it impossible to clearly identify the effect as facilitatory, inhibitory, or a mix of both. In native-language Stroop, these two main components (facilitation and inhibition) can indeed be distinguished … They are respectively defined as the difference in response times between neutral control stimuli and congruent stimuli (facilitation) or between neutral control stimuli and incongruent stimuli (inhibition). While the relative proportions of the components may vary depending on the properties of the neutral stimuli…, the interference component is typically substantially larger than the facilitation component… If the novel word effect were closely linked to the native words effect, then it should at least be similarly divisible into an inhibitory and a facilitatory component. In Experiment 2, we addressed both the question of learning intensity and the question of whether the novel word congruency effect is composed of facilitation, inhibition, or both. Experiment 2… The most important thing to note in this Discussion section is its overall structure. The first paragraph compares what they were looking for (spelled out at the end of the Introduction, among other places) and whether they found it (from the Results section). And paragraph two elaborates more on the findings, focusing on a different aspect (the change in findings across a 24-hour delay). The third and fourth paragraphs highlight two weaknesses (respectively) in Study 1 that should probably be addressed. These two paragraphs are then followed up, predictably, by a transition to the next study (Study 2) that will directly address these weaknesses. Thus, the organizational content of the Discussion section for Experiment 1 is as follows: What did we expect? What did we find? What were the weaknesses in what we just did? How are we going to address those weaknesses? This is a very typical example of how such Discussion sections are organized. 16.1.3.2 Example 2 It is useful here to contrast the first and the last Discussion sections from Guekes, Gaskell, and Zwitserlood (2015). You can see the difference between the end of the first Discussion section, for Experiment 1, and their third, for Experiment 3. That of the third is below. Experiment 3… Materials and Methods… Results… Discussion In the third experiment, we tested whether the novel-word Stroop effect depended on the presence of German words during the Stroop test. We therefore removed the German words from these blocks and otherwise repeated Experiment 1. We added a between-participants manipulation to differentiate predicted consolidation effects from effects of mere temporal order. One group of participants performed the first Stroop block immediately after learning and the second Stroop block about 24 h later. A second group of participants had no Stroop block immediately after learning, but rather performed both blocks on the second day. Results show that in Group 1, the Stroop effect was not present in the block that was administered immediately after learning but only on the second day. In Group 2, with both Stroop blocks on the second day, the effect was present already in the first Stroop block. These results lead to two important conclusions: First, the novel word effect can be observed even when no German color words are included in the Stroop blocks. Second, the differing results between the two groups indicate that, in the absence of the native-language words, the effect arises only after a period that allows memory consolidation. General Discussion… Notable, they omitted any transition too the subsequent section, which is the General Discussion. This is because they believe that they have sufficiently addressed any concerns at this point, and it is time to talk about the experiment in general. 16.1.3.3 Example 3 In the following study, Schneider and Carbon (2017) studied how selfies taken at different angles can affect whether and how people looking at those selfies associate certain certain characteristics such as attractiveness and helpfulness (among others) with those same selfies. The example we use is from Study 2 (of two studies). We also included the very beginning of the section (where Study 2 begins, starting with “Study 1 revealed that perspective…”). This is because the researchers, in this case, chose to place the transition paragraph at the beginning of Study 2, rather than at the end of the Discussion in Study 1 (contrast this with the approach used by Guekes, Gaskell, and Zwitserlood (2015) in section 16.1.3.1). The small takeaway here is that it is also an option to put transitions at the beginning of subsequent sections rather than at the end of preceding sections. What is important in the end is that there is some sort of transition near the change from one study to another. Study 2 Study 1 revealed that perspective has an impact on facial judgments, especially for body weight judgments…; other postulated effects were less pronounced or absent. However, the used viewing perspectives of Study 1 are sometimes found with selfies but some additional ones are even more typical of the selfie style … Just imagine that you are going to take a selfie on your next trip. It is unlikely that you will only rotate your mobile phone rigidly around one axis, but typically you will use a combination of such rotations. Accordingly, the aim of Study 2 was to examine the impact of typical perspectives of selfies on facial judgments… Methods… Results… Discussion The aim of Study 2 was to examine whether more selfie-specific viewing perspectives have an even more pronounced effect on facial judgments. Accordingly, in Study 2, we let participants rate personality variables across different viewing perspectives on the basis of faces. In accordance with the findings of Study 1, we could show that in case of attractiveness judgments were positively affected by horizontally rotating and elevating the camera. Similarly to Study 1, this effect was slightly (but not significantly) more pronounced for the left side of the face compared to the right side. We also reported larger effects for male faces compared to female faces. This suggests a clear preference for lateral and elevated snapshots. This conclusion is supported by findings that elevating the camera plus rotating the camera is generally preferred for taking selfies… An elevation within pure frontal depictions had no effect on attractiveness ratings at all what is in line with Study 1… However, there was a slight (but non-significant) decrease in perceived attractiveness. In the case of the \\(below_{right}\\) condition (which is equivalent to a view from the right bottom) we found a decrease in perceived attractiveness, and this effect was even more pronounced for female faces. [Other authors have] argued that the right side of the owner’s face positively affects the perception of facial attractiveness. However, this effect had not yet been investigated in combination with a classical selfie-style camera upward tilt. Similarly, it could be shown that facial cues can be taken as a valid predictor of body weight and this highly correlates with the perceived health and attractiveness… Regarding the assessment of helpfulness in Study 2, we showed that elevating and rotating the camera had a significant and positive effect. Similarly to Study 1, this effect was again slightly more pronounced in faces showing their right cheek (\\(above_{30°right}\\)). In contrast, we replicated the negative effect of Study 1 (a frontally elevated camera: the above30° condition is equivalent to a taller person looking downwards on a smaller person). At first sight this contradicts the finding of Study 1, where we argued the typical view of a taller person caused people to assess the viewed person as more helpful. The additional horizontal rotation eliminated this effect. We can only speculate at this point, but in the specific combination of tilting and rotating a camera might have induced a higher rating for helpfulness in Study 2 as this perspective reveals many details of the face and also looks quite realistic—the participants probably perceived a face from this perspective as much more of a real face than would have been the case with a flat picture of a face. The variable helpfulness might benefit from such a more holistic capture of a face to a greater extent than other variables. Regarding the body weight judgments, we replicated the height-weight illusion that we also documented for Study 1. From this point of view…, we expected and found generally higher body weight judgments for lower camera positions and generally lower body weight judgments for elevated camera positions. Surprisingly, in cases of lower camera positions (\\(below_{30°left}\\) and \\(below_{30°right}\\)), we were able to show that a further camera rotation slightly reduced the effect of higher body weight judgments and this was significant compared to the below30° control condition. This suggests a strong positive rotation effect on perceived body weight, which is in accordance with the findings of Study 1. Similarly, we also found a slight but non-significant advantage in the combination of elevating and rotating the camera. Taken together, elevating the camera produces significantly lower body weight judgments across all conditions. An additional rotation does not sufficiently improve this effect. Lowering the camera produces significantly higher body weight judgments across all conditions. However, an additional rotation has a significant effect on perceived body weight (lower body weight judgments). General Discussion… The pattern in the Discussion section of this study is similar to that of example 2 above (section 16.1.3.2). The researchers start off by reminding the reader what they were looking for in this particular segment of the study (something covered at least generally at the end of the Introduction), followed by a very brief description of how they did it. The remainder of the first paragraph is devoted to what they actually did find, but restricted to the evaluation of attractiveness among the selfie viewers, one of the key constructs under investigation (reported in the Results section). The second paragraph then deals with the perception of helpfulness, and the last paragraph, with the perception of body weight, each of which had been covered already in subsequent sections of Results. Finally, there is no transition since the next section is the General Discussion. 16.1.3.4 Example in Appendix G There is an example of a Discussion section in Appendix G. It is just a couple of paragraphs. This example builds on the Method example in Appendix C and Results section in Appendix E, which in turn were based on the survey, data, and actual analysis from one of the research groups formed in PSYC 301 for Spring 2019 (Fettig, López Fuentes, &amp; Villarreal, 2019). Following that is another example that pretends to have found significant results. Use this example instead if you actually found a significant effect in your results. Again, this second example is based on a fictional outcome for Fettig, López-Fuentes, and Villarreal (2019). 16.2 Practice writing exercise This writing exercise relates to part of Writing Assignment #3 in your lab (see section 16.3), but just the second part: the Discussion. 16.2.1 Getting ready 16.2.1.1 Form groups Form working dyads (or triads) like you did for the first three exercises of this type. Work with someone from your own research group again, if you can. Come up with a unique nickname for your group and open a Google Doc as you did before, but give it the following name: [YOUR GROUP NICKNAME] Chapter 16 Exercise. Then share the document with the other members of your working group, as well as your TA. 16.2.1.2 Establish counter-findings Soon, you will begin the practice writing exercises for the Discussion section. But first, you will need to review your current findings (either the statistical output in jamovi, or any current working draft you have of your Results section). For the purposes of the writing exercise that follows, you will pretend as if you obtained exactly the opposite results that you did in your actual analysis. That is, if you did get statistically significant results on your t-test, pretend as if you had not. Likewise, you you did not get statistically significant results on your t-test, pretend as if you actually did. The same goes for your ANOVA, but treat the t-test and ANOVA separately. An entailment of this artificial reversal of findings is that some of you may find yourselves in a situation where you need to discuss null findings. We will provide some guidance below on how to do this even though there are few examples in the literature due to the file-drawer effect. 16.2.2 Writing The rest of this writing exercise is actually about writing. But before you begin, type in the Discussion header at the top. Also note that writing exercise has you start on part 1 of the Discussion section. Then you jump to part 3 (the preview). Last, you go back to section 2 to complete that. 16.2.2.1 Part 1: Review and compare In the Results section you simply described the outcomes of the statistical analyses, with minimal reference to what had been expected given what you laid out in your Introduction.131 In contrast, the Discussion section is where you concentrate on comparing what you had expected to find versus what you actually did find. If you found significant differences in the way that you though you would, this section is where you confirm your theory. There is an illustration of this in Example 1 (16.1.3.1, paragraph 1, beginning with “The response-time findings show that this is indeed the case…”). Another illustration of this is in example 2 (section 16.1.3.2, about halfway through the second paragraph, beginning with the line, “These results lead to two important conclusions:…”). Finally, the authors of Example 3 (16.1.3.3) are doing this when they write, “In accordance with the findings of Study 1, we could show that in case of attractiveness judgments were positively affected by…” If you have a null finding (i.e., your study showed a significant effect; therefore, you have to write this section as if there had not been a significant effect), then writing this section will be a little bit more challenging. First, you need to make the clearly (but understandably) false assumption that you did everything right with your survey.132 This will allow you, albeit a little disingenuously, to treat your null findings as confirmation of the null hypothesis. This now means that you can use language like the following: “Our evidence suggests that the notion that there is no X in Y is true.” And please remember that we are only allowing this artificial stipulation in order to make writing easier in this section. 16.2.2.1.1 Feedback across working groups Just like before, find a group from another research study and give them Can comment access to your document (as well as giving your TA Can Edit privileges). Give the other group helpful comments on how they referred back to their original hypotheses (you will need to just trust them on this), as well as how well they tie in their current results into either support or lack of support for the original predictions. Is everything clear? Does anything look ambiguous? As before, when you are done commenting, place a final comment near the end of the document you are commenting on that says, “We are done commenting.” 16.2.2.1.2 Class review As a class, discuss any issues that came up that might benefit from an expert’s perspective (that of your TA). 16.2.2.1.3 Revise Take about 10 minutes to revise (as a pair) any issues that came up either in your counterpart-group’s comments or during class discussion. 16.2.2.2 Part 3: Preview Here, you will provide a brief preview of what is to come in Study 2. In this case, we are actually jumping over a component of the Discussion. The preview is actually the third element, with the transition being the second. However, this part is easy and does not require too much thought. Nonetheless, it will be crucial for your partner group to have in order to help you form a transition paragraph (see section 16.2.2.3 below). First, start by typing in the following words directly after the review and summary that you just completed: [TRANSITION PARAGRAPH WILL GO HERE] Second, you will start typing the preview in the space below the section set aside for the transition paragraph. The space below will be the preview. In the preview, you simply need to state what the next section, in this case Study 2, is going to cover. Simply recall what the last two questions were in your survey, and convey that information to the reader. There is only one example of this in the examples we provided above: Example 1 in section 16.1.3.1 (Again, the other two examples are the final Discussion sections before the General Discussion, so there is no transition statement). 16.2.2.2.1 Feedback across working groups Have your partner working group have a look at your preview. But don’t take too much time on this. The main thing they should be looking for is whether it is clear what will be analyzed in the next section. There really isn’t too much to this section of the Discussion. 16.2.2.2.2 Class review Likewise, do not plan on spending too much time on this as a class, though you should, if for no other reason than to address questions that arose during the writing of this subsection. 16.2.2.2.3 Revise Take 5 minutes to make any necessary revisions. 16.2.2.3 Part 2: Transition In this section, you will begin the transition to the next study (part 3: the preview that you just worked on being the final piece in the transition). For this section you will write over the part you initially put in square brackets: [TRANSITION PARAGRAPH WILL GO HERE] &lt;- overwrite this What goes here is a transition between the review and comparison that you completed at the beginning of the Discussion and the preview you wrote at the bottom. You need to connect the two, if possible. Normally, in most published papers that you read, there will be a good underlying reason for creating a follow-up study. In this case, there may not be since we gave you very little time to generate your variables, and we pre-chose their design. Thus, Study 2 is an artificial follow-up to Study 1. The cart is before the horse. There are a couple of options here, however. It should be fairly easy to create a transition. First, it is possible that there really is some sort of conceptual difference between your categorical predictor variables in Study 1 and your continuous predictor variables in Study 2. If so, this is the best kind of transition you can provide. Emphasize this difference as you develop your transition. In fact, I would encourage you to push hard to make this kind of transition work. The next best option is to just state (in abstract terms), “We just did X, but we haven’t looked at Y.” This is a very straightforward transition as it is already implicit, somewhat, from the end of your Introduction. This approach is not quite as good as the first since it isn’t a “principled” transition that flow naturally out of the study design. But it is an acceptable transition. But if those both fail for some reason, another way to pitch a transition is to note that nominal variables (i.e., the ones you use as predictor variables in t-tests and ANOVAs) tend to be less informative than continuous variables, and that therefore, perhaps the continuous predictor variables (i.e., the ones you will use for correlation and regression) will render more informative results. Consider this last one a backup plan of last resort for your transition. In the end, it’s possible that your feedback group will have some ideas for you for the transition section of the Discussion. Perhaps one of these ideas will allow you to adopt the first strategy above. 16.2.2.3.1 Feedback across working groups This time, focus on your parnter working group’s paper in terms of how natural the transition section feels. If it is not that natural, try to brainstorm on some ideas that they might use to make the transition. When you’re done, let the other group know with a final comment. 16.2.2.3.2 Class review Take some time to review this as a class. This is a chance for the entire class to participate in a brainstorm where possible transitions for each group are discussed. 16.2.2.3.3 Revise Consider your classmates’ ideas about writing this section. Revise if you think it’s going to make this section flow better. 16.3 Writing Assignment #3 After having completed the writing excercise above as well as having read and completed the exercises in chapter 15, you should have a pretty good idea about how to go about the next writing assignment: Writing Assignment #3. See below for more details. 16.3.1 General Instructions Writing Assignment #3 will consist of the Results and Discussion sections of Study 1. In fact, you will use those very headers: Study 1 Results Discussion Like Writing Assignment #2 (Method), this one is authentic in that you will be using your own data from your own study. NOTE: Again, do NOT put your name on this assignment. It will be submitted for anonymous peer review through Peerceptiv, and including your name makes it non-anonymous. Finally, as you did with Assignment #2, you should tag on these Results and Discussion sections to the current, full version of your paper so far (Introduction and Method). It will be easier to review if the reviewers (i.e., your peers) have access to your Introduction as well as your Method section. 16.3.2 Specific Instructions As noted above, Writing Assignment #3 will consist of the Results and Discussion sections, underneath the major header Study 1. The header Study 1 should be centered, and at the top of the page. 16.3.2.1 Results As you have already practiced, your Results section should summarize the statistical analyses carried out using both the t-test on your composite variable using the categorical variable with two levels, and the ANOVA on your composite variable using a oneway ANOVA (not necessarily the One-Way procedure in jamovi, but probably the simple ANOVA option). One of the statistical analyses should be presented first, and then the other, in separate paragraphs. Make sure that the sentences of each paragraph read like normal English prose, preferably prose that you could read out loud to a 5th grader. Inserted among the sentences as parentheticals, or set off by commas, should be any statistics, descriptive or inferential, that you include. In the end, the reader should be able to mentally delete the statistics and read your Results as normal English sentences. Those statistics should be in APA format. You can see the examples we have provided in Chapter 15 [sections 15.2.1.1, 15.2.1.2, and 15.2.1.3], or the example provided in Appendix E. You can also look up the general format for descriptive statistics in section 12.4.1.3.1, and for inferential statistics in section 12.4.1.3.2. Finally, you can refer to handouts provided in the main class. For descriptive statistics, always report the mean, the standard deviation, and the sample size of the group. For inferential statistics, be sure to include the test statistics (along with the p-value, naturally), as well as the 95% confidence intervals (if available), and the effect size. Each analysis should be accompanied by either a table or a figure, but not both.133 In fact, you could choose a figure for one, but a table for the other.134 But whichever you choose for each of your analyses, insert a caption (with tables, captions go above the table; with figures, they go below the figure). Note that if you are using Google Docs, you will need one of the the following Add-Ons: Captionizer or Caption Maker.135 If you are using Office 365 in the cloud, you may have to skillfully type in the caption in the main text. If you are using a stand-alone word processor on your desktop, there is usually an “insert caption” function somewhere (it varies by product). Importantly, the prose of the Results should make reference directly to any tables or figures that are in this section (e.g., “As depicted in Figure…”) There is a rubric in Appendix F for the Results section. 16.3.2.2 Discussion In short, the Discussion section should briefly mention the relevant questions or predictions made at the end of the Introduction. This should be followed by a summary of how the results on that particular question/prediction came out (without using statistics), and whether they supported or disconfirmed any prediction made, or whether the outcome was unresolved. Naturally, the unresolved case is one in which you failed to reject the null hypothesis. Due to the file-drawer effect, there are not as many examples in the literature of this kind of Discussion. However, for the purposes of this paper, you can assume that you did things right and that you should have found the effect if it was there. This is an unwarranted assumption, but we will allow you to make it. This makes the Discussion a little bit easier to write as you can now “pretend” as if you found evidence of no effect.136 After doing this, this section should move on to making a transition to the next study (Study 2). As noted above, one normally lets the needs of the study guide the writing (e.g., we need another study, so we’ll add a section to the paper). But in this case, because of the nature of the class, we set up your writing sections beforehand (putting the cart before the horse). This makes it little bit trickier to come up with a natural transition to Study 2, but in the end, one of the strategies covered in the writing exercise above (section 16.2.2.3) should work. You may also want to consult with your TA here for ideas. Note that, in this case, it is okay if the members of your group all share the same strategy for the transition, though you may not use each other’s writing (something that is never allowed anyway, except in real-world situations where you are co-authoring the same document). However you do this, end this section with a quick preview (1-2 sentences) of what lies ahead in Study 2, which will consist of two analyses, a correlation and a simple regression, both using continuous predictor variables (on the same composite outcome variable). There is a rubric in Appendix H for the Discussion section. In fact, both Assignment #3 and Assignment #4 roughtly conform to the two Study/Experiment structures. That is, Assignment #3 is the Results and Discussion for the t-test and ANOVA, whereas Assignment #4 is the same for the correlation and regression, along with General Discussion at the end. One major difference between your papers and the canonical multi-study paper is that you wrote a Method section that applied to all the studies, instead of having separate methods for each study/experiment (a more common approach).↩︎ In practice, there are often very short comparisons of outcomes to expectations in the Results section. But the purpose of doing so in that section is simply to “hold the reader’s hand.” That is, as is true with any kind of organized writing, it is easier to read through a Results section if it is connected, albeit minimally, to what came before and what comes after. Hence, the brief evaluations you sometimes find in Results sections. You can find these kinds of quick, backward-looking references in Example 3 in Chapter 15, section 15.2.1.3. Key words there are “Critically, as predicted by reconsolidation theory…” in the third paragraph of the example, and the nearly identical “Critically, in line with reconsolidation theory…” in the fourth paragraph.↩︎ Again, the main author of this lab manual, Dr. Patrick Bolger, fully understands that there is no way you could have done this right given the absurdly short amount of time he gave you to prepare the survey.↩︎ It could be the case that for, some strange reason, you can’t do without both. That’s okay too↩︎ Please note though that this would be odd given the nature of the current assignment with two categorical predictor variables, but perhaps appropriate in certain circumstances.↩︎ Note that I found Caption Maker to be somewhat complex for the current assignment. And Captionizier is a little bit wonky. I had to delete the caption and re-insert a few times to make it work.↩︎ There is a way to find evidence for the null hypothesis, but you will need to take a class on Bayesian statistics to find out how.↩︎ "],["WritingGeneralDiscussionSections.html", "Chapter 17 Writing General Discussion sections 17.1 A note on Writing Assignment 4 17.2 Structure of General Discussion sections 17.3 Contents of General Discussion sections 17.4 Practice writing exercise 17.5 Writing Assignment #4", " Chapter 17 Writing General Discussion sections First, a note on Writing Assignment #4. 17.1 A note on Writing Assignment 4 As noted in section 15.1, Writing Assignments #3 and #4 are different from the first two in that they encompass more than one section of the IMRaD style paper. That is, they both contain Results and Discussion sections, whereas Assignments #1 and #2 only contained on section each (the Introduction and the Method, respectively). However, there is also a difference between Writing Assignments #3 and #4. Specifically, the latter contains both less and more than the former. Although the Results sections are about equal across the two assignments,137 the Discussion section for Writing Assignment #4 will be shorter. This is because there is no subsequent study to transition to since the next section is the General Discussion. Therefore, the Discussion section that follows the Results section in Assignment #4 consists of simply the review (see section ) and the comparison, with neither a transition nor a preview (see the Appendix with the Discussion rubric for reference here). And as just implied, Assignment #4 has an extra section, the General Discussion. These differences between Assignments #3 and #4 were also mentioned at the end of section 16.1.2. But it is time to turn our attention to writing the General Discussion. First, we talk about the general organizational pattern of the General Discussion within the broader context of the paper. Then we provide some guidance as to what kind of content to put in this final section of the IMRaD paper. 17.2 Structure of General Discussion sections In an abstract, organizational sense, the General Discussion is the mirror image of the Introduction. Recall that the Introduction progresses from general to specific (see section 13.2). Simply put, the General Discussion proceeds from specific to general.138 For what it’s worth, this completes what is informally referred to as the “hourglass” structure of IMRaD papers. This was mentioned earlier in section 12.4.1.1. We will reproduce the figure from the end of that section here as Figure 17.1 below: Figure 17.1: The hourglass shape of the IMRaD research paper (from Figure 1, p. 420, of P. Turbek et al. (2016). If the paper consists of multiple studies, then the General Discussion forms the bottom of the hourglass (not the Discussion).139 Figure 17.1 also suggests that there is a separate Conclusion section (or here, Conclusions and Implications). You may see this as well in some papers, but it is optional, and perhaps only merited in very long papers as a strategy to help readers organize very long stretches of prose. In our case, we will include any conclusions and/or implications within the General Discussion. 17.3 Contents of General Discussion sections Overall, the General Discussion is going to evaluate the entire paper, focusing mostly on what was predicted to happen versus what did happen, the extent to which the current study answers the questions it set out to answer, and what might come next in the research process. But the first part of the General Discussion is quite predictably a synthesis. 17.3.1 Synthesizing The synthesis is comprised of a summary followed by an comparison of the overall predictions of the paper against those findings. However, example 2 below (section 17.3.1.2.2) bucks this trend across studies to some degree. Sometimes, these are reversed, where the predictions of the paper (from the Introduction are) summarized, then compared against the findings from the studies. 17.3.1.1 Summarizing Whatever may lie ahead in the General Discussion,140 one thing will always come somewher near the beginning of this section: Namely, the authors will summarize the previous Results and Discussion sections into one summary, usually in one somewhat concise paragraph. Thus, there may be a bit of repetition here from earlier sections (though not word-by-word, which would be awkward). The point of this is to prepare the reader for a comparison of the overall predictions of the study against the overall findings. 17.3.1.2 Comparing Roughly parallel to what is written in individual Discussion sections in a multiple-study IMRaD paper, the summary of findings is followed by a comparison of those findings against what was predicted (usually indicated at the end of the Introduction). Except this time, it’s the findings overall that are compared against the predictions overall. In other words, the General Discussion addresses the “big picture,” whereas the individual Discussion sections had addressed the “smaller picture” of that particular portion of the study. Note that often, however, a review of the predictions will be followed by a summary the findings. The choice of order here is stylistic, and up to the author(s). The question the researchers usually have to ask themselves at this point is as follows: Did our findings in general support, in general, the main prediction(s) we made at the outset? There is much more for the researcher to discuss. However, we will temporarily stop here in order to illustrate the writing concepts described above. NOTE: Note that due to publication bias in science (and particularly in psychological science), most of the primary research articles that you read will have Discussion or General Discussion sections that confirm the predictions of the study in one way or another. This is not really as it should be since well-conducted studies that find null effects ought also be commonly published. But they are not. An unfortunate, mild141 side effect of this state of the field is that there are few examples to turn to when writing up General Discussion sections where there is a failure to find what was predicted (e.g., failures to reject the null hypothesis). That said, there are plenty of such sections that discuss findings that were surprising relative to the predictions made (e.g., the researchers thought the outcome would be significant in one direction, but it was significant in the other direction). Since there are so many possibilities when it comes to content for the General Discussion, we will provide examples as we go. That is, for each content possibility in the General Discussion, we will provide an immediate example or two. 17.3.1.2.1 Example 1 We begin with the summary of findings. Below is that very section from the paper we cited earlier by Guekes, Gaskell, and Zwitserlood (2015) in Chapter 16, first mentioned in section 16.1.3.2. We included the final paragraph of the Discussion of Experiment 3 to illustrate that unlike the Discussion sections of Experiment 1 and Experiment 2, there is no transition this time since the subsequent section (after Experiment 3) is the General Discussion. No transition is required. We also added boldface to highlight where the summaries of each of the summaries begins. Experiment 3… Materials and Methods… Results… Discussion… …Results show that in Group 1, the Stroop effect was not present in the block that was administered immediately after learning but only on the second day. In Group 2, with both Stroop blocks on the second day, the effect was present already in the first Stroop block. These results lead to two important conclusions: First, the novel word effect can be observed even when no German color words are included in the Stroop blocks. Second, the differing results between the two groups indicate that, in the absence of the native-language words, the effect arises only after a period that allows memory consolidation. General Discussion In three experiments, we tested the semantic links of novel color words that had been associated with color concepts through lexical association with native language (German) color words. To assess which conditions are necessary for semantic learning, the learning and test phases were realized such that they minimized semantic processing. In Experiment 1, novel words were associated with native-language color words until almost perfect discrimination performance. They were then entered into the Stroop task together with German color words. We observed substantial novel word Stroop effects both immediately after learning and 1 day later. A significant three-way interaction indicated that the reduction of the effect from Day 1 to Day 2 in the German words contrasted significantly with a simultaneous increase of the effect in the novel words, thus suggesting an influence of memory consolidation. In Experiment 2, learning intensity was considerably reduced and neutral control stimuli were added to the Stroop blocks. We again observed substantial Stroop-congruency effects directly after learning and 24 h later. A detailed analysis including the control condition showed that interference made up a significant portion of the novel-word Stroop effect. In Experiment 3, we repeated Experiment 1, but crucially removed the German words from the Stroop task, so that the novel words were now tested without any L1 context from the learning phase. The novel-word Stroop effect was now not observed immediately after learning, but only 24 h later. Results from a second group that had a different time course of Stroop blocks showed that the delayed emergence of the effect in Group 1 is not due to a simple build-up or training effect from one block to the next. Rather, it must be related to the temporal distance between learning and test–that is, to memory consolidation. In the example above (directly under General Discussion), even if you don’t understand exactly what was said, you can see the summaries of each of the experiments: In Experiment 1…, In Experiment 2…, and In Experiment 3… Importantly, note that the very last sentence of this paragraph contains a strong hint regarding how they are going to interpret their findings in light of their predictions, which were at least partly focused on memory consolidation. From the Introduction: A further aim of our study was to test whether the establishment and availability of such semantic links depends on an opportunity for memory consolidation. (Geukes, Gaskell, &amp; Zwitserlood, 2015, p. 3) Recall that we claimed that authors will always compare their overall findings against their overall expectations. And this is indeed the case here. But it doesn’t happen immediately in this case. Rather, the authors here address a couple of qualifications to their findings. This takes place over the two paragraph subsequent to the first one in the General Discussion. However, in the fourth paragraph of the General Discussion (Geukes, Gaskell, &amp; Zwitserlood, 2015, p. 13), the authors do return to the main issue of memory consolidation, and how overall results relate. Perhaps the most interesting aspect of our results is that the opportunity for consolidation affected Stroop performance, and that this consolidation effect was further moderated by context, that is, by the presence of German color word trials in the Stroop task. This impact of memory consolidation on the integration of novel words fits with data from word-learning studies on the learning of word forms only… or on the acquisition of form and meaning… The data from Experiment 3 in particular demonstrate that memory consolidation is relevant for associating novel words with meaning, not only for integrating novel word forms into lexical networks. This is consistent with the [Complementary Learning Systems] account of word-learning…. 17.3.1.2.2 Example 2 The second example we use (Schneider &amp; Carbon, 2017), excerpted below, was also referenced earlier in Chapter 16, section 16.1.3.3. So you are already familiar with it. It was the study about selfies. But before we provide you with this example, it is important to understand something about this paper. Specifically, it was not meant to evaluate theory or compare models, but rather to provide information about a phenomenon that, hitherto, little was known about. Here are couple of excerpts from earlier in the paper that point to this characteristic of the research. The first is from the very first two sentences of the paper: Taking selfies is now becoming a standard human habit. However, as a social phenomenon, research is still in the fledgling stage and the scientific framework is sparse. (Schneider &amp; Carbon, 2017, p. 1) The second is from later in the Introduction: The aim of the present study was to provide fundamental information what impact a change of perspective has on a variety of higher-order variables that are relevant for expressing personality and for mating. To the authors’ knowledge, there is no systematic investigation of how viewing perspective affects the perception of higher cognitive variables (such as personality variables) on basis of faces, especially for more selfie-style conditions. (Schneider &amp; Carbon, 2017, p. 3) This fundamental approach in the paper sets up how the General Discussion is structured. In essence, the results of the experiments (and not their implications for theory) are the fundamental objects of study to be covered in the General Discussion. With this understanding in mind, it is instructive to read the few two sentences of the General Discussion in Schneider and Carbon (2017): General Discussion The main goal of this study was to reveal the impact of perspective on persons depicted via selfies. In two studies, we revealed clear effects of perspective on higher cognitive processes (namely the perception of person-related variables on the basis of facial depictions)… (Schneider &amp; Carbon, 2017, p. 12) This summary is obviously much shorter than the equivalent summary (summarized first above) by Geukes, Gaskell, and Zwitserlood (2015). But it is actually not the end of the summary. But what directly follows these first two sentences is instead a reminder to the reader of the nature of this study as phenomenon description instead of theory evaluation. It reads like a miniaturized version of the Introduction: … Research on selfies has revealed that persons who shoot selfies want to express their mood, their personality and even their lifestyle via selfies, so they try to optimize this information by intuitively adapting the camera position… Previous work documented that in cases of classical portraits there were a lot of compositional suggestions and artificial rules which were applied to gain pictures of high appeal… However, scientific research is quite far from achieving consistent results about the meaningfulness and effects of these rules in general… In contrast, regarding the social phenomenon of taking selfies, one may find only a small number of suggestions, often in a relative unsystematic way, for taking the “best” selfie… and some photographic rules like the “high-angle shot”… However, there is little knowledge about whether and how exactly these aspects may have an impact on the perception of a given face. Moreover, there are some hints toward a general deviation from known photographic principles in selfies… and the impact of a typical selfie-style perspective has yet to be investigated. (Schneider &amp; Carbon, 2017, p. 12) The next two paragraphs do just that: They summarize the findings overall. In order not to belabor the point, below we quote only the first part of the each paragraph: Accordingly, our results suggest that perspective has a significant impact on the perception of the beholder, especially for attractiveness, helpfulness, sympathy, intelligence, and associated body weight: Study 1 investigated the impact of… (Schneider &amp; Carbon, 2017, p. 12) And here is the opening to the second such paragraph: In Study 2, we investigated the effect of more selfie-style viewing perspectives (typical combination of camera rotation and camera pitch) and only found… (Schneider &amp; Carbon, 2017, p. 12) The paragraph that follows is about how the authors interpreted their findings, but without much reference to theory since there isn’t much in the first place (with ordering elements boldfaced): How can the complex data pattern be interpreted? First of all: Perspective has a significant impact on the perception of higher-cognitive variables (such as person-related variables) on the basis of faces. Secondly: Effects of perspective were in contrast to some past findings… suggesting that selfies constitute an own class of pictorial presentations of a person… Furthermore, our results highlight the importance of the visibility of certain features in facial stimuli, per se (e.g., regarding the perception of dominance, our results underline the visibility of the neck as an important cue to masculinity and dominance). Thirdly: Interestingly, for most of the variables effects were significant for the 30° head turn (left and right hemiface) images, but not the 15° head turn images. We have at least two reasons for this discrepancy in mind: On the one hand, the 15° rotation is just too similar to the frontal condition, at least to detect any differences from the frontal view by means of the given experimental setting with limited sample sizes which were only capable of revealing effect sizes of small to medium effect sizes but not, for example, very small effects. On the other hand, referring to research papers which systematically varied other kinds of rotation, e.g., planar rotations, we also observed a certain range of rotations for which essential variables did not change… Fourthly: In contrast to the common standpoint that we are able to make meaningful suggestions about “how to take the perfect selfie,” our results indicate that we are a long way from having any clear references. (Schneider &amp; Carbon, 2017, p. 12) Again, the important inference from the example presented above is that although it is the norm to provide predictions in the Introduction and then to compare one’s findings against those predictions in the General Discussion (or Discussion), that is not true in all cases. In this study, there were few predictions to begin with since the phenomenon is generally under-investigated so far. So the study was quite exploratory. The authors were very explicit (and deliberately repetitive, it seems) about this, making sure both to inform and remind the reader that this was the case. This makes sense. The default expectation in research is that any new research be based on past research. If it is not, it will need extra justification before reviewers will consider it for publication. 17.3.2 Clarifying For the purposes of this manual, we will label the next section of the General Discussion the Clarification. It turns out to be difficult to find a single English word that describes this section well. The word clarification should be interpreted liberally here to apply to various approaches the researchers might take in their write-up. This is a very diverse and usually challenging section to write as it serves as a guide to the reader as to how to interpret the findings, both within the current study and in the broader context of research. In other words, “How should I interpret what I just read, and how and what exactly does it ultimately contribute to the world of science?” NOTE: Importantly, note that the scope of the discussion broadens here. This fits with the description made earlier (section 17.2) of the General Discussion proceeding from specific to general, like the bottom of an hourglass. Content that might be included here encompasses (but is not limited to) the following, depending on the exact outcomes of the study: Helping the reader interpret results Were any of the studies more informative than the others? Or do they sum up as equal contributors to a general finding? Did any of them qualify the interpretation of any of the others? Application to previous research Do the studies support previous research? If so, do they support a particular theory/model over others? Do they fail to support any previous research? If so, do they point to new directions in theory/modeling? Do other studies in the literature corroborate your findings? If so, which and how? Do other studies contradict your findings? If so, how? Is there a resolution? Were there any surprises? What does this mean for the field? Limitations Are there important limitations to the study that the reader should be aware of?142 If so, how generally does the current study apply to previous research? Can these limitations be overcome in future studies? If so, how? As just noted above, there is a great deal of diversity in terms of how the researchers go about clarifying aspects of their overall study. We will provide merely two examples so as not to make this section stretch on indefinitely. They are from the the now familiar studies by both Geukes, Gaskell, and Zwitserlood (2015) and Schneider and Carbon (2017). 17.3.2.1 Example 1 Immediately after paragraph four, reproduced above, Geukes, Gaskell, and Zwitserlood (2015) tie in their results to previous research, along with corroboration from another study. The beginnings of each of these threads is boldfaced so that they are easier to identify. Finally, how can our immediate but context-dependent effect be reconciled with what is known about neural correlates of learning and retrieval? Figure 6 [not shown here] illustrates how learning context may moderate effects of memory consolidation in semantic word learning. We assume that the employed training regime results in an immediate hippocampal association between the German (L1) word and its novel counterpart. This novel association means that the L1 word provides a mediating link in memory between the novel (L2) word and the color semantics. So, even prior to an opportunity for consolidation (provided by sleep, in our case), Stroop effects can be obtained, as long as the L1 word is present in the Stroop task as a contextual cue that “primes” or temporarily strengthens this indirect association. In fact, there is direct evidence for the involvement of the hippocampus during associative learning of the type implemented here: Breitenstein et al. (2005) used event-related fMRI while participants learned novel words in the scanner. Correlated amplitude changes between the hippocampus and neocortical regions were observed, in line with the overall evidence for the importance of the hippocampus in the formation of arbitrary associations in memory…. Clearly, the first part of the passage ties in the research to previous research, and the second part identifies another experiment that showed similar results, thereby corroborating the current findings, and strengthening the current researchers’ case. 17.3.2.2 Example 2 Returning to the General Discussion from Schneider and Carbon (2017), the authors note some limitations of the study after conveying their general findings. The first weakness is followed by a call for future research. whereas the second weakness is followed by a rebuttal to that self-identified weakness. The major turning points of the paragraph are boldfaced. We would also like to mention some limitations of this study: Past research revealed that direct vs. averted gazes have a direct impact on the perception of a given face… More precisely,… an averted gaze has a negative effect on the perception of attractiveness. However, the effect of the combination of averted head plus direct gaze vs. frontal face plus averted gaze across different viewing perspectives on the perception of higher cognitive variables (like those we used) has not yet been investigated. In this study, we did not investigate such a combination, which would incidentally be very much in accord with some Renaissance portraits like La Gioconda by Leonardo da Vinci…. Future research should address such further settings to enrich the existing knowledge base on selfies. Another weakness of the present study is that we neither could control the actual size of the presented face on the monitor nor the actual viewing distance. Moreover, we must expect that display color, contrast and brightness were not at the same level across all participants. This might affect the perception of a face dramatically. However, the fact that we could replicate the height-weight illusion… makes it conjecturable that other effects were relative stable. Similarly, other studies… used relatively unstandardized images that could not be controlled along those variables, and though [sic] revealed consistent results. 17.3.3 Concluding The last part of the General Discussion (indeed, the last part of the entire paper) is usually either a nod to possible future directions of research, or a quick summary of the overall import of the study, or both. Whatever it turns out to be, it will be the broadest statement yet about the study, thereby consummating the hourglass shape of the paper. We get an example of each from each of our continuing examples. 17.3.3.1 Example 1 Geukes, Gaskell, and Zwitserlood (2015) provide an example of a General Discussion that ends with a statement as to the overall meaning of the study for the field. In this case, there is no direct statement with respect to future directions, except to suggest that “careful experimental manipulations” are in order for future studies investigating such phenomena. In sum, our results stress that careful experimental manipulations are necessary to fully capture the intricate learning and memory processes involved in the acquisition of novel meaningful words. The brain recruits multiple resources to immediately associate newly learned material with well-established knowledge. The context in which learning takes place, and the particular aspects that the learning situation provides or focuses upon, are important for the immediacy of effects that indicate the integration of newly learned words. A stable and strong integration in existing semantic networks, diagnosed by automatic effects in suitable tasks, seems to require consolidation, to become less dependent on contextual cues from the learning situation. (Geukes, Gaskell, &amp; Zwitserlood, 2015) 17.3.3.2 Example 2 Schneider and Carbon (2017) provide a slightly different ending to their paper. In it, they place their study in the context of possible future research. Despite all the back draws [sic] you always face with standardized and systematically varied material, such experimental material can test already small effects which might be tested with more ecologically valid material in the field later on. We hope that our study contributes to the understanding on how perspective can change the assessment of higher cognitive variables. This will help to sensitize selfie-ists how [sic] powerful the use of perspective might be in conveying their inner states. 17.3.4 Example in Appendix I There is an example in Appendix I of a General Discussion. It is actually a write-up of the results from Fettig, López Fuentes, and Villarreal (2019). For the sake of completeness, it also includes Study 2, which is then followed by the General Discussion. Thus, this example mirrors what is expected for Writing Assignment #4. 17.4 Practice writing exercise This writing exercise relates to part of Writing Assignment #4 in your lab (see section 17.5), but mostly just the last part: the General Discussion. 17.4.1 Getting ready 17.4.1.1 Form groups Form working dyads (or triads) like you did for prior exercises of this type. Work with someone from your own research group again, if you can. Come up with a unique nickname for your group and open a Google Doc as you did before, but give it the following name: [YOUR GROUP NICKNAME] Chapter 17 Exercise. Then share the document with the other members of your working group, as well as your TA. 17.4.1.2 Establish counter-findings Like last time (section 16.2.1.2), review your current findings (either the statistical output in jamovi, or any current working draft you have of your Results section). Again, for these exercises, you will write as if you obtained exactly the opposite results that you did in your actual analysis. If you did get statistically significant results on your correlation, pretend as if you had not. Likewise, you you did not get statistically significant results on your correlation, pretend as if you actually did. The same goes for your simple regression, but treat the correlation and regression separately. After doing this, some of you may need to discuss null findings. You will need to be creative here. 17.4.1.3 Create Study 2 outline You obviously have not yet written anything for Study 2 yet. Nor should you for this writing exercise, which is about the General Discussion. Instead, come up with an outline, with your partner(s), concerning roughly what your Results and Discussion would say if you had to write it all out. You will use this to help you write a General Discussion below. 17.4.2 Writing The writing you will in the sections that follow is about the General Discussion. So write that header at the top of your document, centered. 17.4.2.1 Part 1: Synthesizing In this section, you are going to review what you expected to find among all your variables, and compare your counter-findings with these predictions. The focus here is on the variables as a set, not so much the invidual vairables (which you did already in previous Discussion sections). That is, try to discuss them as a group. We have provided examples above of this (sections 17.3.1.2.1 and 17.3.1.2.2). But these are both a bit long-winded. It might be better to consult Appendix I for an example here. Note that the example there is based off the actual results, which failed to reject the null hypothesis in all cases. That said, the beginning of any General Discussion or Discussion section in almost any IMRaD paper will have an example here. 17.4.2.1.1 Feedback across working groups Just like before, find a group from another research study and give them Can comment access to your document (as well as giving your TA Can Edit privileges). Give the other group helpful comments on how they referred back to their original hypotheses (you will need to just trust them on this), as well as how well they tie in their current results into either support or lack of support for the original predictions. Is everything clear? Does anything look ambiguous? Do they do a good job of treating all their results as a set? Or are they treating them like they might have in the individual Discussion sections that preceded the General Discussion? Like before, when you are done commenting, place a final comment near the end of the document you are commenting on that says, “We are done commenting.” 17.4.2.1.2 Class review Get together as a class and discuss any issues that came up that might benefit from your TA’s perspective. 17.4.2.1.3 Revise Take about 10 minutes to revise (as a pair) any issues that came up either in the comments of your counterpart group or during class discussion that you just finished. 17.4.2.2 Part 2: Clarifying This section may be challenging to write. We provided examples above in sections 17.3.2.1 and 17.3.2.2. Chances are that the latter of the two examples (falling under “limitations” of the study) is going to be more appropriate since the surveys in this class are so quickly put together that there are bound to be weaknesses here that you noticed after sending the survey out. In fact, it would not surprise us at all if this were by far the dominant clarification strategy among you. Nonetheless, there are alternatives. You may want to consider developing such strategies as the following: connections with previous studies,143 which “study” (Study 1 or Study 2) was the stronger of the pair and what that says. Alternatively, which statistical test showed the most promise144 conflicts among findings within the paper and possible resolutions conflicts with past research or expectations, and possible explanations corroboration from outside findings or even anectdotal perceptions or news stories145 But whatever strategy you choose, remember that the main point is to inform the public of what you have studied and what you have found. Irrespective of whether you had statistically significant results or not, the public should “walk away” from reading your article being more informed than they were when they began reading your article. You have sort of a scientific responsibility here. Both overestimating and understimating the importance of your study can mislead the public. 17.4.2.2.1 Feedback across working groups Provide the other working group some feedback as to whether their clarification is convincing or not. If not, can you think of another way to go about clarifying their results? Is there another strategy that might bear more fruit? When done, place a final comment near the end of the document you are commenting on that says, “We are done commenting on the Clarification.” 17.4.2.2.2 Class review This would be a really good opportunity to tap into the creativity and expertise of your TA. To the same extent that you read textbooks, they read IMRaD papers. Therefore, this classroom discussion should take longer in order to flesh out some creative possibilities here. 17.4.2.2.3 Revise Take about 10 minutes to revise this section to maximize its potential to be a convincing qualitative analysis of your overall findings. 17.4.2.2.4 Part 3: Concluding This is undoubtedly the easiest of all the sections to write, and many authors do not take it very seriously. Nonetheless, it is almost always there, at least in papers in Psychology. Most of you have probably developed ideas already on how you might have carried out your own study more effectively, again, an artifact of having been asked to put together a survey so hastily. Still, you have have some other ideas on how to conclude the paper. If your findings (in the current activity, counter-findings) were significant (as we imagine many of yours were), then you should think of tweaks (small changes) that you might apply to your current study to improve its resolution, or even discover something else that is related. Try to be imaginitive here, though the bar is rather low in this section since the primary work occurred already in the clarification section. Whatever choice you make here, however, the paper should end on a pretty general note, either a general synopsis, or a look forward to future research. 17.4.2.2.5 Feedback across working groups Look at your counterpart group’s final section and think about how you might have ended it. Give them some ideas, as they will to you. Let them know when you are done with a comment in the document itself. 17.4.2.2.6 Class review This would also be a good opportunity to share different strategies here. The emphasis here would be less on TA expertise, and more on the collective creative thinking of individual groups. 17.4.2.2.7 Revise Take about five minutes to revise this section of the paper. 17.5 Writing Assignment #4 17.5.1 General Instructions Writing Assignment #4 will consist of the Results and Discussion sections of Study 2. In fact, you will use those very headers: Study 2 Results Discussion In this sense, it is very much like Writing Assignment #3. However, there will be two pretty signficant changes. First, the Discussion for Study 2 in Writing Assignment #4 will leave out the transition and the preview. There is no need for them since the subsequent section is the General Discussion.146 Second, the paper will also include the General Discussion. Therefore, the final document will ultimately have the following headers: Study 2 Results Discussion General Discussion NOTE: To repeat, do NOT put your name on this assignment. It will be submitted Peerceptiv for anonymous review; including your name on it makes it non-anonymous. And as you did with Assignments #2 and #3, you should add the Study 2 to the current, full version of your paper (with the Introduction, Method, and Study 1). If reviewers can refer back in your study to key areas, it will be easier for them to review the current assignment. You should also not be afraid to modify your Introduction to cohere better with your Discussion sections and General Discussion. We understand that you wrote the Introduction at the very beginning of the semester, without understanding, really, what was coming up. You may now have a better sense of you should have been predicting with your study, and you should feel free to make modifications there.147 Also, if you are so inclined, you can modify the Method section to be more specific about the statistical design of your study (under Design and Analysis). This would be for self-edification and personal satisfaction, however, as it’s not likely that your peer-reviewers would refer back to that. 17.5.2 Specific Instructions As noted above, Writing Assignment #4 will consist not only of the Results and Discussion sections for Study 2, but also the General Discussion. But for the current assignment, the first header should say Study 2, and should be centered at the top of the page. 17.5.2.1 Results Your Results section should summarize the statistical analyses carried out using both the correlation between your composite variable and one of your continuous, predictor variables, and then a simple correlation between your composite variable and the other continuous, predictor variable. The analyses should be done in separate paragraphs. Be sure to use sentences that function in plain English if you removed the statistics from them. As just implied, the reported statistics should be inserted into the prose as parentheticals or set off by commas, semicolons, etc. Your reader should be able to use those cues to delete the statistics (mentally) and read the Results section as regular prose. The reported statistics should, of course, be in APA format. You can refer to the examples we have provided in Chapter 15 [sections 15.2.1.1, 15.2.1.2, and 15.2.1.3], or the example provided in Appendix I. You can also look up the general format for descriptive statistics in section 12.4.1.3.1, and for inferential statistics in section 12.4.1.3.2. Finally, you can refer to handouts provided in the main class. For descriptive statistics, always report the mean, the standard deviation, and the sample size of the group. For inferential statistics, be sure to include the test statistics (along with the p-value, naturally), as well as the 95% confidence intervals (if available). The effect sizes for the correlation and regression are the test statistics themselves (the correlation coefficient and the regression coefficient, respectively). Thus, there is no reason in Study 2 to report effect sizes. Each analysis should be accompanied by a Figure (a scatterplot in this case). Do not use tables here. In this case, you can use the Correlation matrix function under Plot in the Correlation Matrix analysis under Regression in jamovi. Or you can use the plotting function available under Exploration &gt; scatr &gt; Scatterplot. You do not need to worry for the correlation that the axes will be in their raw values, though you should note this148 Refer to Assignment #3 in Chapter 16 section 16.3.2.1 for instructions on how to insert figure captions. Again, be sure to include in the prose of the Results direct reference to the figures that are in this section (e.g., “As depicted in Figure…”). There is a rubric in Appendix F for the Results section. This has not changed. 17.5.2.2 Discussion In short, this Discussion section should briefly mention the relevant questions or predictions made at the end of the Introduction (i.e., the continuous predictor variables). This should be followed by a summary of how the results on that particular question/prediction came out (without using statistics), and whether they supported or disconfirmed any prediction made, or whether the outcome was unresolved. However, unlike in Writing Assignment #3 (for Study 1), there is no transition for this Discussion section since the next section is the General Discussion. Once you have written out what you need for the review and comparison, you are done. There is a rubric in Appendix H for the Results section. However, the third and fourth criteria in that rubric (i.e., the Transition and the Preview) do not apply to the last study in a multi-study design. 17.5.2.3 General Discussion In this section, the last of your paper, you should strategize to write it out in three sections. As spelled out in the bulk of this chapter, you should synthesize your findings (see section 17.3.1), then clarify them (for lack of a better term; see section 17.3.2), and then conclude your study (see section 17.3.3). In short, to synthesize your findings, you both summarize your findings as a set, and compare that summary to the expectations you set up at the end of the Introduction. To clarify your study, you tell your reader not only the best way to intepret the current study, but also the best way to place the ultimate findings into the wider world of science. That is, perhaps they need to be qualified, or extended, or taken with a grain of salt. Whatever your opinion here, you should let your reader know. To conclude your study, you should somehow point to a future direction of your research, or warn others about the lessons you learned, or simply state flatly your general finding for the reader to remember, or some combination of the above. Be sure your study ends on the most general note possible. There is a rubric for the General Discussion in Aappendix J. Naturally, the three rubrics (F, H, and J) will be combined (ignoring the 3rd and 4th criteria in Appendix H) to evaluate this assignment. You can also find an example of this assignment in Appendix I, something we wrote based on the data from this class project from Fettig, López Fuentes, and Villarreal (2019), students who formed a research group in the PSYC 301 class during Spring, 2019. There is merely a switch to correlation and simple regression from the independent samples t-test and the One Way ANOVA↩︎ This specific-to-general progression is also true of Discussion (not General Discussion) sections in single-study papers↩︎ The image is simplified, and more relevant to a single-study paper.↩︎ or Discussion in a single-study paper↩︎ There are much, much more severe side effects of this, like the file-drawer effect and replication failures. But we do not dicuss these here as they would be tangential.↩︎ It is scientifically unwise to over-sell your study in terms of its soundness.↩︎ We realize how unlikely this is since there was no time in this class to look into supporting research.↩︎ Clearly, you would probably be making these “stories” up since you are working off of counter findings.↩︎ Understand however, that doing this would be restricted to this class since we allowed you use these in the Introduction. In the scientific world, anecdotes are next to worthless as data.↩︎ In a strange sense, the lack of a transition here is a signal that the next section must be the General Discussion.↩︎ This should not be confused with HARK-ing (“hypothesizing after results are known”).You should understand that changing one’s hypothesis according the research results is considered a QRP (a questionable reserch practice). That is not what we are suggesting here. Rather, we are suggesting that you go back and make yourself clearer as to what you should (past tense) have been predicting in the first place. This is clarification, not HARKing.↩︎ Recall that Pearson’s r is simply a simple regression where both variables have been standardized into z-scores.↩︎ "],["a-introduction-example.html", "A: Introduction example", " A: Introduction example WARNING: This document will be uploaded to Turnitin. It will also have been read by your classmates. Therefore, it is a very bad idea to copy this to modify to use in your own results section. Doing so would be a form of plagiarism. Personality and domain generalizability in the anti-vaccination movement If one so chose, it would not be difficult to find what seems to be an unusually large number of people who oppose vaccinations. One could search on social media or on search engines for groups and websites that warn people against getting their children vaccinated according to public health guidelines. A time-traveler from the mid-20th century might find this situation very disconcerting. Ochmann and Roser mapped the infection and death rates from polio throughout the 20th century on the website https://ourworldindata.org/polio. Polio was a relatively rare disease worldwide prior to the 20th century, but then began spreading, especially during the 1940s and 1950s, at which point it had become an epidemic. But with the introduction of the polio vaccine in 1955, these rates dropped dramatically, and were statistically zero by the mid-1960s. Naturally, people of that generation must have in awe of the power of vaccines. Any debate about the effectiveness of vaccines must have seemed out of the question. But several decades later, in 1998, a British physician named Andrew Wakefield (along with 12 colleagues) published a widely discussed study in the journal The Lancet, claiming that there was a link between the MMR (measles-mumps-rubella) vaccine and autism. The study was later found to be quite fraudulent in several respects. The article was retracted by the Lancet, and Wakefield was not only disgraced in the UK, but also lost his medical license there. He subsequently moved to the US, where he still, somehow, retained some credibility. This is where things got “interesting.” Wakefield has somehow retained a sizeable cadre of ardent followers despite his utterly catastrophic loss of credibility in the medical establishment. His anti-vaccination movement seems to have grown. Or if it has not grown in number, it certainly has in the degree of zealotry. His followers can be found on social media and elsewhere relentlessly criticizing the advocates of mainstream medical science, where repeated studies have found no link between the MMR vaccine and autism. In fact, the benefits to public vaccination in general far outweigh the vaccine-injury risk posed to individuals for any given vaccine. Historical data clearly show this. One of the more interesting questions we, as psychologists, can ask is the following: What sort of underlying personality characteristic drives people to states of such obsessive science denial? Surely, the ultimate answer to this will be complex, so for the purposes of the current paper,we asked more specific questions, namely: 1) Is this personality trait “specific,” and thus restricted to certain domains (e.g., vaccines vs. extraterrestrials vs. global warming) and not others? Or 2) Is it “general,” thereby applying across multiple domains (e.g., not only vaccines, but also extraterrestrials, etc.)? We were also interested in whether demographic variables or intellectual identification with mainstream knowledge communities had an effect. We designed a survey that would pose various scenarios to respondents that had to do with conspiracy theories that are commonly touted in social media. We recorded not only agreement/disagreement with the likelihood of those conspiracies, but also critical demographic information (e.g., sex, age, political affiliation, education level, socioeconomic status, etc.), as well as some variables measuring degree of intellectual identification with the information professions (e.g., science, governmental agencies, the intelligence community, etc.). Although this study is exploratory in nature, our best guess is that conspiracy thinking is domain specific since it takes so much energy to be a denialist in any one domain. Doing so across domains would probably be too much for most people. But that is just a relatively uninformed guess. It could very well be the case that it is generalized across domains. The answers to the questions raised here may ultimately help us find ways to minimize the damaging effects that conspiracy thinking can engender. "],["b-introduction-rubric.html", "B: Introduction rubric", " B: Introduction rubric Table 17.1: Rubric for Introduction sections. Weight Section Advanced Proficient Developing Emerging 5% The opening The opening sentence is general (and perhaps even commonplace), but clearly related to the topic funnel that follows. The opening sentence is general (and perhaps even commonplace), but it is mostly related to the topic funnel that follows. The opening sentence is general (and perhaps even commonplace), but it is only distantly related to the topic funnel that follows. There is no general, opening statement. The writer starts abruptly by delving straight into the topic funnel in a fairly specific manner. 50% The topic funnel The topic starts as more specific than the opening, and ends as more general than whatever section immediately follows it (e.g., gap, methods summary, research question, etc.). In between, it proceeds naturally in a general-to-specific direction. The topic starts just as general as the opening, and/or ends as only slightly more general than the section immediately follows it (e.g., gap, methods summary, research question, etc.). In between, it proceeds in a general-to-specific direction, but the progression seems a little unnatural or awkward at times. The topic starts just as general as the opening and/or end just as specific as whatever section immediately follows it (e.g., gap, methods summary, research question, etc.). In between, it may proceed in only a vaguely general-to-specific direction. The topic actually either starts a more general than the opening, or ends as more specific than whatever section immediately follows it (e.g., gap, methods summary, research question, etc.). Or in between, it proceeds in an unorganized way. 10% The research gap After the main body, but before any of the sections that are supposed to follow it (e.g., the method summary, research question, etc.), the author identifies a gap in knowledge that proceeds naturally from the paragraphs that preceded it. After the main body, but before any of the sections that are supposed to follow it (e.g., the method summary, research question, etc.), the author identifies a gap in knowledge, but the gap arises somewhat abruptly, given the paragraphs that preceded it. The author identifies a gap in knowledge, but the gap arises somewhat abruptly and may be placed strangely, like after the brief review of the methods, or in the middle of the body somewhere. There is either no gap in knowledge identified anywhere, or if there is one, it is not obvious to the reader. 5% The method summary After identifying the research gap, the author included a brief and very clear outline of the methods that will be used to address the gap. After identifying the research gap, the author included a brief and fairly clear outline of the methods that will be used to address the gap. The author included a somewhat unclear outline of the methods that will be used. Or if the outline was clear, it was placed awkwardly, like after the research question, or before the identification of the gap. The author also included nothing or very little about the methods that will be used. On the other hand, they may have written too much here, where much of what is written probably belongs in the Methods section. 15% The research question/hypothesis The Introduction ends with a precise well-reasoned, research question (and/or hypothesis) that clearly fills the gap in knowledge identified above it. Appropriate predictions are made. The Introduction ends with a precise, but slightly less than perfectly reasoned research question (and/or hypothesis) that mostly fills the gap in knowledge identified above it. Predictions are made, but they may not be perfectly well thought out. The Introduction ends with a research question (and/or hypothesis), but it is not very precise or well-reasoned that somewhat fills the gap in knowledge identified above it. The Introduction does not end with a clear research question (and/or hypothesis). 5% The meaning The introduction ends with an explicit, clear, and relatively sober (i.e., slightly understated) statement about the wider meaningfulness of the study. The introduction ends with an explicit, and clear statement about the wider meaningfulness of the study. But the meaningfulness might be slightly exaggerated. The introduction ends with an explicit statement about the wider meaningfulness of the study. But the meaningfulness might be somewhat unclear or slightly exaggerated. Alternatively, it appears not at the end, but rather earlier. The wider meaningfulness of the study may not be stated at all, or if it is, it is not at all justified given the nature of the study. 10% Language and style The writing is informal academic style, yet retains simplicity in expression. Transitions are always clear between paragraphs. And the ordering of sentences within paragraphs is always coherent, and cohesive. The writing is in formal academic style, but sometimes gets too ornate and/or ambiguous. Transitions are usually clear between paragraphs. And the ordering of sentences within paragraphs is usually coherent, and cohesive. The writing is in formal academic style, but gets too ornate or ambiguous. Transitions between paragraphs are usually unclear. Within paragraphs, sentence ordering is usually hard to follow, but careful readers can ultimately make sense of it. The writing is too informal. There may be no clear transitions between paragraphs. It may be difficult to discern any sentence-ordering pattern within paragraphs; readers are left fatigued by the effort required to make sense of the ordering. "],["c-methods-example.html", "C: Methods example", " C: Methods example WARNING: This document will be uploaded to Turnitin. It will also have been read by your classmates. Therefore, it is a very bad idea to copy this to modify to use in your own results section. Doing so would be a form of plagiarism. The survey design and data for the example Method section below is adapted from a classroom project by Fettig, López Fuentes, and Villarreal (2019):149 Method Participants There were a total of 37 participants who completed the survey. Twenty-three of them were women (62.2%), and the remainder were men (n = 14, 37.8%). The ages (in years) ranged from 18 to 22 (M = 19.1, SD = 0.95), with the men being slightly older and more varied in age (M = 19.4, SD = 1.09) than the women (M = 19.0, SD = 0.825). The mean, the median, and the mode of age were all equal at 19. The participants were recruited via convenience sampling through two GroupMe accounts shared by students from two large classes in Psychology at Texas A&amp;M from the previous year. Materials The survey consisted of eleven questions overall. Two of the variables measured participant demographics: Sex (male vs. female); and Age (in years). Two other questions had to do with administration (covered in Procedures below). The remaining seven variables had directly to do with the research questions at hand. These are covered next in the order in which they appeared in the survey to the participants. Dependent variables. There were three dependent variables that were designed to be averaged into a single composite variable: Sleep Quality. The first, Sleep Satisfaction, was worded as follows: “How satisfied are you with the amount of sleep you get each night?” Responses were measured on a Likert scale from 1 to 5, with 1 corresponding to “Not at all satisfied,” and 5 corresponding to “Very satisfied.” The second variable Restedness, was worded in the survey as follows: “On a scale of 1 to 5, how rested do you feel on average?” This question used the same scale, with 1 corresponding to “Not at all rested,” and 5 corresponding to “Very rested.” Finally, the third dependent variable, Typical Hours Slept, was simply the number of hours that the participant estimated they slept per night. This number was entered directly by the participant. Independent variables. There were four independent variables. The first two were nominal variables. One was a two-level nominal variable called Extracurricular Activities.The question that corresponded to the variable was as follows: “Are you a part of extra curricular activities? i.e., clubs, sports, greek life.” Participants could answer either “Yes” or “No.” The other nominal variable was a three-level variable called Employment Status (“Are you employed?”). There were three options for the participant: “No,” “Full-time,” and “Part-time.” The next two independent variables were continuous. The first of these was Academic Hours (“How many academic hours are you currently taking?”). Participants entered the number of credit hours they were taking that semester. The second (and last of the four independent variables) was Workload Intensity: “How do you rate the intensity of your overall workload, including academic, work-related, and extracurricular commitments?” This was measured on a 1-5 Likert scale with 1 corresponding to “Not intense at all,” and 5 corresponding to “Very intense.” Design &amp; Analysis We were interested in how Sleep Quality (the composite, dependent variable) was predicted by whether students were involved in extracurricular activities, the degree to which they were employed, their current academic course load, and the intensity of their general workload (work, academic, and otherwise). The calculation of the composite variable, Sleep Quality, first required the conversion of the three, raw dependent variables into z-scores, and from there into a composite score. Thus, the following variables were converted to z-scores: Sleep Satisfaction (M = 3, SD = 1); Restedness (M = 2.92, SD = 1.12); and Typical Hours Slept (M = 6.45, SD = 1.12). After the standardization into z-scores, the three newly standardized scores were averaged into the single composite. An internal reliability analysis showed that item consistency was quite high (Cronbach’s Alpha = 0.86). An analysis at the Alpha-level change due to single-item dropping revealed that removing any of the items would result in a drop in Alpha. Therefore, all three variables were retained for the composite. The resulting composite, Sleep Quality (M = 0, SD = 0.883) ranged from a score of -2.39 to a maximum score of 1.73. This served as the single dependent variable in all subsequent analyses. Procedures The researchers simply sent the link to this survey out to potential respondents in the GroupMe list mentioned above under Participants. Upon receiving the survey, participants were informed about the nature of the study, that participation was optional, and that no personally identifying information would be recorded. At this point, there were given the opportunity to opt out of the study. If they continued with the study, they were asked all the questions mentioned above in the same order as above. Importantly, the participants were not required to respond to any of the questions; they could leave any response blank if they so chose. Finally, at the end of the study, participants were given the option to exclude their data from analysis. None of the participants chose this option. After two weeks of soliciting responses, the final data set was selected for analysis. Permission to use their data was given each author separately in late September, 2019, via a series of email exchanges.↩︎ "],["d-method-rubric.html", "D: Method rubric", " D: Method rubric Table 17.2: Rubric for Method sections. Weight Section Advanced Proficient Developing Emerging 5% Participants (basic) ALL the basic statistics are included. These would include such statistics as the total count analyzed, counts per sex, and the mean and standard deviation of age. MOST of the basic statistics are included, but ONE of the following is missing or under-reported: the total count analyzed, counts per sex, and the mean and standard deviation of age. Most of the basic statistics are included, BUT MORE THAN ONE of the following is missing or under-reported: the total count analyzed, counts per sex, and the mean and standard deviation of age. ALMOST NONE of the basic demographic information about participants is included. 10% Participants (elaborated) The descriptions here are FULLY detailed enough for an outside researcher to replicate the sample of participants. The descriptions here are ALMOST detailed enough for an outside researcher to replicate the sample of participants, BUT AN IMPORTANT DETAIL SEEMS TO BE MISSING. The descriptions here are NOT REALLY detailed enough for an outside researcher to replicate the sample of participants. MORE THAN ONE IMPORTANT DETAILS SEEM TO BE MISSING. There is HARDLY ANY OR NO information here that would allow an outside researcher to replicate the participants in the study. 30% Materials This section is characterized by details of the instrument itself (e.g., survey questions, experimental trials) that are FULLY detailed enough to allow an outside researcher to create virtually the same instrument. This section is characterized by details of the instrument itself (e.g., survey questions, experiment trials) that are ALMOST detailed enough to allow an outside researcher to create virtually the same instrument. However, ONE major element IS missing. This section is characterized by details of the instrument itself (e.g., survey questions, experiment trials) that are INSUFFICIENT to allow an outside researcher to create virtually the same instrument. MORE THAN ONE major element IS missing. There is HARDLY ANY OR NO information here that would allow an outside researcher to replicate the instrument used in the study. 20% Design The following descriptions are FULLY detailed enough for another researcher to carry out the same data analysis (in jamovi): how the dependent and independent variables relate; any data transformations; and any analyses of internal consistency. The following descriptions are ALMOST detailed enough for another researcher to carry out the same data analysis (in jamovi): how the dependent and independent variables relate; any data transformations; and any analyses of internal consistency. The following descriptions are NOT REALLY detailed enough for another researcher to carry out the same data analysis (in jamovi): how the dependent and independent variables relate; any data transformations; and any analyses of internal consistency. There is HARDLY ANY OR NO information here that would allow an outside researcher to carry out the same data analysis. 25% Procedure This section describes the steps that both the researcher and participants took. Both are both FULLY detailed enough to allow an outside researcher to virtually re-create the same sequence of events for a new set of participants. This section describes the steps that both the researcher and participants took. Both are both ALMOST detailed enough to allow an outside researcher to virtually re-create the same sequence of events for a new set of participants. This section describes the steps that both the researcher and participants took. Both are both NOT REALLY detailed enough to allow an outside researcher to virtually re-create the same sequence of events for a new set of participants. There is HARDLY ANY OR NO information here that would allow an outside researcher to replicate the same sequence of events for a new set of participants. 5% Level of detail The content is sufficiently detailed for replication, but not overly so. The content provides a little too much detail. The content clearly provides too much detail The content is very seriously over-detailed. 5% General language The writing is formal, yet simple. Transitions are always clear between paragraphs. And the ordering of sentences within paragraphs is always coherent, and cohesive. The writing is formal, but sometimes too ornate or ambiguous. Transitions are usually clear between paragraphs. the ordering of sentences within paragraphs is mostly coherent, and cohesive. The writing is either too informal or gets too ornate or ambiguous. Transitions between paragraphs might be unclear. Within paragraphs, sentence ordering might be hard to follow, but careful readers can ultimately make sense of it. The writing is too informal. There may be no clear transitions between paragraphs. It may be difficult to discern any sentence-ordering pattern within paragraphs; readers are left mentally fatigued by the effort required to make sense of the ordering. "],["e-results-example.html", "E: Results example", " E: Results example WARNING: This document will be uploaded to Turnitin. It will also have been read by your classmates. Therefore, it is a very bad idea to copy this to modify to use in your own Results section. Doing so would be a form of plagiarism. TIP: As noted in Chapter 14, section 14.4.1.5, the proper way to use the example we have provided below is to paraphrase it. To do this, read through the example, section by section. For each section, read for meaning. Make sure you understand it. Then minimize it in your screen and bring up the document that you are writing. Critically, walk away for a few minutes. Wash a couple of dishes; dust off a shelf, write a text message to a friend; do some jumping jacks; whatever, but actively do something totally unrelated to writing this section. Then return after 2-3 minutes, and write out the relevant section in your own words, according to the content you remember reading from the example passage (adjusted for your own content, naturally). The cognitive reality here is that you will remember the content after 2-3 minutes, but you will have forgotten both the ordering of information and the exact wording of the original [The first evidence of this was found by Sachs (1967)]. This way, there is very little chance that you will plagiarize. The survey design and data for the example Results sections below are adapted from a classroom project by Fettig, López Fuentes, and Villarreal (2019). Study 1\\(^{\\dagger}\\) Results In this portion of the study, we looked at the effects on overall sleep quality from two categorical variables. The first was whether students were involved in extracurricular activities. This was answered as either Yes or No in the survey, and therefore had two levels. The second was their employment status, and could be answered as No (not employed), Part-time, or Full-time. Therefore, it had three levels. The variable Extracurricular Activities was analyzed using an independent-samples t-test, and the variable Employment Status was analyzed using a One-Way ANOVA. For the t-test, we used Welch’s test, which is corrected for heterogeneity of variance, whereas in the One-Way ANOVA, we tested the assumption of homogeneity of variance with Levene’s Test. Extracurricular Activities The results of the analysis of Extracurricular Activities on Sleep Quality is represented in Figure 1 below. Figure 17.2: Figure 1. The effect of extracurricular activities on sleep quality. Error bars represent the 95% confidence interval. Sleep quality is on a standardized scale. Although participants who reported being involved in extracurricular activities indicated that their sleep quality was better overall (M = 0.0522, SD = 0.891, n = 31) than their counterparts who reported not being involved in extracurricular activities (M = -0.285, SD = 0.858, n = 6), the difference was non-significant, Welch’s t(7.25) = -0.885, p = .405, d =-0.384, 95% CI of the difference in means [-1.35, 0.87]. It seems that whatever differences there were between the groups may have been due to chance. Employment Status The results of the analysis of Employment Status on Sleep Quality is represented in Figure 2 below. Figure 17.3: Figure 2. The effect of employment status on sleep quality. Error bars represent the 95% confidence interval. Sleep quality is on a standardized scale. There seems to be a trend for participants who are working full time to report having lower sleep quality (M = -0.798, SD = not available, n = 1) than either the participants who were not working at all (M = 0.0124, SD = 0.915, n = 28) or who were working part-time (M = 0.0563, SD = 0.831, n = 8). However, there were no significant differences among the groups, F(2,34) = 0.504, p = 0.609, partial eta-squared = 0.029.150 In fact, as is clear to see, there was only one participant who indicated they were working full time. These results cannot be trusted since there is no within-group variance associated with a factor level with an n of 1. If we drop that level and re-run the analysis between the students either working part time or not at all, we also get no statistically significant effect, F(1,34) = 0.00772, p = 0.931, partial eta-squared = 0. Discussion… \\(\\dagger\\) relevant only to Writing Assignment #3 Study 2\\(^{\\dagger\\dagger}\\) Results For study two, we examined how the following two variables affected Sleep Quality: 1) the number of units taken at the university (Academic Hours); and 2) the intensity of one’s overall workload situation (Workload Intensity), which included academic as well as non-academic workload. As explained in the Method section, the former, Academic Hours, was measured simply in academic units, whereas the latter, Workload Intensity, was measured on a Likert scale from 1 to 5, with 1 indicating not intense at all and 5 representing very intense. The intensity of overall student workload is analyzed first. Workload Intensity We used Pearson’s r to uncover the relation between Sleep Quality and Workload Intensity. This relation can be seen below in Figure 3 below. Figure 17.4: Figure 3. The effect of Workload Intensity on Sleep Quality. Both Sleep Quality and Workload Intensity are on standardized scales. This relation was statistically significant, r(34) = -.34, p = .043, 95% CI [-.60, -.013]. Simply put, as one’s overall workload increases, one’s quality of sleep declines. Academic Hours The outcome variable Sleep Quality was regressed on Academic Hours using a simple linear regression. The results can be seen in Figure 4 below. Figure 17.5: Figure 4. Workload Intensity regressed on Academic Hours. Sleep Quality (standardized) is predicted to be at 0.133 if a “student”151 were taking no credits at all. This intercept is not significantly different from zero, b = 0.133, t(34) = 0.097, p = .92, 95% CI [-2.66, 2.93]. This suggests, albeit rather oddly, that if a student did not register for any academic units at all, their quality of sleep would not be distinguishable from the average. Standardized Sleep Quality did decrease by a miniscule amount (b=.0093, 95% CI [-0.203, 0.185]) for every extra academic unit taken (starting in this sample of students at 10 units). This tiny drop in quality of sleep was, unsurprisingly, not statistically significant, t(34) = -0.097, p = .92. In general then, the quality of sleep that students report seems not to be affected at all by their courseload. Discussion… \\(\\dagger\\dagger\\) relevant only to Writing Assignment #4 NOTE: THIS WOULD NOT APPEAR AT ALL IN THE RESULTS SECTION; IT IS HYPOTHETICAL. However, if there had been a significant difference, we would have reported the post-hoc tests as follows, which would normally highlight at least one significant difference, but not in this case since there was no significant difference in the omnibus test: \"We also ran post-hoc comparisons using the Holm correction. There was no significant difference between those who reported working full time and those who reported not working at all, t(34) = -0.886, p = 1. Nor was there any significant difference between full-timers and those who reported working part time, t(34) = -0.897, p = 1. Finally, there was no significant difference between the part-timers and those who reported not working at all, t(34) = -0.122, p = 1.↩︎ Note that the value at the intercept is meaningless in this study since there are, almost by definition, no active students taking 0 units.↩︎ "],["f-results-rubric.html", "F: Results rubric", " F: Results rubric Table 17.3: Rubric for Results sections. Weight Section Advanced Proficient Developing Emerging 20% Organization The analyses are presented in a HIGHLY organized manner, with one analysis coming clearly before the other. The analyses are presented in a MOSTLY organized manner, with one analysis coming clearly before the other, but OCCASIONALLY THE ANALYSES GET MIXED UP. The analyses are presented in a SOMEWHAT DISORGANIZED manner, making the organization DIFFICULT TO FOLLOW OVERALL, but if the reader works hard enough, they can make sense of it. The analyses are presented in a DISORGANIZED manner, with multiple analyses being presented within a single paragraph, woven together ALMOST AT RANDOM. 20% Prose If one were to remove the descriptive and inferential statistics, only CLEAR, PLAIN, EASILY READABLE English sentences would remain. If one were to remove the descriptive and inferential statistics, READABLE English sentences would remain, but the language might be SLIGHTLY AWKWARD at times. If one were to remove the descriptive and inferential statistics, English sentences would remain, but there would be SUBSTANTIAL ISSUES WITH READABILITY. Statistics are tangled up with sentences so that if the numbers were removed, MOSTLY INCOMPLETE SENTENCES WOULD REMAIN. 20% Statistics All relevant statistics are included and presented in the format suggested by the American Psychological Association. All relevant statistics are included and presented in the format suggested by the American Psychological Association, but not consistently. Not all relevant statistics are included, and they are not presented consistently in the format suggested by the American Psychological Association. Not all relevant statistics are included. Nore are they presented in APA format at all, really. 20% Tables/Figures Tables/Figures include all important, descriptive statistical information, presented in an orderly, or visually transparent manner. Tables/Figures include most important, descriptive statistical information, presented in an orderly, or visually transparent manner. Tables/Figures include most important, descriptive statistical information, but they are presented in a disorderly, or a somewhat visually confusing manner. Tables/Figures are quite unorganized in terms of the information conveyed. It is difficult to infer what the key statistics are from the table/figure. 20% General language The writing is formal, yet simple. Transitions are always clear between paragraphs. And the ordering of sentences within paragraphs is always coherent, and cohesive. The writing is formal, but sometimes too ornate or ambiguous. Transitions are usually clear between paragraphs. the ordering of sentences within paragraphs is mostly coherent, and cohesive. The writing is either too informal or gets too ornate or ambiguous. Transitions between paragraphs might be unclear. Within paragraphs, sentence ordering might be hard to follow, but careful readers can ultimately make sense of it. The writing is too informal. There may be no clear transitions between paragraphs. It may be difficult to discern any sentence-ordering pattern within paragraphs; readers are left mentally fatigued by the effort required to make sense of the ordering. "],["g-discussion-example.html", "G: Discussion example", " G: Discussion example WARNING: This document will be uploaded to Turnitin. It will also have been read by your classmates. Therefore, it is a very bad idea to copy this to modify to use in your own Discussion section. Doing so would be a form of plagiarism. TIP: As noted in Chapter 14, section 14.4.1.5, the proper way to use the example we have provided below is to paraphrase it. To do this, read through the example, section by section. For each section, read for meaning. Make sure you understand it. Then minimize it in your screen and bring up the document that you are writing. Critically, walk away for a few minutes. Wash a couple of dishes; dust off a shelf, write a text message to a friend; do some jumping jacks; whatever, but actively do something totally unrelated to writing this section. Then return after 2-3 minutes, and write out the relevant section in your own words, according to the content you remember reading from the example passage (adjusted for your own content, naturally). The cognitive reality here is that you will remember the content after 2-3 minutes, but you will have forgotten both the ordering of information and the exact wording of the original [The first evidence of this was found by Sachs (1967)]. This way, there is very little chance that you will plagiarize. The survey design and data for the example Discussion section below is adapted from a classroom project by Fettig, López Fuentes, and Villarreal (2019). Study 1… Results …(see Appendix E) Discussion\\(^{\\dagger}\\) The purpose of this study was to look at how different aspects of student life affect sleep quality. Study 1 addressed how two relatively broad categories of student life affected sleep quality, namely, whether students were involved in extracurricular activities or not, and what their employment status was. We were expecting to find that students involved in extracurricular activities would report having lower quality sleep than students who were not involved in such activities. We were also expecting to find that students who were not working at all would indicate that they had better sleep than students who were working part-time or full-time. Additionally, we were expecting to find that students working part-time would fare better on sleep quality than students working full time. There were trends in the data, but no significant effects. That is, although students who were actually engaged in extracurricular activities reported sleeping better (surprisingly) than those who were not engaged, the difference was not significant. It may be the case that our study was under-powered, but the trend in the data is difficult to explain. Under the statistically unjustified assumption of a Type II error, it could be that actively distracting oneself from the learning demands of academia actually ameliorates the factors that lead to disturbed sleep. Likewise, there were some odd (non-significant) trends with the analysis of employment status. Recall that we were expecting to find that those who were working full-time would report sleeping worse than either those working part-time or those not working at all. However, also recall that there was only one participant who reported working full time, so we could not analyze the data with that level in the ANOVA. Therefore, in the ANOVA with only two levels, we found no statistically significant difference between those who were working part-time and those not working at all. At best, there seemed to be a great deal more variation among the part-timers vs. the non-workers. This might be worth further investigation. However, it seems somehow incomplete to restrict oneself to analyzing sleep quality by how students fit in to broad student/professional categories like extracurricular activity and employment. Therefore, in addition to addressing these relatively broad categories, we also addressed a couple of ways in which student life can be intense or not. Specifically, they responded to questions on how many academic hours they were taking and the intensity of their overall workload (academic, professional, etc.). Naturally, we expected to find that the more intense a student’s life is (either in terms of academic or overall workload), the more likely they would report lower levels of sleep quality. This is the topic of Study 2, which follows below. \\(\\dagger\\) relevant only to Writing Assignment #3 Study 2… Results …(see Appendix E) Discussion\\(^{\\dagger\\dagger}\\) In Study 2, we looked at two variables that measured workload. The first was Workload Intensity, which measured the participant’s overall workload. The second was Academic Hours, which simply documented the academic workoload of the participant. Clearly, the former subsumes the latter, so the comparison of the two will give us some insight into the locus of sleep-quality effects. We were expecting to find that overal workload intensity would correlate negatively with sleep quality, and that is exactly what happened. The negative correlation between the two was significant. The harder one works overall, the lower one’s sleep quality. This is not too terribly suprising, however. What is interesting is how this result came out in light of the other variable we measured: Academic Hours. For any student, the number of academic hours that they are taking is naturally going to affect their overall workload; they form a component of that workload. So one would expect that if overall workload is negatively affecting sleep quality, so must academic workload. But this turned out not to be the case. The simple regression we ran between the number of academic hours a student was taking and their workload turned out to be negative, but not significantly so. In fact, the effect size was miniscule, r = -.0166. If our data is sound, then these results suggest that what is driving down sleep quality is not academics, but rather the extra responsibilities in general that students take on. This could come in the form of outside employment, extracurricular activites, family obligations, or any number of things. The current analysis is not set up to disentagle these potential causes, but it does point to the need for further analysis. However, we will cover the combined findings from Study 1 and Study 2 in the General Discussion below. This will yield a few more insights on future direction in this vein of research. \\(\\dagger\\dagger\\) relevant only to Writing Assignment #4 "],["h-discussion-rubric.html", "H: Discussion rubric", " H: Discussion rubric Table 17.4: Rubric for transitional Discussion sections (Note: the Transition and Preview criteria do not apply to the last Discussion section before the General Discussion) Weight Section Advanced Proficient Developing Emerging 20% Review This section begins with a QUICK SUMMARY of the predictions made at the end of the Introduction, specifically: those VARIABLES JUST PRESENTED in the Results section. This section begins with a SUMMARY of the relevant predictions made at the end of the Introduction, but the summary is TOO LONG. This section begins with a QUICK SUMMARY of the predictions made at the end of the Introduction, BUT IT COVERS VARIABLES BEYOND THOSE JUST PRESENTED in the Results section. This section DOES NOT BEGIN WITH ANY MENTION of the predictions made at the end of the Introduction. OR it is is TOO LONG and COVERS IRRELEVANT VARIABLES. 30% Comparison The information presented earlier in the Results section is COMPARED DIRECTLY to the predictions made at the end of the Introduction. INCLUDED ARE STATEMENTS regarding confirmation, disconfirmation, or lack of resolution with respect to those predictions. The information presented earlier in the Results section is compared ONLY INDIRECTLY to the predictions made at the end of the Introduction (i.e., the comparisons are vague). INCLUDED ARE STATEMENTS regarding confirmation, disconfirmation, or lack of resolution with respect to those predictions. The information presented earlier in the Results section is COMPARED DIRECTLY to the predictions made at the end of the Introduction. But there are NO STATEMENTS regarding confirmation, disconfirmation, or lack of resolution with respect to those predictions. The information presented in the Results section is NOT compared to the predictions made at the end of the Introduction (i.e., they are merely repeated from the Results section), NOR are there any statements regarding the confirmation or disconfirmation of predictions (or lack of resolution). 20% Transition A REASONABLE JUSTIFICATION is provided for why there is a subsequent study/experiment. A justification is provided for why there is a subsequent study/experiment, but ONE COULD IMAGINE A BETTER JUSTIFICATION. A JUSTIFICATION is provided for why there is a subsequent study/experiment, but is is NOT WELL-REASONED. There is NO JUSTIFICATION PROVIDED for why there is a subsequent study/experiment in this paper. 10% Preview A CONCISE 1-to-2 sentence summary of the subsequent study is provided. A summary of the subsequent study is provided, but it is too DETAILED. It seems like much of this material could just be used in the next section itself. A summary of the subsequent study is provided, but it is too SHORT. That is, it is difficult to infer what lies ahead. There is no 1-to-2 sentence summary AT ALL of what lies ahead in the study that follows. 20% General language The writing is formal, yet simple. Transitions are always clear between paragraphs. And the ordering of sentences within paragraphs is always coherent, and cohesive. The writing is formal, but sometimes too ornate or ambiguous. Transitions are usually clear between paragraphs. the ordering of sentences within paragraphs is mostly coherent, and cohesive. The writing is either too informal or gets too ornate or ambiguous. Transitions between paragraphs might be unclear. Within paragraphs, sentence ordering might be hard to follow, but careful readers can ultimately make sense of it. The writing is too informal. There may be no clear transitions between paragraphs. It may be difficult to discern any sentence-ordering pattern within paragraphs; readers are left mentally fatigued by the effort required to make sense of the ordering. "],["i-general-discussion-example.html", "I: General Discussion example", " I: General Discussion example WARNING: This document will be uploaded to Turnitin. It will also have been read by your classmates. Therefore, it is a very bad idea to copy this to modify to use in your own General Discussion section. Doing so would be a form of plagiarism. TIP: As noted in Chapter 14, section 14.4.1.5, the proper way to use the example we have provided below is to paraphrase it. To do this, read through the example, section by section. For each section, read for meaning. Make sure you understand it. Then minimize it in your screen and bring up the document that you are writing. Critically, walk away for a few minutes. Wash a couple of dishes; dust off a shelf, write a text message to a friend; do some jumping jacks; whatever, but actively do something totally unrelated to writing this section. Then return after 2-3 minutes, and write out the relevant section in your own words, according to the content you remember reading from the example passage (adjusted for your own content, naturally). The cognitive reality here is that you will remember the content after 2-3 minutes, but you will have forgotten both the ordering of information and the exact wording of the original [The first evidence of this was found by Sachs (1967)]. This way, there is very little chance that you will plagiarize. The survey design and data for the example section below is adapted from a classroom project by Fettig, López Fuentes, and Villarreal (2019). Introduction… Method… [see Appendix C] Study 1… Results… [see Appendix E] Discussion… [see Appendix G] Study 2 Results… [see Appendix E] Discussion… [see Appendix G] General Discussion At the outset of this study, we predicted that if students are given a chance to indicate their levels of daily student stress on a continuum rather than as categories, there would be a greater chance of connecting these stressors to sleep quality. This was true at one level. That is, we did not find any effects in Study 1, where both independent variables were categorical in nature. But we did find an effect in Study 2, where both independent variables were on continuous scales. Thus, at least at one level, it is true that perhaps we need scales with finer resolution in order to detect the relations between daily student stressors and sleep quality. However, Study 2 also showed us that it was not merely the difference between categorical and continuous scales of measurement that led to differences in detecting the relation between daily student stressors and overall sleep quality. Rather, it was the difference between the part and the whole. That is, although both questions in Study 2 were given on continuous scales, the first (academic course load) was yet another sub-component of overall daily stressors, whereas the second (overall work load) asked students to indicate their combined stress level across all domains of their daily student life (academic, work, and extracurricular). Only the latter was significant, which compromises the notion that it was categorical versus continuous scales that mattered most in this study. Instead, what seems to be true is that it was not, in fact, this difference across scales of measurement that mattered, but rather the whole versus the components of the construct. Critically, the second question in Study 2 combined the three previous questions (i.e., both from Study 1, and the first from Study 2) into one. None of the first three turned out to be statistically significant, whereas the last question did. In the end, this suggest that what matters most is what you are measuring, not how you are doing it. As it should be easy to infer, a major weakness of this study was its initial premise: namely, that what matters most in studies of psychological phenomena is quantitative precision. Quantitative resolution is indeed important at some level. We do not dispute this. But it is subordinate to the importance of finding measures that truly address the phenomenon under scrutiny. If we had restricted ourselves to the subcomponents of overall daily workload without ever asking our last, single question regarding overall workload, we might never have found how they must be combined in order to arrive at a measure that can predict sleep quality. All that said, it is also possible (indeed likely) that a composite variable of three continuous variables, each of which addressed the workload subcomponents, would have given us a similar, or even better connection with overall sleep quality. Naturally, this was not possible in the current study since two of the three variables here were categorical. In the future, we hope that our study might serve as a lesson to future researchers who get too drawn in by issues of statistical measurement. That is, make sure that your statistical measures follow your construct development, and not the other way around. In other words, do not put the cart before the horse. "],["j-general-discussion-rubric.html", "J: General Discussion rubric", " J: General Discussion rubric Table 17.5: Rubric for the General Discussion Weight Section Advanced Proficient Developing Emerging 30% Synthesis The information presented across the various Discussion sections is SUMMARIZED AND COMPARED (e.g., confirmation, disconfirmation, or lack of resolution) AS A SET, VERY CLEARLY to the highest-level predictions made at the end of the Introduction. The information presented across the various Discussion sections is SUMMARIZED AND COMPARED (e.g., confirmation, disconfirmation, or lack of resolution) AS A SET, SOMEWHAT CLEARLY to the highest-level predictions in the Introduction. The information presented across the various studies is LESS THAN CLEARLY SUMMARIZED AND/OR COMPARED, AS A SET to the highest-level predictions in the Introduction. They are treated separately, and not synthesized under one unifying idea somehow. The information presented across the various Discussion sections is NEITHER SUMMARIZED NOR COMPARED AS A SET to the highest-level predictions from the Introduction. 30% Clarification The writing is EFFECTIVE in conveying how to interpret the current findings within EITHER the CURRENT OR the BROADER scientific context, OR BOTH. The writing COULD HAVE BEEN MORE EFFECTIVE in conveying how to interpret the current findings within EITHER the CURRENT OR the BROADER scientific context, OR BOTH. The writing WAS FAIRLY INEFFECTIVE in conveying how to interpret the current findings within EITHER the CURRENT OR the BROADER scientific context, OR BOTH. The writing is DEVOID OF ANY ATTEMPT to convey how to interpret the current findings within EITHER the CURRENT OR the BROADER scientific context. 20% Conclusion The last few sentences are GENERAL and ILLUMINATING in nature, yet STILL QUITE WELL connected to the study at hand. The last few sentences are GENERAL in nature, but COULD HAVE BEEN MORE ILLUMINATING. They are still WELL CONNECTED to the study at hand. The last few sentences are MORE SPECIFIC in nature THAN THEY COULD HAVE BEEN, and COULD HAVE BEEN MORE ILLUMINATING. The last few sentences seem like they are more like a CONTINUATION OF THE CLARIFICATION SECTION. In fact, there is little to indicate that there is a separate “Finalization” section at all. 20% General language The writing is formal, yet simple. Transitions are always clear between paragraphs. And the ordering of sentences within paragraphs is always coherent, and cohesive. The writing is formal, but sometimes too ornate or ambiguous. Transitions are usually clear between paragraphs. the ordering of sentences within paragraphs is mostly coherent, and cohesive. The writing is either too informal or gets too ornate or ambiguous. Transitions between paragraphs might be unclear. Within paragraphs, sentence ordering might be hard to follow, but careful readers can ultimately make sense of it. The writing is too informal. There may be no clear transitions between paragraphs. It may be difficult to discern any sentence-ordering pattern within paragraphs; readers are left mentally fatigued by the effort required to make sense of the ordering. "],["references.html", "References", " References Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., … Iannone, R. (2021). Rmarkdown: Dynamic documents for r. Retrieved from https://CRAN.R-project.org/package=rmarkdown Barker, J. E., Semenov, A. D., Michaelson, L., Provan, L. S., Snyder, H. R., &amp; Munakata, Y. (2014). Less-structured time in children’s daily lives predicts self-directed executive functioning. Frontiers in Psychology, 5, 1–16. https://doi.org/10.3389/fpsyg.2014.00593 Bazerman, C. (2010). The informed writing: Using sources in the disciplines. Retrieved from http://wac.colostate.edu/books/informedwriter/ Beetz, A., Uvnäs-Moberg, K., Julius, H., &amp; Kotrschal, K. (2012). Psychosocial and psychophysiological effects of human-animal interactions: The possible role of oxytocin. Frontiers in Psychology, 3, 1–15. https://doi.org/10.3389/fpsyg.2012.00234 Bickel, P. J., Hammel, E. A., &amp; O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from berkeley. Science, 187(4175), 398–404. Retrieved from http://www.jstor.org/stable/1739581 Bolger, P. (2019). A lab manual for PSYC 301: Elementary statistics for psychology. Retrieved from https://scholars.library.tamu.edu/vivo/display/n53f9e9ad Brotherton, R., French, C., &amp; Pickering, A. (2013). Measuring belief in conspiracy theories: The generic conspiracist beliefs scale. Frontiers in Psychology, 4, 1–15. https://doi.org/10.3389/fpsyg.2013.00279 Christophel, E., &amp; Schnotz, W. (2017). Frontiers in Psychology, 8, 1–11. https://doi.org/10.3389/fpsyg.2017.01681 Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). New York, NY: Routlege. Colzato, L., Szapora, A., &amp; Hommel, B. (2012). Meditate to create: The impact of focused-attention and open-monitoring training on convergent and divergent thinking. Frontiers in Psychology, 3, 1–5. https://doi.org/10.3389/fpsyg.2012.00116 Dambacher, M., &amp; Kliegl, R. (2007). Synchronizing timelines: Relations between fixation durations and N400 amplitudes during sentence reading. Brain Research, 1155, 147–162. https://doi.org/10.1016/j.brainres.2007.04.027 Dekker, S., Lee, N., Howard-Jones, P., &amp; Jolles, J. (2012). Neuromyths in education: Prevalence and predictors of misconceptions among teachers. Frontiers in Psychology, 3, 1–8. https://doi.org/10.3389/fpsyg.2012.00429 Delacre, M., Lakens, D., &amp; Leys, C. (2017). Why psychologists should by default use welch’s t-test instead of student’s t-test. International Review of Social Psychology, 30, 92–101. https://doi.org/http://dx.doi.org/10.5334/irsp.82 Dong, Y., Liu, Y., Jia, Y., Li, Y., &amp; Li, C. (2018). Effects of facial expression and facial gender on judgment of trustworthiness: The modulating effect of cooperative and competitive settings. Frontiers in Psychology, 9, 1–5. https://doi.org/10.3389/fpsyg.2018.02022 Driscoll, D. L. (2011). Introduction to primary research: Observations, surveys, and interviews. In C. Lowe &amp; P. Zemliansky (Eds.), Writing spaces: Readings on writing (Vol. 2, pp. 153–174). Anderson, SC: Parlor Press. Fettig, K., López Fuentes, J., &amp; Villarreal, H. (2019). The effect of workload on sleep. An unpublished class project submitted to PSYC 301 at Texas A&amp;M in Spring, 2019. Geukes, S., Gaskell, M. G., &amp; Zwitserlood, P. (2015). Stroop effects from newly learned color words: Effects of memory consolidation and episodic context. Frontiers in Psychology, 6, 1–16. https://doi.org/10.3389/fpsyg.2015.00278 IBM Corp. (2019). SPSS (Version 25.0). Retrieved from https://www.ibm.com/analytics/spss-statistics-software James, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J. R., Milton, A. L., &amp; Holmes, E. A. (2015). Computer game play reduces intrusive memories of experimental trauma via reconsolidation-update mechanisms. Psychological Science, 26(8), 1201–1215. https://doi.org/10.1177/0956797615583071 Jung, N., Wranke, C., Hamburger, K., &amp; Knauff, M. (2014). How emotions affect logical reasoning: Evidence from experiments with mood-manipulated participants, spider phobics, and people with exam anxiety. Frontiers in Psychology, 5, 1–12. https://doi.org/10.3389/fpsyg.2014.00570 Kachel, S., Steffens, M. C., &amp; Niedlich, C. (2016). Traditional masculinity and femininity: Validation of a new scale assessing gender roles. Frontiers in Psychology, 7, 1–19. https://doi.org/10.3389/fpsyg.2016.00956 Kaufman, S. B., Yaden, D. B., Hyde, E., &amp; Tsukayama, E. (2019). The light vs. Dark triad of personality: Contrasting two very different profiles of human nature. Frontiers in Psychology, 10, 1–26. https://doi.org/10.3389/fpsyg.2019.00467 Keller, K., Troesch, L. M., Loher, S., &amp; Grob, A. (2016). The relation between effortful control and language competence—a small but mighty difference between first and second language learners. Frontiers in Psychology, 7, 1–12. https://doi.org/10.3389/fpsyg.2016.01015 Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4, 1–12. https://doi.org/10.3389/fpsyg.2013.00863 Lilienfeld, S. O., Sauvigné, K. C., Lynn, S. J., Cautin, R. L., Latzman, R. D., &amp; Waldman, I. D. (2015). Fifty psychological and psychiatric terms to avoid: A list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases. Frontiers in Psychology, 6, 1–15. https://doi.org/10.3389/fpsyg.2015.01100 Love, J., Dropmann, D., &amp; Selker, R. (2019). jamovi blog. Retrieved from https://blog.jamovi.org/ Macdonald, K., Germine, L., Anderson, A., Christodoulou, J., &amp; McGrath, L. M. (2017). Dispelling the myth: Training in education or neuroscience decreases but does not eliminate beliefs in neuromyths. Frontiers in Psychology, 8, 1–16. https://doi.org/10.3389/fpsyg.2017.01314 McIntyre, K. P. (2016). Online handout. San Antonio, Texas: Trinity University. Mehr, S. A., Song, L. A., &amp; Spelke, E. S. (2016). For 5-month-old infants, melodies are social. Psychological Science, 27(4), 486–501. https://doi.org/10.1177/0956797615626691 Navarro, D. J., &amp; Foxcroft, D. R. (2019). Learning statistics with jamovi: A tutorial for psychology students and other beginners (Version 0.70). https://doi.org/10.24384/hgc3-7p15 Nisbet, E., &amp; Zelenski, J. (2013). The NR-6: A new brief measure of nature relatedness. Frontiers in Psychology, 4, 1–11. https://doi.org/10.3389/fpsyg.2013.00813 P. Turbek, S., Chock, T., Donahue, K., Havrilla, C., M. Oliverio, A., K. Polutchko, S., … Vimercati, L. (2016). Scientific writing made easy: A step-by-step guide to undergraduate writing in the biological sciences. The Bulletin of the Ecological Society of America, 11, 5644–5652. https://doi.org/10.1002/bes2.1258 Paris, J., Ricardo, A., Rymond, D., &amp; Johnson, A. (2019). Child growth and development (Version 1.2). Santa Clarita, CA: College of the Canyons. Perneger, T. V., &amp; Hudelson, P. M. (2004). Writing a research article: advice to beginners. International Journal for Quality in Health Care, 16(3), 191–192. https://doi.org/10.1093/intqhc/mzh053 Poulson, B. (2019). Datalab.cc. Retrieved from https://datalab.cc/ R Core Team. (2021). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org/ Rafi, J. (2019). The jamovi quickstart guide. Retrieved from https://www.jamoviguide.com/index.html Raskin, R., &amp; Terry, H. (1988). A principal-components analysis of the narcissistic personality inventory and further evidence of its construct validity. Journal of Personality and Social Psychology, 54(5), 890–902. Retrieved from http://proxy.library.tamu.edu.srv-proxy1.library.tamu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&amp;db=pdh&amp;AN=1988-25254-001&amp;site=ehost-live Rogers, T., &amp; Milkman, K. L. (2016). Reminders through association. Psychological Science, 27(7), 973–986. https://doi.org/10.1177/0956797616643071 Rollero, C., &amp; De Piccoli, N. (2017). Self-objectification and personal values. An exploratory study. Frontiers in Psychology, 8, 1–8. https://doi.org/10.3389/fpsyg.2017.01055 RStudio Team. (2015). RStudio: Integrated Development Environment for R. Retrieved from http://www.rstudio.com/ Sachs, J. S. (1967). Recognition memory for syntactic and semantic aspects of connected discourse. Perception &amp; Psychophysics, 2, 437–442. Schäfer, T., Sedlmeier, P., Städtler, C., &amp; Huron, D. (2013). The psychological functions of music listening. Frontiers in Psychology, 4, 1–33. https://doi.org/10.3389/fpsyg.2013.00511 Schneider, T. M., &amp; Carbon, C.-C. (2017). Taking the perfect selfie: Investigating the impact of perspective on the perception of higher cognitive variables. Frontiers in Psychology, 8, 971. https://doi.org/10.3389/fpsyg.2017.00971 Schroeder, J., &amp; Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidate’s appeal. Psychological Science, 26(6), 877–891. https://doi.org/10.1177/0956797615572906 Sollaci, L. B., &amp; Pereira, M. G. (2004). The introduction, methods, results, and discussion (IMRAD) structure: A fifty-year survey. Journal of the Medical Library Association, 92(3), 364–371. Steinmayr, R., Weidinger, A. F., Schwinger, M., &amp; Spinath, B. (2019). The importance of students’ motivation for their academic achievement – replicating and extending previous findings. Frontiers in Psychology, 10, 1–11. https://doi.org/10.3389/fpsyg.2019.01730 The jamovi Project. (2019). Jamovi (Version 1.0.0). Retrieved from https://www.jamovi.org/ Tukey, J. W. (1977). Exploratory data analysis. Reading, MA: Addison Wesley. Turing, A. M. (1950). I.—Computing Machinery and Intelligence. Mind, LIX(236), 433–460. https://doi.org/10.1093/mind/LIX.236.433 University of California Museum of Paleontology. (2019). Understanding Science. Retrieved August 11, 2019, from http://www.understandingscience.org Ward, A., &amp; Wegner, D. (2013). Mind-blanking: When the mind goes away. Frontiers in Psychology, 4, 1–15. https://doi.org/10.3389/fpsyg.2013.00650 Watson, J. D., &amp; Crick, F. H. C. (1953). Molecular Structure of Nucleic Acids: A Structure for Deoxyribose Nucleic Acid. Nature, 171, 737–738. https://doi.org/10.1038/171737a0 Weisberg, Y., DeYoung, C., &amp; Hirsh, J. (2011). Gender differences in personality across the ten aspects of the big five. Frontiers in Psychology, 2, 1–11. https://doi.org/10.3389/fpsyg.2011.00178 Wendorf, C. A. (2018). Statistics for social science: A sourcebook of basic statistical methods. Stevens Point, WI: Self. Wickham, H. (2014). Tidy data. The Journal of Statistical Software, 59. Retrieved from http://www.jstatsoft.org/v59/i10/ Wortman-Wunder, E., &amp; Kiefer, K. (2012). Writing the scientific paper. Retrieved from https://writing.colostate.edu/guides/guide.cfm?guideid=83 Xie, Y. (2020). Bookdown: Authoring books and technical documents with r markdown. Retrieved from https://github.com/rstudio/bookdown "]]
